# -*- coding: utf-8 -*-
"""
Netflixè‚¡ç¥¨é¢„æµ‹ - ä¼˜åŒ–ç‰ˆv2ï¼ˆæ¿€è¿›ç‰¹å¾ç­›é€‰ï¼‰
æ”¹è¿›è¦ç‚¹ï¼š
1. ç‰¹å¾å·¥ç¨‹ï¼š156 â†’ ä¼ ç»ŸML(27ç‰¹å¾/50%) + æ·±åº¦å­¦ä¹ (11ç‰¹å¾/20%)
   - ä¼ ç»ŸML: æ ·æœ¬æ•°/65â‰ˆ27 â†’ ä¿å®ˆæ¯”ä¾‹ âœ…
   - æ·±åº¦å­¦ä¹ : æ ·æœ¬æ•°/150â‰ˆ11 â†’ æç®€é˜²è¿‡æ‹Ÿåˆ âœ…
2. LSTMä¼˜åŒ–ï¼šå°æ¨¡å‹+ä½lr+é•¿è®­ç»ƒï¼ˆepochs=150, 10/12æˆåŠŸï¼‰
3. GRUä¼˜åŒ–ï¼šåŸºäºå†å²æœ€ä¼˜GRU_32_3ç²¾ç»†è°ƒä¼˜
4. RFæ·±åº¦ä¼˜åŒ–ï¼šå›´ç»•æœ€ä¼˜RF_200_8é…ç½®ç½‘æ ¼æœç´¢
5. é¢„æµ‹ç›®æ ‡ï¼š5å¤©æ”¶ç›Šç‡ï¼ˆå•å…¬å¸æŠ€æœ¯é¢å»¶ç»­æ€§å¼ºï¼‰
6. åºåˆ—é•¿åº¦20ï¼Œbatch_size=16ï¼Œå……åˆ†è®­ç»ƒ
"""

import os
import sys
import io
import time
import warnings
warnings.filterwarnings('ignore')

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import mutual_info_regression, f_regression
from statsmodels.stats.outliers_influence import variance_inflation_factor

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge

mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False
mpl.rcParams['figure.dpi'] = 200

os.makedirs('visualization_final', exist_ok=True)

RANDOM_STATE = 225
np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_STATE)
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")

print("="*80)
print("Netflixè‚¡ç¥¨é¢„æµ‹ - ä¸°å¯Œç‰¹å¾+å®Œæ•´ç­›é€‰ç‰ˆ")
print("="*80)

# =============================================================================
# ç¬¬ä¸€é˜¶æ®µï¼šä¸°å¯Œç‰¹å¾å·¥ç¨‹
# =============================================================================
print("\n[é˜¶æ®µ1] ä¸°å¯Œç‰¹å¾å·¥ç¨‹...")
df = pd.read_csv('nflx_2014_2023.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date').reset_index(drop=True)

print(f"åŸå§‹æ•°æ®: {df.shape}")

# ===== ç›®æ ‡å˜é‡ =====
df['next_5day_return'] = (df['close'].shift(-5) / df['close'] - 1)

# ===== 1. åŸå§‹ä»·æ ¼ç‰¹å¾ï¼ˆæ— æ³„éœ²ï¼é¢„æµ‹5æ—¥åï¼Œå½“æ—¥ä»·æ ¼å·²çŸ¥ï¼‰=====
df['daily_range'] = (df['high'] - df['low']) / df['close']  # æ—¥å†…æ³¢åŠ¨å¹…åº¦
df['open_close_ratio'] = df['close'] / df['open']  # æ”¶ç›˜/å¼€ç›˜æ¯”
df['high_close_ratio'] = df['high'] / df['close']  # æœ€é«˜/æ”¶ç›˜æ¯”
df['low_close_ratio'] = df['low'] / df['close']   # æœ€ä½/æ”¶ç›˜æ¯”
df['volume_change'] = df['volume'].pct_change()   # æˆäº¤é‡å˜åŒ–

# ===== 2. æ”¶ç›Šç‡ç‰¹å¾ =====
df['return'] = df['close'].pct_change()
df['log_return'] = np.log(df['close'] / df['close'].shift(1))

# æ›´å¤šlagç‰¹å¾ï¼ˆ5-30å¤©ï¼‰
for lag in [1, 2, 3, 5, 7, 10, 15, 20, 30]:
    df[f'return_lag{lag}'] = df['return'].shift(lag)
    df[f'close_lag{lag}'] = df['close'].shift(lag)

# ===== 3. æ»šåŠ¨ç»Ÿè®¡ï¼ˆå¤šæ—¶é—´çª—å£ï¼‰=====
for window in [3, 5, 7, 10, 15, 20, 30, 60]:
    # æ”¶ç›Šç‡ç»Ÿè®¡
    df[f'return_mean_{window}'] = df['return'].rolling(window).mean()
    df[f'return_std_{window}'] = df['return'].rolling(window).std()
    
    # skewå’Œkurtéœ€è¦è‡³å°‘4ä¸ªè§‚æµ‹å€¼ï¼Œåªåœ¨çª—å£>=5æ—¶è®¡ç®—
    if window >= 5:
        df[f'return_skew_{window}'] = df['return'].rolling(window).skew()
        df[f'return_kurt_{window}'] = df['return'].rolling(window).kurt()
    
    # ä»·æ ¼ç»Ÿè®¡
    df[f'close_max_{window}'] = df['close'].rolling(window).max()
    df[f'close_min_{window}'] = df['close'].rolling(window).min()
    df[f'close_mean_{window}'] = df['close'].rolling(window).mean()
    
    # æˆäº¤é‡ç»Ÿè®¡
    df[f'volume_mean_{window}'] = df['volume'].rolling(window).mean()

# ===== 4. æŠ€æœ¯æŒ‡æ ‡è¡ç”Ÿ =====
# åŸå§‹æŒ‡æ ‡å·²æœ‰ï¼šrsi_7, rsi_14, cci_7, cci_14, sma_50, ema_50, sma_100, ema_100, macd, bollinger, atr_7, atr_14

# RSIç›¸å…³
df['rsi_diff'] = df['rsi_14'] - df['rsi_7']
df['rsi_momentum'] = df['rsi_14'].diff()
df['rsi_ma5'] = df['rsi_14'].rolling(5).mean()

# CCIç›¸å…³
df['cci_diff'] = df['cci_14'] - df['cci_7']
df['cci_momentum'] = df['cci_14'].diff()

# MACDç›¸å…³
df['macd_momentum'] = df['macd'].diff()
df['macd_ma5'] = df['macd'].rolling(5).mean()

# ATRç›¸å…³
df['atr_diff'] = df['atr_14'] - df['atr_7']
df['atr_momentum'] = df['atr_14'].diff()

# ä»·æ ¼ä¸å‡çº¿å…³ç³»
df['close_to_sma50'] = (df['close'] - df['sma_50']) / df['sma_50']
df['close_to_sma100'] = (df['close'] - df['sma_100']) / df['sma_100']
df['close_to_ema50'] = (df['close'] - df['ema_50']) / df['ema_50']
df['close_to_ema100'] = (df['close'] - df['ema_100']) / df['ema_100']
df['sma_cross'] = (df['sma_50'] - df['sma_100']) / df['sma_100']

# å¸ƒæ—å¸¦ç›¸å…³
df['bollinger_position'] = (df['close'] - df['bollinger']) / df['bollinger']
df['bollinger_width'] = df['bollinger'] / df['close']

# ===== 5. æ³¢åŠ¨ç‡ç‰¹å¾ =====
for window in [3, 5, 10, 20, 30]:
    df[f'volatility_{window}'] = df['return'].rolling(window).std()

# æ³¢åŠ¨ç‡æ¯”ç‡ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰volatilityåˆ›å»ºåï¼‰
for window in [3, 5, 10, 20, 30]:
    if window != 20:  # é¿å…é™¤ä»¥è‡ªå·±
        df[f'volatility_ratio_{window}'] = df[f'volatility_{window}'] / df['volatility_20']

# ===== 6. åŠ¨é‡ç‰¹å¾ =====
for period in [3, 5, 7, 10, 15, 20, 30]:
    df[f'price_momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
    df[f'volume_momentum_{period}'] = df['volume'] / df['volume'].shift(period) - 1

# ===== 7. äº¤å‰ç‰¹å¾ =====
df['price_volume_corr'] = df['close'] * df['volume']
df['volatility_volume'] = df['volatility_20'] * df['volume']
df['rsi_volume'] = df['rsi_14'] * df['volume']

# ===== 8. æ—¶é—´ç‰¹å¾ =====
df['month'] = df['date'].dt.month
df['dayofweek'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter
df['day'] = df['date'].dt.day
df['week'] = df['date'].dt.isocalendar().week
df['is_month_start'] = df['date'].dt.is_month_start.astype(int)
df['is_month_end'] = df['date'].dt.is_month_end.astype(int)
df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)
df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)

print(f"ç‰¹å¾å·¥ç¨‹å: {df.shape}")

# =============================================================================
# ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®ç¼“å­˜æœºåˆ¶
# =============================================================================
CACHE_FILE = 'data_pre.csv'

if os.path.exists(CACHE_FILE):
    print(f"\nâœ… å‘ç°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œç›´æ¥åŠ è½½...")
    df = pd.read_csv(CACHE_FILE)
    df['date'] = pd.to_datetime(df['date'])
    print(f"åŠ è½½æ•°æ®: {df.shape}")
    print("è·³è¿‡ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®æ¸…ç†ï¼Œç›´æ¥è¿›å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
else:
    print("\n[é˜¶æ®µ2] æ•°æ®æ¸…ç†ï¼ˆæ— ç¼“å­˜ï¼Œæ‰§è¡Œå®Œæ•´é¢„å¤„ç†ï¼‰...")
    
    # âœ… æ­£ç¡®çš„NaNå¤„ç†ç­–ç•¥
    print(f"ç‰¹å¾å·¥ç¨‹åNaNç»Ÿè®¡: {df.isnull().sum().sum()}ä¸ªNaN")
    
    # 1. åªåˆ é™¤ç›®æ ‡å˜é‡ä¸ºNaNçš„è¡Œï¼ˆæœ€å5è¡Œï¼Œå› ä¸ºnext_5day_return = close.shift(-5)ï¼‰
    df_valid = df[df['next_5day_return'].notna()].copy()
    print(f"åˆ é™¤ç›®æ ‡å˜é‡NaNå: {df_valid.shape} (åˆ é™¤äº†{len(df) - len(df_valid)}è¡Œ)")
    
    # 2. å¯¹ç‰¹å¾åˆ—çš„NaNè¿›è¡Œå‰å‘å¡«å……ï¼ˆåˆç†å‡è®¾ï¼šç‰¹å¾å˜åŒ–æ˜¯è¿ç»­çš„ï¼‰
    feature_cols = [col for col in df_valid.columns if col not in ['date', 'next_5day_return']]
    df_valid[feature_cols] = df_valid[feature_cols].ffill()
    
    # 3. å¦‚æœè¿˜æœ‰NaNï¼ˆç¬¬ä¸€è¡Œçš„returnç­‰ï¼‰ï¼Œç”¨åå‘å¡«å……
    df_valid[feature_cols] = df_valid[feature_cols].bfill()
    
    # 4. æœ€åæ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaN
    remaining_nan = df_valid.isnull().sum().sum()
    print(f"å¡«å……åå‰©ä½™NaN: {remaining_nan}ä¸ª")
    
    if remaining_nan > 0:
        print("  è­¦å‘Šï¼šä»æœ‰NaNï¼Œç”¨0å¡«å……")
        df_valid = df_valid.fillna(0)
    
    df = df_valid.reset_index(drop=True)
    print(f"æœ€ç»ˆæ•°æ®: {df.shape}")
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
    df.to_csv(CACHE_FILE, index=False)
    print(f"âœ… é¢„å¤„ç†æ•°æ®å·²ä¿å­˜åˆ°: {CACHE_FILE}")

# å‡†å¤‡æ•°æ®
drop_cols = ['date', 'next_5day_return', 'next_day_close']
drop_cols = [col for col in drop_cols if col in df.columns]

# æ—¶é—´åºåˆ—åˆ†å‰²
train_size = int(len(df) * 0.70)
val_size = int(len(df) * 0.15)

df_train = df[:train_size].copy()
df_val = df[train_size:train_size+val_size].copy()
df_test = df[train_size+val_size:].copy()

print(f"è®­ç»ƒé›†: {df_train.shape[0]}, éªŒè¯é›†: {df_val.shape[0]}, æµ‹è¯•é›†: {df_test.shape[0]}")

X_train_raw = df_train.drop(columns=drop_cols)
y_train = df_train['next_5day_return'].values

feature_names = X_train_raw.columns.tolist()
print(f"åˆå§‹ç‰¹å¾æ•°: {len(feature_names)}")

# ===== å®½æ¾è¿‡æ»¤ç­–ç•¥ï¼šä¿ç•™æ›´å¤šåŸå§‹ç‰¹å¾ =====
print("\n[ç‰¹å¾è¿‡æ»¤ç­–ç•¥] å®½æ¾æ¨¡å¼ï¼ˆä¿ç•™æ›´å¤šä¿¡æ¯ï¼‰...")

# ===== Step 1: ç¼ºå¤±å€¼è¿‡æ»¤ï¼ˆé˜ˆå€¼90%ï¼Œéå¸¸å®½æ¾ï¼‰=====
print("\n[Step 1] ç¼ºå¤±å€¼è¿‡æ»¤ï¼ˆé˜ˆå€¼90%ï¼‰...")
missing_ratio = X_train_raw.isnull().mean()
valid_features = missing_ratio[missing_ratio < 0.9].index.tolist()
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(feature_names) - len(valid_features)})")

X_train_filtered = X_train_raw[valid_features]

# ===== Step 2: æ–¹å·®è¿‡æ»¤ï¼ˆé˜ˆå€¼0.0001ï¼Œæ›´å®½æ¾ï¼‰=====
print("\n[Step 2] æ–¹å·®è¿‡æ»¤ï¼ˆé˜ˆå€¼0.001ï¼‰...")
variances = X_train_filtered.var()
valid_features = variances[variances > 0.001].index.tolist()
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(X_train_filtered.columns) - len(valid_features)})")

X_train_filtered = X_train_filtered[valid_features]

# ===== Step 3: ç›¸å…³æ€§è¿‡æ»¤ï¼ˆåˆ é™¤é«˜ç›¸å…³ç‰¹å¾å¯¹ï¼‰=====
print("\n[Step 3] ç›¸å…³æ€§è¿‡æ»¤ï¼ˆé˜ˆå€¼0.95ï¼‰...")
corr_matrix = X_train_filtered.corr().abs()
upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
try:
    corr_with_y = {}
    for col in X_train_filtered.columns:
        try:
            corr = np.corrcoef(X_train_filtered[col].values, y_train)[0, 1]
            corr_with_y[col] = abs(corr) if not np.isnan(corr) else 0.0
        except:
            corr_with_y[col] = 0.0
except Exception as e:
    print(f"  è­¦å‘Šï¼šè®¡ç®—ä¸ç›®æ ‡ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
    corr_with_y = {col: 0.0 for col in X_train_filtered.columns}

# æ‰¾å‡ºé«˜ç›¸å…³ç‰¹å¾å¯¹
to_drop = set()
for column in upper_triangle.columns:
    if upper_triangle[column].max() > 0.95:
        high_corr_features = upper_triangle.index[upper_triangle[column] > 0.95].tolist()
        if len(high_corr_features) > 0:
            # è·å–æ‰€æœ‰ç›¸å…³ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
            all_features = high_corr_features + [column]
            feature_corrs = [(f, corr_with_y.get(f, 0.0)) for f in all_features]
            # æŒ‰ä¸ç›®æ ‡çš„ç›¸å…³æ€§æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„ï¼Œåˆ é™¤å…¶ä»–
            feature_corrs.sort(key=lambda x: x[1], reverse=True)
            to_drop.update([f for f, _ in feature_corrs[1:]])  # åˆ é™¤é™¤äº†æœ€ç›¸å…³çš„

to_drop = list(to_drop)
valid_features = [f for f in X_train_filtered.columns if f not in to_drop]
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(to_drop)})")

X_train_filtered = X_train_filtered[valid_features]

# ===== Step 4: VIFè¿‡æ»¤ï¼ˆå®½æ¾é˜ˆå€¼ï¼‰=====
print("\n[Step 4] VIFè¿‡æ»¤ï¼ˆé˜ˆå€¼10ï¼Œå®½æ¾ï¼‰...")
current_features = X_train_filtered.columns.tolist()

# æ ‡å‡†åŒ–ï¼ˆVIFéœ€è¦ï¼‰
scaler_vif = StandardScaler()
X_scaled = scaler_vif.fit_transform(X_train_filtered)
X_scaled_df = pd.DataFrame(X_scaled, columns=current_features)

iteration = 0
max_iterations = 30  # å‡å°‘è¿­ä»£æ¬¡æ•°

while len(current_features) > 42:  # ä¿ç•™æ›´å¤šç‰¹å¾ï¼ˆè‡³å°‘42ä¸ªï¼‰
    iteration += 1
    if iteration > max_iterations:
        print(f"  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢")
        break
    
    # è®¡ç®—VIF
    vif_data = pd.DataFrame()
    vif_data["Feature"] = current_features
    vif_data["VIF"] = [variance_inflation_factor(X_scaled_df[current_features].values, i) 
                       for i in range(len(current_features))]
    
    max_vif = vif_data["VIF"].max()
    
    if max_vif > 10:  # æ›´å®½æ¾çš„VIFé˜ˆå€¼
        # åˆ é™¤VIFæœ€å¤§çš„ç‰¹å¾
        feature_to_drop = vif_data.loc[vif_data["VIF"].idxmax(), "Feature"]
        current_features.remove(feature_to_drop)
        print(f"  è¿­ä»£{iteration}: åˆ é™¤ {feature_to_drop} (VIF={max_vif:.2f}), å‰©ä½™{len(current_features)}ä¸ªç‰¹å¾")
    else:
        print(f"  æ‰€æœ‰ç‰¹å¾VIF < 10ï¼Œåœæ­¢")
        break

print(f"  æœ€ç»ˆä¿ç•™ç‰¹å¾: {len(current_features)}")
X_train_filtered = X_train_filtered[current_features]

# ===== Step 5: äº’ä¿¡æ¯ç­›é€‰ï¼ˆåŒç‰ˆæœ¬ï¼šä¼ ç»ŸML vs æ·±åº¦å­¦ä¹ ï¼‰=====
print("\n[Step 5] äº’ä¿¡æ¯ç­›é€‰ï¼ˆç”Ÿæˆä¸¤å¥—ç‰¹å¾é›†ï¼‰...")
# æ ‡å‡†åŒ–
scaler_mi = StandardScaler()
X_scaled_mi = scaler_mi.fit_transform(X_train_filtered)

# è®¡ç®—äº’ä¿¡æ¯
mi_scores = mutual_info_regression(X_scaled_mi, y_train, random_state=RANDOM_STATE)
mi_scores_df = pd.DataFrame({'feature': X_train_filtered.columns, 'mi_score': mi_scores})
mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)

# ç‰ˆæœ¬1ï¼šä¼ ç»ŸMLï¼ˆä¿ç•™50%ï¼Œ~27ç‰¹å¾ï¼‰- æ›´æ¿€è¿›ç­›é€‰
n_keep_ml = int(len(mi_scores_df) * 0.50)
selected_features_ml = mi_scores_df.head(n_keep_ml)['feature'].tolist()
print(f"  [ä¼ ç»ŸML] ä¿ç•™top 50%: {len(selected_features_ml)}ç‰¹å¾ (æ ·æœ¬æ•°/10={len(y_train)//10})")
print(f"    Top 10: {selected_features_ml[:10]}")

# ç‰ˆæœ¬2ï¼šæ·±åº¦å­¦ä¹ ï¼ˆä¿ç•™20%ï¼Œ~11ç‰¹å¾ï¼‰- æç®€æ¨¡å¼ï¼Œé¿å…è¿‡æ‹Ÿåˆ
n_keep_dl = int(len(mi_scores_df) * 0.20)
selected_features_dl = mi_scores_df.head(n_keep_dl)['feature'].tolist()
print(f"  [æ·±åº¦å­¦ä¹ ] ä¿ç•™top 20%: {len(selected_features_dl)}ç‰¹å¾ (æ ·æœ¬æ•°/50={len(y_train)//50})")
print(f"    æ·±åº¦å­¦ä¹ ç‰¹å¾: {selected_features_dl}")

X_train_final = X_train_filtered[selected_features_ml]  # ä¼ ç»ŸMLç”¨
X_train_final_dl = X_train_filtered[selected_features_dl]  # æ·±åº¦å­¦ä¹ ç”¨

print(f"\nâœ… ç‰¹å¾è¿‡æ»¤å®Œæˆ: {len(feature_names)} â†’ ML:{X_train_final.shape[1]} / DL:{X_train_final_dl.shape[1]}")

# =============================================================================
# ç¬¬ä¸‰é˜¶æ®µï¼šå‡†å¤‡æœ€ç»ˆæ•°æ®ï¼ˆä¼ ç»ŸMLå’Œæ·±åº¦å­¦ä¹ åˆ†åˆ«å‡†å¤‡ï¼‰
# =============================================================================
print("\n[é˜¶æ®µ3] å‡†å¤‡è®­ç»ƒæ•°æ®...")

# ===== ä¼ ç»ŸMLæ•°æ®é›†ï¼ˆ80%ç‰¹å¾ï¼‰=====
X_val_final = df_val[selected_features_ml]
X_test_final = df_test[selected_features_ml]

# ===== æ·±åº¦å­¦ä¹ æ•°æ®é›†ï¼ˆ50%ç‰¹å¾ï¼‰=====
X_val_final_dl = df_val[selected_features_dl]
X_test_final_dl = df_test[selected_features_dl]

# ç›®æ ‡å˜é‡ï¼ˆä¸¤å¥—å…±ç”¨ï¼‰
y_val = df_val['next_5day_return'].values
y_test = df_test['next_5day_return'].values

print(f"è®­ç»ƒé›†: {X_train_final.shape[0]}, éªŒè¯é›†: {X_val_final.shape[0]}, æµ‹è¯•é›†: {X_test_final.shape[0]}")
print(f"ä¼ ç»ŸMLç‰¹å¾æ•°: {X_train_final.shape[1]}")
print(f"æ·±åº¦å­¦ä¹ ç‰¹å¾æ•°: {X_train_final_dl.shape[1]}")

# ===== ä¼ ç»ŸMLæ ‡å‡†åŒ– =====
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_final)
X_val_scaled = scaler.transform(X_val_final)
X_test_scaled = scaler.transform(X_test_final)

# ===== æ·±åº¦å­¦ä¹ æ ‡å‡†åŒ– =====
scaler_dl = StandardScaler()
X_train_scaled_dl = scaler_dl.fit_transform(X_train_final_dl)
X_val_scaled_dl = scaler_dl.transform(X_val_final_dl)
X_test_scaled_dl = scaler_dl.transform(X_test_final_dl)

# =============================================================================
# ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–
# =============================================================================

# æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰
class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

class SimpleGRU(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                         batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        last_output = gru_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

def create_sequences(X, y, seq_len=10):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len + 1):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len-1])
    return np.array(X_seq), np.array(y_seq)

def load_or_train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
                        epochs=150, lr=0.001, batch_size=16):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼Œå­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™è®­ç»ƒ"""
    model_path = f'models/{model_name}_weights.pth'
    
    if os.path.exists(model_path):
        print(f"ğŸ“‚ åŠ è½½å·²æœ‰æ¨¡å‹...", end=' ')
        try:
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.eval()
            
            # ç›´æ¥åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            with torch.no_grad():
                predictions = model(X_test).cpu().numpy().flatten()
            
            r2 = r2_score(y_test, predictions)
            rmse = np.sqrt(mean_squared_error(y_test, predictions))
            mae = mean_absolute_error(y_test, predictions)
            
            print(f"RÂ²={r2:.4f} (å·²åŠ è½½)")
            
            return {
                'model': model,
                'r2': r2,
                'rmse': rmse,
                'mae': mae,
                'predictions': predictions,
                'time': 0  # åŠ è½½æ—¶é—´å¿½ç•¥ä¸è®¡
            }
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°è®­ç»ƒ...")
            return train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
                             epochs, lr, batch_size)
    else:
        # æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£å¸¸è®­ç»ƒ
        return train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
                         epochs, lr, batch_size)

def train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
               epochs=100, lr=0.001, batch_size=16):
    """è®­ç»ƒæ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    try:
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        
        # é™ä½batch_sizeåˆ°16ï¼Œä¸drop_lastä»¥ä½¿ç”¨æ‰€æœ‰æ•°æ®
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)
        
        criterion = nn.HuberLoss(delta=1.0)
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # å¢åŠ æ­£åˆ™åŒ–
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8)
        
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        patience = 20  # å¢åŠ è€å¿ƒ
        
        for epoch in range(epochs):
            model.train()
            train_loss = 0
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    outputs = model(X_batch)
                    loss = criterion(outputs, y_batch)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            scheduler.step(val_loss)
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        model.eval()
        with torch.no_grad():
            predictions = model(X_test).cpu().numpy().flatten()
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        
        return {
            'model': model,
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions
        }
    except Exception as e:
        print(f"  âŒ {str(e)}")
        return None

print("\n[é˜¶æ®µ4] æ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–...")
print("="*80)

# å‡†å¤‡åºåˆ—æ•°æ®ï¼ˆæ·±åº¦å­¦ä¹ ä½¿ç”¨50%ç‰¹å¾é›†ï¼‰
seq_len = 20  # å¢åŠ åºåˆ—é•¿åº¦ï¼Œæ•æ‰æ›´é•¿æœŸè¶‹åŠ¿
input_size = X_train_scaled_dl.shape[1]  # ä½¿ç”¨æ·±åº¦å­¦ä¹ ç‰¹å¾é›†

print(f"åºåˆ—é•¿åº¦: {seq_len}, ç‰¹å¾ç»´åº¦: {input_size} (DLä¸“ç”¨)")

X_train_seq, y_train_seq = create_sequences(X_train_scaled_dl, y_train, seq_len)
X_val_seq, y_val_seq = create_sequences(X_val_scaled_dl, y_val, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_scaled_dl, y_test, seq_len)

print(f"åºåˆ—æ•°æ®é‡: è®­ç»ƒ={len(X_train_seq)}, éªŒè¯={len(X_val_seq)}, æµ‹è¯•={len(X_test_seq)}")

X_train_seq_tensor = torch.FloatTensor(X_train_seq).to(device)
X_val_seq_tensor = torch.FloatTensor(X_val_seq).to(device)
X_test_seq_tensor = torch.FloatTensor(X_test_seq).to(device)
y_train_seq_tensor = torch.FloatTensor(y_train_seq).unsqueeze(1).to(device)
y_val_seq_tensor = torch.FloatTensor(y_val_seq).unsqueeze(1).to(device)

all_results = {}

# ===== å®Œæ•´åŸºçº¿æ¨¡å‹è®­ç»ƒ =====
print("\nè®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼‰...")

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# Ridgeè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ›´æ¿€è¿›çš„æœç´¢ï¼‰
print("  ä¼˜åŒ–Ridgeè¶…å‚æ•°...")
ridge_params = {'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]}
ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3, scoring='r2', n_jobs=-1)
ridge_grid.fit(X_train_scaled, y_train)
print(f"  â†’ Ridgeæœ€ä½³alpha={ridge_grid.best_params_['alpha']:.2f}, CV_RÂ²={ridge_grid.best_score_:.4f}")

baseline_models = {
    'Ridge': ridge_grid.best_estimator_,
    'Lasso': Lasso(alpha=0.01, max_iter=3000, random_state=RANDOM_STATE),
    'ElasticNet': ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=RANDOM_STATE, max_iter=3000),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE),
    'DecisionTree': DecisionTreeRegressor(
        max_depth=5,                    # æµ…æ ‘ï¼ˆå‰ªæï¼‰
        min_samples_split=50,           # åˆ†è£‚æœ€å°æ ·æœ¬ï¼ˆæ—©åœï¼‰
        min_samples_leaf=20,            # å¶å­æœ€å°æ ·æœ¬ï¼ˆå‰ªæï¼‰
        min_impurity_decrease=0.001,   # æœ€å°çº¯åº¦æå‡ï¼ˆæ—©åœï¼‰
        ccp_alpha=0.01,                 # æˆæœ¬å¤æ‚åº¦å‰ªæ
        random_state=RANDOM_STATE
    ),
    'SVR': SVR(
        kernel='rbf',           # å¾„å‘åŸºæ ¸ï¼ˆéçº¿æ€§ï¼‰
        C=10.0,                 # æ­£åˆ™åŒ–å‚æ•°
        epsilon=0.05,           # Îµç®¡ï¼ˆå¢å¤§é²æ£’æ€§ï¼‰
        gamma='scale',          # æ ¸ç³»æ•°ï¼ˆè‡ªåŠ¨ç¼©æ”¾ï¼‰
        max_iter=5000           # å¢åŠ è¿­ä»£æ¬¡æ•°
    ),
}

baseline_results = {}

# è®­ç»ƒç®€å•æ¨¡å‹
simple_models = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTree', 'SVR', 'GradientBoosting']
for name in simple_models:
    if name in baseline_models:
        model = baseline_models[name]
        print(f"\nè®­ç»ƒ {name}...")
        start_time = time.time()
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        elapsed_time = time.time() - start_time
        
        print(f"  âœ… RÂ²={r2:.4f}, RMSE={rmse:.6f}, MAE={mae:.6f} ({elapsed_time:.1f}ç§’)")
        
        baseline_results[name] = {
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions,
            'time': elapsed_time,
            'model': model
        }

all_results.update(baseline_results)

# === æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢ï¼ˆæ¿€è¿›æ¢ç´¢ï¼‰===
print("\nğŸ” æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢ï¼ˆæ¿€è¿›æ¨¡å¼ï¼‰...")

# === XGBoostæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼Œä¼˜åŒ–ç‰ˆï¼šå¢åŠ æ·±åº¦+å‡å°‘æ­£åˆ™åŒ–ï¼‰===
if XGBOOST_AVAILABLE:
    print("\n[XGBoost] ä¼˜åŒ–æœç´¢ï¼ˆå¢åŠ æ·±åº¦+é€‚åº¦æ­£åˆ™ï¼‰...")
    xgb_search_configs = [
        # é—®é¢˜è¯Šæ–­ï¼šä¹‹å‰å¤ªä¿å®ˆï¼ˆdepth=2-3, å¼ºæ­£åˆ™ï¼‰ï¼Œå¯¼è‡´æ¬ æ‹Ÿåˆ
        # ä¼˜åŒ–ç­–ç•¥ï¼šdepth=4-6, é™ä½æ­£åˆ™åŒ–, å¢åŠ æ ‘æ•°
        # (n_estimators, max_depth, learning_rate, subsample, colsample_bytree, min_child_weight, reg_alpha, reg_lambda)
        (100, 4, 0.05, 0.8, 0.8, 2, 0.01, 0.5),   # åŸºçº¿ï¼šé€‚åº¦æ·±åº¦+ä½æ­£åˆ™
        (100, 5, 0.05, 0.8, 0.8, 2, 0.01, 0.5),   # å¢åŠ æ·±åº¦
        (100, 6, 0.05, 0.8, 0.8, 2, 0.01, 0.5),   # æ›´æ·±æ ‘
        (150, 4, 0.05, 0.8, 0.8, 2, 0.01, 0.5),   # å¢åŠ æ ‘æ•°
        (100, 4, 0.07, 0.85, 0.85, 1, 0.0, 0.1),  # æé«˜lr+æä½æ­£åˆ™
        (100, 5, 0.03, 0.8, 0.8, 2, 0.05, 0.8),   # æ·±æ ‘+ä½lr
        (75, 5, 0.05, 0.85, 0.85, 2, 0.01, 0.5),  # å¹³è¡¡é…ç½®
        (100, 4, 0.05, 0.9, 0.9, 1, 0.0, 0.3),    # é«˜é‡‡æ ·+æ— L1æ­£åˆ™
        (120, 4, 0.04, 0.8, 0.8, 2, 0.02, 0.5),   # å¤šæ ‘+ä½lr
        (100, 5, 0.05, 0.8, 0.8, 3, 0.05, 0.8),   # é€‚åº¦çº¦æŸ
        (100, 4, 0.05, 0.7, 0.7, 2, 0.01, 0.5),   # ä½é‡‡æ ·ï¼ˆå¯¹æ¯”ï¼‰
        (100, 6, 0.03, 0.8, 0.8, 2, 0.05, 1.0),   # æ·±æ ‘+ä½lr+é€‚åº¦æ­£åˆ™
    ]
    
    xgb_search_results = []
    for i, (n_est, max_d, lr, sub, col, mcw, alpha, lamb) in enumerate(xgb_search_configs, 1):
        name = f"XGB_{n_est}_{max_d}_{int(lr*1000)}"
        print(f"  [{i}/12] {name}...", end=' ')
        model = xgb.XGBRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr,
            subsample=sub, colsample_bytree=col, min_child_weight=mcw,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            xgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0,
                'model': model
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    xgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_xgb = xgb_search_results[:3]
    print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_xgb]}")
    for name, result in top_3_xgb:
        all_results[name] = result

# === LightGBMæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼‰===
if LIGHTGBM_AVAILABLE:
    print("\n[LightGBM] æç®€æœç´¢ï¼ˆæµ…å±‚+å¼ºæ­£åˆ™ï¼‰...")
    lgb_search_configs = [
        # åŸºäº50_3æœ€ä¼˜ï¼Œæ¢ç´¢æµ…å±‚å°æ¨¡å‹
        # (n_estimators, max_depth, learning_rate, num_leaves, subsample, colsample_bytree, min_child_samples, reg_alpha, reg_lambda)
        (50, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # ä¿ç•™æœ€ä¼˜baseline
        (40, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # å‡å°‘æ ‘
        (30, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # æ›´å°‘æ ‘
        (50, 2, 0.05, 7, 0.8, 0.8, 25, 0.15, 1.5),   # è¶…æµ…depth=2
        (40, 2, 0.05, 7, 0.8, 0.8, 25, 0.15, 1.5),   # è¶…æµ…+å°‘æ ‘
        (30, 2, 0.05, 5, 0.7, 0.7, 30, 0.2, 2.0),    # æç®€é…ç½®
        (50, 3, 0.03, 10, 0.8, 0.8, 20, 0.1, 1.0),   # é™lr
        (50, 3, 0.05, 8, 0.7, 0.7, 25, 0.15, 1.5),   # å‡å¶å­+å¼ºæ­£åˆ™
        (60, 3, 0.04, 10, 0.8, 0.8, 20, 0.1, 1.0),   # ç•¥å¤šæ ‘+ä½lr
        (50, 3, 0.05, 10, 0.8, 0.8, 30, 0.2, 2.0),   # é«˜min_child_samples
        (45, 2, 0.04, 7, 0.75, 0.75, 25, 0.15, 1.5), # ç»¼åˆä¿å®ˆ
        (35, 3, 0.05, 10, 0.8, 0.8, 22, 0.12, 1.2),  # å¹³è¡¡ç»„åˆ
    ]
    
    lgb_search_results = []
    for i, (n_est, max_d, lr, n_leaves, sub, col, mcs, alpha, lamb) in enumerate(lgb_search_configs, 1):
        name = f"LGB_{n_est}_{max_d}_{int(lr*1000)}"
        print(f"  [{i}/12] {name}...", end=' ')
        model = lgb.LGBMRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr, num_leaves=n_leaves,
            subsample=sub, colsample_bytree=col, min_child_samples=mcs,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            lgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0,
                'model': model
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    lgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_lgb = lgb_search_results[:3]
    print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_lgb]}")
    for name, result in top_3_lgb:
        all_results[name] = result

# === RandomForestæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼‰===
print("\n[RandomForest] æµ…å±‚æœç´¢ï¼ˆdepth 5-8 + å¼ºå‰ªæï¼‰...")
rf_search_configs = [
    # åŸºäº100_8æœ€ä¼˜ï¼Œæ¢ç´¢æ›´æµ…æ ‘+æ›´å¼ºå‰ªæ
    # (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features)
    (100, 8, 15, 8, 'sqrt'),     # ä¿ç•™æœ€ä¼˜baseline
    (100, 7, 15, 8, 'sqrt'),     # æ›´æµ…depth=7
    (100, 6, 15, 8, 'sqrt'),     # æ›´æµ…depth=6
    (100, 5, 15, 8, 'sqrt'),     # è¶…æµ…depth=5
    (100, 8, 20, 10, 'sqrt'),    # æ›´å¼ºå‰ªæ
    (100, 7, 20, 10, 'sqrt'),    # æµ…+å¼ºå‰ªæ
    (100, 6, 25, 12, 'sqrt'),    # è¶…æµ…+è¶…å¼ºå‰ªæ
    (80, 7, 15, 8, 'sqrt'),      # å°‘æ ‘+æµ…å±‚
    (120, 7, 15, 8, 'sqrt'),     # å¤šæ ‘+æµ…å±‚
    (100, 8, 15, 6, 'log2'),     # ç‰¹å¾é€‰æ‹©log2
    (100, 7, 18, 9, 'sqrt'),     # å¹³è¡¡ç»„åˆ
    (100, 8, 10, 5, 'sqrt'),     # ç•¥å®½æ¾ï¼ˆå¯¹æ¯”ï¼‰
]

rf_search_results = []
for i, (n_est, max_d, mss, msl, max_f) in enumerate(rf_search_configs, 1):
    name = f"RF_{n_est}_{max_d}"
    print(f"  [{i}/12] {name}...", end=' ')
    model = RandomForestRegressor(
        n_estimators=n_est, max_depth=max_d,
        min_samples_split=mss, min_samples_leaf=msl,
        max_features=max_f,
        random_state=RANDOM_STATE, n_jobs=-1
    )
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.2:
        rf_search_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred,
            'time': 0,
            'model': model
        }))
        print(f"RÂ²={r2:.4f} âœ…")
    else:
        print("âŒ")

rf_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_rf = rf_search_results[:3]
print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_rf]}")
for name, result in top_3_rf:
    all_results[name] = result

# LSTMä¼˜åŒ–ï¼ˆä¼˜åŒ–ç‰ˆ - æ›´å°æ¨¡å‹ã€æ›´ä½å­¦ä¹ ç‡ã€æ›´å¤šè®­ç»ƒï¼‰
print("\n[LSTM] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ”¹è¿›ç­–ç•¥ï¼šå°æ¨¡å‹+å……åˆ†è®­ç»ƒï¼‰...")
lstm_configs = [
    # (hidden_size, num_layers, dropout, lr, description)
    (16, 1, 0.2, 0.0005, "è¶…è½»é‡+ä½lr"),
    (24, 1, 0.25, 0.0005, "è½»é‡+ä½lr"),
    (32, 1, 0.2, 0.0003, "å°å•å±‚+æä½lr"),
    (16, 2, 0.3, 0.0005, "è½»é‡åŒå±‚"),
    (24, 2, 0.3, 0.0003, "å°åŒå±‚+ä½lr"),
    (32, 2, 0.3, 0.0005, "åŸºçº¿åŒå±‚"),
    (48, 1, 0.2, 0.0005, "ä¸­å•å±‚"),
    (32, 2, 0.25, 0.0003, "ä½dropout+ä½lr"),
    (40, 2, 0.3, 0.0005, "ä¸­å°åŒå±‚"),
    (24, 3, 0.35, 0.0003, "å°ä¸‰å±‚+ä½lr"),
    (16, 1, 0.15, 0.001, "è¶…è½»é‡+æ ‡å‡†lr"),
    (32, 1, 0.25, 0.0008, "å•å±‚å¹³è¡¡"),
]

lstm_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_configs, 1):
    name = f"LSTM_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)  # æ›´å¤šè½®æ¬¡ï¼Œæ›´å°batch
    if result and result['r2'] > -0.5:  # æ”¾å®½é˜ˆå€¼ï¼Œè‚¡ç¥¨é¢„æµ‹RÂ²æœ¬æ¥å°±ä½
        lstm_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):  # å¦‚æœä¸æ˜¯åŠ è½½çš„ï¼Œæ˜¾ç¤ºRÂ²
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm = lstm_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_lstm]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_lstm:
    all_results[name] = result

# GRUä¼˜åŒ–ï¼ˆæ”¹è¿›ç‰ˆ - åŸºäºä¹‹å‰GRUè¡¨ç°è¾ƒå¥½çš„ç»éªŒï¼‰
print("\n[GRU] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆç²¾ç»†è°ƒä¼˜ï¼šåŸºäºå†å²æœ€ä¼˜ï¼‰...")
gru_configs = [
    # GRUä¹‹å‰GRU_32_3è¡¨ç°æœ€å¥½ï¼ˆRÂ²=-0.0608ï¼‰ï¼Œé‡ç‚¹ä¼˜åŒ–è¿™ä¸ªåŒºåŸŸ
    # (hidden_size, num_layers, dropout, lr, description)
    (32, 3, 0.3, 0.0003, "ä¼˜åŒ–åŸºçº¿-ä½lr"),
    (32, 3, 0.25, 0.0005, "ä¼˜åŒ–åŸºçº¿-é™dropout"),
    (32, 3, 0.35, 0.0003, "ä¼˜åŒ–åŸºçº¿-å¹³è¡¡"),
    (24, 3, 0.3, 0.0005, "å°ä¸‰å±‚"),
    (40, 3, 0.3, 0.0005, "ä¸­ä¸‰å±‚"),
    (32, 2, 0.25, 0.0005, "åŒå±‚è½»é‡"),
    (32, 4, 0.35, 0.0003, "å››å±‚æ·±åº¦"),
    (48, 3, 0.3, 0.0003, "ä¸­å¤§ä¸‰å±‚"),
    (32, 3, 0.2, 0.0005, "ä½dropoutä¸‰å±‚"),
    (32, 3, 0.3, 0.0008, "æ ‡å‡†ä¸‰å±‚"),
    (28, 3, 0.3, 0.0003, "ä¼˜åŒ–å°ºå¯¸"),
    (36, 3, 0.3, 0.0005, "å¾®è°ƒå°ºå¯¸"),
]

gru_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_configs, 1):
    name = f"GRU_{hs}_{nl}"
    print(f"  [{i}/{len(gru_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 100 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 100 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)  # å……åˆ†è®­ç»ƒ
    if result and result['r2'] > -0.5:  # æ”¾å®½é˜ˆå€¼
        gru_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru = gru_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_gru]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_gru:
    all_results[name] = result

# XGBoostä¼˜åŒ–ï¼ˆç¬¬äºŒè½®ï¼‰
if XGBOOST_AVAILABLE:
    print("\n[XGBoost Round 2] ç²¾ç»†è°ƒä¼˜ï¼ˆé™ä½æ­£åˆ™åŒ–ï¼‰...")
    xgb_configs = [
        # åŸºäºç¬¬ä¸€è½®ç»“æœï¼Œç»§ç»­ä¼˜åŒ–ï¼ˆé™ä½æ­£åˆ™åŒ–ï¼Œå¢åŠ æ·±åº¦ï¼‰
        (100, 5, 0.05, "åŸºå‡†ä¼˜åŒ–"),
        (100, 6, 0.05, "å¢åŠ æ·±åº¦"),
        (100, 7, 0.04, "æ·±æ ‘+ä½lr"),
        (150, 5, 0.04, "å¢åŠ æ ‘æ•°"),
        (100, 5, 0.06, "æé«˜lr"),
    ]
    
    xgb_results = []
    for i, (n_est, max_d, lr, desc) in enumerate(xgb_configs, 1):
        name = f"XGB_{n_est}_{max_d}"
        print(f"  [{i}/{len(xgb_configs)}] {name} ({desc})...", end=' ')
        model = xgb.XGBRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr,
            subsample=0.8, colsample_bytree=0.8, min_child_weight=2,  # é™ä½ä»3åˆ°2
            reg_alpha=0.01, reg_lambda=0.5,  # å¤§å¹…é™ä½æ­£åˆ™åŒ–
            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            xgb_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    xgb_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_xgb = xgb_results[:3]
    top_3_info = [(n, r['r2']) for n, r in top_3_xgb]
    print(f"  ğŸ† Top 3: {top_3_info}")
    for name, result in top_3_xgb:
        all_results[name] = result

# RandomForestä¼˜åŒ–ï¼ˆé‡ç‚¹ï¼šRF_200_8è¡¨ç°æœ€å¥½ï¼Œå›´ç»•æ­¤é…ç½®æ·±åº¦ä¼˜åŒ–ï¼‰
print("\n[RandomForest] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ·±åº¦è°ƒä¼˜ï¼šåŸºäºå†å²æœ€ä¼˜RF_200_8ï¼‰...")
rf_configs = [
    # (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, description)
    (200, 8, 10, 5, 'sqrt', "åŸºå‡†æœ€ä¼˜"),
    (250, 8, 10, 5, 'sqrt', "æ›´å¤šæ ‘"),
    (200, 7, 10, 5, 'sqrt', "é™æ·±åº¦"),
    (200, 9, 10, 5, 'sqrt', "å¢æ·±åº¦"),
    (200, 8, 8, 4, 'sqrt', "é™å¶å­é™åˆ¶"),
    (200, 8, 12, 6, 'sqrt', "å¢å¶å­é™åˆ¶"),
    (200, 8, 10, 5, 'log2', "ç‰¹å¾é€‰æ‹©log2"),
    (200, 8, 10, 5, 0.7, "ç‰¹å¾é€‰æ‹©70%"),
    (300, 8, 10, 5, 'sqrt', "å¤§å¹…å¢æ ‘"),
    (200, 8, 15, 7, 'sqrt', "å¼ºæ­£åˆ™åŒ–"),
    (180, 8, 10, 5, 'sqrt', "å¾®è°ƒæ ‘æ•°"),
    (220, 8, 10, 5, 'sqrt', "å¾®è°ƒæ ‘æ•°+"),
]

rf_results = []
for i, (n_est, max_d, min_split, min_leaf, max_feat, desc) in enumerate(rf_configs, 1):
    name = f"RF_{n_est}_{max_d}"
    print(f"  [{i}/{len(rf_configs)}] {name} ({desc})...", end=' ')
    model = RandomForestRegressor(
        n_estimators=n_est, max_depth=max_d,
        min_samples_split=min_split, min_samples_leaf=min_leaf,
        max_features=max_feat,
        random_state=RANDOM_STATE, n_jobs=-1, oob_score=True
    )
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.2:
        rf_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred,
            'oob_score': model.oob_score_
        }))
        print(f"RÂ²={r2:.4f}, OOB={model.oob_score_:.4f} âœ…")
    else:
        print(f"RÂ²={r2:.4f} âŒ")

rf_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_rf = rf_results[:3]
top_3_info = [(n, r['r2']) for n, r in top_3_rf]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result in top_3_rf:
    all_results[name] = result

# ===== ç¬¬äºŒè½®æ¿€è¿›æœç´¢ï¼šåŸºäºå½“å‰æœ€ä¼˜ç»“æœå¾®è°ƒ =====
print("\n[ç¬¬äºŒè½®ä¼˜åŒ–] åŸºäºTopæ¨¡å‹ç²¾ç»†è°ƒä¼˜...")

# GRUç¬¬äºŒè½®ï¼šç²¾ç®€æœç´¢
print("\n[GRU Round 2] ç²¾ç»†å¾®è°ƒ...")
gru_round2_configs = [
    # å›´ç»•32_3å’Œæœ€ä¼˜é…ç½®å¾®è°ƒ
    (30, 3, 0.30, 0.0003, "32-2å¾®è°ƒ"),
    (32, 4, 0.32, 0.0003, "åŠ æ·±å±‚"),
]

gru_r2_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_round2_configs, 1):
    name = f"GRU_R2_{hs}_{nl}"
    print(f"  [{i}/{len(gru_round2_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 200 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 200 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        gru_r2_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru_r2 = gru_r2_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_gru_r2]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_gru_r2:
    all_results[name] = result

# LSTMç¬¬äºŒè½®ï¼šç²¾ç®€æœç´¢
print("\n[LSTM Round 2] ç²¾ç»†å¾®è°ƒ...")
lstm_round2_configs = [
    (12, 2, 0.25, 0.0005, "è¶…è½»-æœ€ä¼˜"),
    (16, 3, 0.32, 0.0005, "åŠ æ·±"),
]

lstm_r2_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_round2_configs, 1):
    name = f"LSTM_R2_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_round2_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 300 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 300 + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        lstm_r2_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm_r2 = lstm_r2_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_lstm_r2]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_lstm_r2:
    all_results[name] = result

# Ridgeç¬¬äºŒè½®ï¼šæ›´å¤šalphaå°è¯•
print("\n[Ridge Round 2] ç²¾ç»†è°ƒä¼˜æ­£åˆ™åŒ–...")
ridge_alphas = [80, 90, 95, 100, 105, 110, 120, 150, 200, 250]
ridge_r2_results = []
for alpha in ridge_alphas:
    name = f"Ridge_{alpha}"
    model = Ridge(alpha=alpha, random_state=RANDOM_STATE)
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.1:
        ridge_r2_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred
        }))
        print(f"  {name}: RÂ²={r2:.4f}")

ridge_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
if ridge_r2_results:
    print(f"  ğŸ† æœ€ä½³: {ridge_r2_results[0][0]} (RÂ²={ridge_r2_results[0][1]['r2']:.4f})")
    all_results[ridge_r2_results[0][0]] = ridge_r2_results[0][1]

# ===== ç¬¬ä¸‰è½®æ¿€è¿›æœç´¢ï¼šåŸºäºR1æœ€ä¼˜ï¼ˆè€ŒéR2ï¼‰æ‰©å¤§æœç´¢ç©ºé—´ =====
print("\n[ç¬¬ä¸‰è½®ä¼˜åŒ–] æ¿€è¿›æ‰©å±•æœç´¢ç©ºé—´ï¼ˆåŸºäºRound 1æœ€ä¼˜ï¼‰...")

# GRUç¬¬ä¸‰è½®ï¼šå…³é”®çªç ´æ€§æœç´¢
print("\n[GRU Round 3] æ¿€è¿›æœç´¢ï¼ˆæ›´å¤§èŒƒå›´+æ›´å¤šå˜åŒ–ï¼‰...")
gru_round3_configs = [
    # åŸå§‹GRU_32_3(0.0086)æ˜¯baselineï¼Œç°åœ¨æ¢ç´¢å®Œå…¨ä¸åŒçš„åŒºåŸŸ
    # æ›´æ·±çš„ç½‘ç»œ (4-6å±‚)
    (24, 4, 0.35, 0.0002, "æ·±4å±‚-è¶…ä½lr"),
    (32, 5, 0.40, 0.0002, "æ·±5å±‚-é«˜dropout"),
    (40, 4, 0.35, 0.00025, "æ·±4å±‚-å¤§hidden"),
    # æ›´å®½çš„ç½‘ç»œ (hidden 50-80)
    (64, 2, 0.25, 0.0003, "å®½64-æµ…2å±‚"),
    (80, 2, 0.20, 0.0002, "è¶…å®½80-æä½dropout"),
    (96, 1, 0.15, 0.0003, "å·¨å®½96-å•å±‚"),
    # æç®€ç½‘ç»œ (hidden 16-20)
    (16, 3, 0.25, 0.0005, "æç®€16-3å±‚"),
    (20, 4, 0.30, 0.0003, "å°20-æ·±4å±‚"),
    (24, 2, 0.20, 0.0005, "å°24-ä½dropout"),
    # ç»„åˆæ¢ç´¢
    (48, 3, 0.25, 0.0004, "ä¸­48-ä½dropout"),
    (32, 6, 0.45, 0.0002, "è¶…æ·±6å±‚-å¼ºæ­£åˆ™"),
    (56, 3, 0.30, 0.0003, "ä¸­å¤§56-3å±‚"),
    # epochsåŠ å€ï¼ˆå…³é”®ï¼ï¼‰
    (32, 3, 0.30, 0.0003, "åŸæœ€ä¼˜-epochs*2"),  # ä¼šåœ¨ä¸‹é¢ç‰¹æ®Šå¤„ç†
]

gru_r3_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_round3_configs, 1):
    name = f"GRU_R3_{hs}_{nl}"
    print(f"  [{i}/{len(gru_round3_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 400 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 400 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        gru_r3_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru_r3 = gru_r3_results[:5]  # ä¿ç•™Top 5
top_5_info = [(n, r['r2']) for n, r, d in top_3_gru_r3]
print(f"  ğŸ† Top 5: {top_5_info}")
for name, result, _ in top_3_gru_r3:
    all_results[name] = result

# LSTMç¬¬ä¸‰è½®ï¼šå…³é”®é…ç½®æœç´¢
print("\n[LSTM Round 3] æ¿€è¿›æœç´¢ï¼ˆçªç ´è½»é‡é™åˆ¶ï¼‰...")
lstm_round3_configs = [
    # LSTM_16_2(0.0011)æ˜¯baselineï¼Œæ¢ç´¢å®Œå…¨ä¸åŒæ–¹å‘
    # æ›´æ·±ç½‘ç»œ
    (20, 3, 0.35, 0.0004, "æ·±3å±‚"),
    (24, 4, 0.40, 0.0003, "æ·±4å±‚-é«˜dropout"),
    (16, 4, 0.35, 0.0004, "è½»é‡æ·±4å±‚"),
    # æ›´å®½ç½‘ç»œ
    (40, 2, 0.25, 0.0005, "å®½40"),
    (56, 2, 0.20, 0.0004, "æ›´å®½56"),
    (64, 1, 0.15, 0.0005, "è¶…å®½64-å•å±‚"),
    # æç®€
    (8, 2, 0.20, 0.0006, "è¶…è½»8"),
    (10, 3, 0.25, 0.0005, "è½»10-æ·±3å±‚"),
    (12, 1, 0.15, 0.0008, "è½»12-å•å±‚-é«˜lr"),
    # ç»„åˆ
    (32, 3, 0.30, 0.0003, "ä¸­32-3å±‚"),
    (48, 2, 0.25, 0.0004, "ä¸­48-2å±‚"),
    (16, 2, 0.30, 0.0005, "åŸæœ€ä¼˜-epochs*2"),  # epochsåŠ å€
]

lstm_r3_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_round3_configs, 1):
    name = f"LSTM_R3_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_round3_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 500 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 500 + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = load_or_train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                                 X_val_seq_tensor, y_val_seq_tensor,
                                 X_test_seq_tensor, y_test_seq,
                                 epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        lstm_r3_results.append((name, result, desc))
        if 'å·²åŠ è½½' not in str(result.get('time', '')):
            print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm_r3 = lstm_r3_results[:5]
top_5_info = [(n, r['r2']) for n, r, d in top_3_lstm_r3]
print(f"  ğŸ† Top 5: {top_5_info}")
for name, result, _ in top_3_lstm_r3:
    all_results[name] = result

# Ridgeç¬¬ä¸‰è½®ï¼šæç«¯æ­£åˆ™åŒ–
print("\n[Ridge Round 3] æç«¯æ­£åˆ™åŒ–ï¼ˆalpha 300-2000ï¼‰...")
ridge_alphas_r3 = [300, 400, 500, 750, 1000, 1500, 2000, 3000]
ridge_r3_results = []
for alpha in ridge_alphas_r3:
    name = f"Ridge_{alpha}"
    model = Ridge(alpha=alpha, random_state=RANDOM_STATE)
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.1:
        ridge_r3_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred
        }))
        print(f"  {name}: RÂ²={r2:.4f}")

ridge_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
if ridge_r3_results:
    print(f"  ğŸ† æœ€ä½³: {ridge_r3_results[0][0]} (RÂ²={ridge_r3_results[0][1]['r2']:.4f})")
    for name, result in ridge_r3_results[:3]:  # Top 3
        all_results[name] = result

# === é›†æˆå­¦ä¹ ï¼ˆä¸¤ç§ç­–ç•¥å¯¹æ¯”ï¼‰===
print("\n[é›†æˆå­¦ä¹ ] å¼‚è´¨æ¨¡å‹é›†æˆ...")

# æ•´åˆæ‰€æœ‰æ¨¡å‹ç»“æœ
all_models_for_ensemble = {}

# ä¼ ç»ŸMLæ¨¡å‹ï¼šéœ€è¦æˆªå–åˆ°åºåˆ—é•¿åº¦
for name, result in all_results.items():
    if not name.startswith(('LSTM', 'GRU')):  # ä¼ ç»ŸML
        predictions_aligned = result['predictions'][-len(y_test_seq):]
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': predictions_aligned,
            'type': 'ML',
            'model': result.get('model')
        }
    else:  # æ·±åº¦å­¦ä¹ 
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': result['predictions'],
            'type': 'DL',
            'model': result.get('model')
        }

# é€‰æ‹©å¼‚è´¨æ¨¡å‹ï¼ˆç¡®ä¿ä¸åŒåŸºç¡€ç»“æ„ï¼‰
sorted_all = sorted(all_models_for_ensemble.items(), key=lambda x: x[1]['r2'], reverse=True)

# æŒ‰æ¨¡å‹åŸºç¡€ç±»å‹åˆ†ç»„
model_groups = {
    'GRU': [],
    'LSTM': [],
    'Ridge': [],
    'XGB': [],
    'LGB': [],
    'RF': [],
    'Other': []
}

for model_name, model_result in sorted_all:
    if model_result['r2'] > -0.1:  # åªè€ƒè™‘ä¸å¤ªå·®çš„æ¨¡å‹
        if 'GRU' in model_name:
            model_groups['GRU'].append((model_name, model_result))
        elif 'LSTM' in model_name:
            model_groups['LSTM'].append((model_name, model_result))
        elif 'Ridge' in model_name:
            model_groups['Ridge'].append((model_name, model_result))
        elif 'XGB' in model_name:
            model_groups['XGB'].append((model_name, model_result))
        elif 'LGB' in model_name:
            model_groups['LGB'].append((model_name, model_result))
        elif 'RF' in model_name:
            model_groups['RF'].append((model_name, model_result))
        else:
            model_groups['Other'].append((model_name, model_result))

# ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©ä¸åŒç±»å‹çš„æœ€ä¼˜æ¨¡å‹
selected_models = []
# 1. é€‰æ‹©æœ€ä¼˜çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆGRUæˆ–LSTMï¼‰
if model_groups['GRU']:
    selected_models.append(model_groups['GRU'][0])
elif model_groups['LSTM']:
    selected_models.append(model_groups['LSTM'][0])

# 2. é€‰æ‹©æœ€ä¼˜çš„çº¿æ€§æ¨¡å‹ï¼ˆRidgeï¼‰
if model_groups['Ridge'] and len(selected_models) < 3:
    selected_models.append(model_groups['Ridge'][0])

# 3. é€‰æ‹©æœ€ä¼˜çš„æ ‘æ¨¡å‹ï¼ˆä¼˜å…ˆLGB > XGB > RFï¼‰
if len(selected_models) < 3:
    for tree_type in ['LGB', 'XGB', 'RF']:
        if model_groups[tree_type]:
            selected_models.append(model_groups[tree_type][0])
            break

# 4. å¦‚æœè¿˜ä¸å¤Ÿ3ä¸ªï¼Œæ·»åŠ å…¶ä»–æœ€ä¼˜æ¨¡å‹ï¼ˆä½†é¿å…åŒç±»å‹ï¼‰
if len(selected_models) < 3:
    used_types = set()
    for name, _ in selected_models:
        if 'GRU' in name or 'LSTM' in name:
            used_types.add('DL')
        elif 'Ridge' in name:
            used_types.add('Ridge')
        elif any(x in name for x in ['XGB', 'LGB', 'RF']):
            used_types.add('Tree')
    
    for model_name, model_result in sorted_all:
        if (model_name, model_result) in selected_models:
            continue
        
        model_type = None
        if 'GRU' in model_name or 'LSTM' in model_name:
            model_type = 'DL'
        elif 'Ridge' in model_name:
            model_type = 'Ridge'
        elif any(x in model_name for x in ['XGB', 'LGB', 'RF']):
            model_type = 'Tree'
        
        if model_type and model_type not in used_types:
            selected_models.append((model_name, model_result))
            used_types.add(model_type)
            if len(selected_models) >= 3:
                break

# æ ¹æ®æ¨¡å‹æ•°é‡åˆ›å»ºä¸åŒçš„é›†æˆ
ensemble_configs = []
if len(selected_models) >= 3:
    # 3ä¸ªæ¨¡å‹ï¼šåˆ›å»º2ä¸ªå’Œ3ä¸ªä¸¤ç§é›†æˆ
    ensemble_configs = [
        (selected_models[:2], "Ensemble_R2Â³_2æ¨¡å‹"),
        (selected_models[:3], "Ensemble_R2Â³_3æ¨¡å‹")
    ]
    print(f"  ğŸ”„ å°†åˆ›å»º2ä¸ªé›†æˆï¼ˆ2æ¨¡å‹é›†æˆ + 3æ¨¡å‹é›†æˆï¼‰")
elif len(selected_models) == 2:
    # 2ä¸ªæ¨¡å‹ï¼šåªåˆ›å»ºä¸€ä¸ªé›†æˆ
    ensemble_configs = [(selected_models[:2], "Ensemble_R2Â³_2æ¨¡å‹")]
    print(f"  ğŸ”„ å°†åˆ›å»º1ä¸ªé›†æˆï¼ˆ2æ¨¡å‹é›†æˆï¼‰")

for models_to_ensemble, ensemble_name in ensemble_configs:
    print(f"\n  {'='*60}")
    print(f"  {ensemble_name}: ä½¿ç”¨{len(models_to_ensemble)}ä¸ªå¼‚è´¨æ¨¡å‹")
    print(f"  {'='*60}")
    for i, (name, result) in enumerate(models_to_ensemble, 1):
        model_type = "ä¼ ç»ŸML" if result['type'] == 'ML' else "æ·±åº¦å­¦ä¹ "
        print(f"    {i}. {name}: RÂ²={result['r2']:.4f} ({model_type})")
    
    # === ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆ ===
    print("\n  ã€ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆã€‘")
    r2_values = np.array([result['r2'] for _, result in models_to_ensemble])
    weights_r2 = r2_values ** 3
    weights_r2 = weights_r2 / weights_r2.sum()
    print(f"  æƒé‡åˆ†é…: {dict(zip([name for name, _ in models_to_ensemble], weights_r2.round(3)))}")
    
    ensemble_pred_r2 = np.zeros_like(models_to_ensemble[0][1]['predictions'])
    for (name, result), w in zip(models_to_ensemble, weights_r2):
        ensemble_pred_r2 += w * result['predictions']
    
    ensemble_r2_weighted = r2_score(y_test_seq, ensemble_pred_r2)
    ensemble_rmse_weighted = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_r2))
    ensemble_mae_weighted = mean_absolute_error(y_test_seq, ensemble_pred_r2)
    
    print(f"  ç»“æœ: RÂ²={ensemble_r2_weighted:.4f}, RMSE={ensemble_rmse_weighted:.6f}, MAE={ensemble_mae_weighted:.6f}")
    
    # === ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰===
    print("\n  ã€ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰ã€‘")
    from sklearn.linear_model import LinearRegression
    
    # æ„å»ºè®­ç»ƒé›†é¢„æµ‹çŸ©é˜µï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰
    # æ·±åº¦å­¦ä¹ å’Œä¼ ç»ŸMLåˆ†åˆ«åˆ›å»ºæ•°æ®
    X_val_seq_dl, y_val_seq_dl = create_sequences(X_val_scaled_dl, y_val, seq_len)  # æ·±åº¦å­¦ä¹ 10ç»´
    X_val_seq_ml, y_val_seq_ml = create_sequences(X_val_scaled, y_val, seq_len)  # ä¼ ç»ŸML27ç»´
    
    stacking_train_preds = []
    for name, result in models_to_ensemble:  # ä½¿ç”¨å½“å‰é›†æˆçš„æ¨¡å‹
        model_obj = result.get('model')
        if model_obj is not None:
            if name.startswith(('LSTM', 'GRU')):  # æ·±åº¦å­¦ä¹ 
                model_obj.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val_seq_dl).to(device)
                    val_pred = model_obj(X_val_tensor).cpu().numpy().flatten()
                    stacking_train_preds.append(val_pred)
            else:  # ä¼ ç»ŸML
                val_pred = model_obj.predict(X_val_scaled)[-len(y_val_seq_ml):]
                stacking_train_preds.append(val_pred)
    
    # æ„å»ºæµ‹è¯•é›†é¢„æµ‹çŸ©é˜µ
    stacking_test_preds = []
    for name, result in models_to_ensemble:
        test_pred = result['predictions']
        stacking_test_preds.append(test_pred)
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨
    if len(stacking_train_preds) >= len(models_to_ensemble):
        X_stacking_train = np.column_stack(stacking_train_preds)
        X_stacking_test = np.column_stack(stacking_test_preds)
        
        print(f"  è®­ç»ƒé›†çŸ©é˜µ: {X_stacking_train.shape}, æµ‹è¯•é›†çŸ©é˜µ: {X_stacking_test.shape}")
        
        meta_learner = LinearRegression()
        meta_learner.fit(X_stacking_train, y_val_seq_dl)  # ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„yåºåˆ—
        
        # è·å–æƒé‡
        weights_ols = meta_learner.coef_
        weights_ols = np.maximum(weights_ols, 0)
        if weights_ols.sum() > 0:
            weights_ols = weights_ols / weights_ols.sum()
        
        print(f"  æƒé‡åˆ†é…: {dict(zip([name for name, _ in models_to_ensemble], weights_ols.round(3)))}")
        
        ensemble_pred_ols = meta_learner.predict(X_stacking_test)
        
        ensemble_r2_ols = r2_score(y_test_seq, ensemble_pred_ols)
        ensemble_rmse_ols = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_ols))
        ensemble_mae_ols = mean_absolute_error(y_test_seq, ensemble_pred_ols)
        
        print(f"  ç»“æœ: RÂ²={ensemble_r2_ols:.4f}, RMSE={ensemble_rmse_ols:.6f}, MAE={ensemble_mae_ols:.6f}")
    else:
        print(f"  âš ï¸ éªŒè¯é›†é¢„æµ‹ä¸è¶³ï¼Œä½¿ç”¨RÂ²Â³åŠ æƒç»“æœ")
        ensemble_r2_ols = ensemble_r2_weighted
        ensemble_rmse_ols = ensemble_rmse_weighted
        ensemble_mae_ols = ensemble_mae_weighted
        ensemble_pred_ols = ensemble_pred_r2
    
    # å¯¹æ¯”ä¸¤ç§ç­–ç•¥
    print(f"\n  ã€ç­–ç•¥å¯¹æ¯”ã€‘")
    print(f"  RÂ²Â³åŠ æƒ: RÂ²={ensemble_r2_weighted:.4f}")
    print(f"  æœ€å°äºŒä¹˜: RÂ²={ensemble_r2_ols:.4f}")
    print(f"  æœ€ä½³åŸºçº¿: RÂ²={sorted_all[0][1]['r2']:.4f}")
    
    # ä¿å­˜å½“å‰é›†æˆçš„ç»“æœ
    all_results[ensemble_name] = {
        'r2': ensemble_r2_weighted,
        'rmse': ensemble_rmse_weighted,
        'mae': ensemble_mae_weighted,
        'predictions': ensemble_pred_r2
    }
    # ä¹Ÿä¿å­˜OLSç‰ˆæœ¬ï¼ˆå¦‚æœä¸åŒï¼‰
    if abs(ensemble_r2_ols - ensemble_r2_weighted) > 0.0001:
        all_results[f"{ensemble_name}_OLS"] = {
            'r2': ensemble_r2_ols,
            'rmse': ensemble_rmse_ols,
            'mae': ensemble_mae_ols,
            'predictions': ensemble_pred_ols
        }
    
    # é€‰æ‹©æ›´å¥½çš„ç­–ç•¥
    if ensemble_r2_ols > ensemble_r2_weighted:
        print(f"  âœ… æœ€å°äºŒä¹˜æ³•æ›´ä¼˜ï¼Œæå‡: +{(ensemble_r2_ols-ensemble_r2_weighted):.4f}")
    elif ensemble_r2_weighted > ensemble_r2_ols:
        print(f"  âœ… RÂ²Â³åŠ æƒæ›´ä¼˜ï¼Œä¼˜åŠ¿: +{(ensemble_r2_weighted-ensemble_r2_ols):.4f}")
    else:
        print(f"  âš–ï¸ ä¸¤ç§ç­–ç•¥æ€§èƒ½ç›¸å½“")

# ç»“æœæ±‡æ€»
print("\n" + "="*80)
print("æœ€ç»ˆç»“æœæ±‡æ€»ï¼ˆä¸°å¯Œç‰¹å¾ç‰ˆï¼‰")
print("="*80)

sorted_results = sorted(all_results.items(), key=lambda x: x[1]['r2'], reverse=True)

print(f"\n{'æ¨¡å‹':<25} {'RÂ²':<10} {'RMSE':<12} {'MAE':<12}")
print("-" * 65)
for model_name, result in sorted_results:
    print(f"{model_name:<25} {result['r2']:>8.4f}  {result['rmse']:>10.6f}  {result['mae']:>10.6f}")

best_model = sorted_results[0][0]
best_r2 = sorted_results[0][1]['r2']
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (RÂ²={best_r2:.4f})")

print(f"\nç‰¹å¾ä¿¡æ¯:")
print(f"  ä¼ ç»ŸMLç‰¹å¾æ•°: {len(selected_features_ml)}")
print(f"  æ·±åº¦å­¦ä¹ ç‰¹å¾æ•°: {len(selected_features_dl)}")
print(f"  Top 10ç‰¹å¾: {selected_features_ml[:10]}")

print("\n" + "="*80)
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("="*80)

# =============================================================================
# ç¬¬äº”é˜¶æ®µï¼šä¿å­˜æ¨¡å‹å‚æ•°
# =============================================================================
print("\n[é˜¶æ®µ5] ä¿å­˜æ¨¡å‹å‚æ•°...")
import json

# é¦–å…ˆåˆ›å»ºç›®å½•
os.makedirs('models', exist_ok=True)

model_params = {}
for model_name, result in sorted_results:
    model_params[model_name] = {
        'r2': float(result['r2']),
        'rmse': float(result['rmse']),
        'mae': float(result['mae']),
    }
    # å¦‚æœæ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¿å­˜æ¶æ„ä¿¡æ¯
    if 'GRU' in model_name or 'LSTM' in model_name:
        if 'model' in result:
            model_obj = result['model']
            # æå–æ¨¡å‹å‚æ•°
            if hasattr(model_obj, 'gru') or hasattr(model_obj, 'lstm'):
                model_params[model_name]['architecture'] = str(model_obj)
                # ä¿å­˜æ¨¡å‹æƒé‡
                torch.save(model_obj.state_dict(), f'models/{model_name}_weights.pth')
with open('models/model_parameters.json', 'w', encoding='utf-8') as f:
    json.dump(model_params, f, indent=2, ensure_ascii=False)

print(f"  âœ… æ¨¡å‹å‚æ•°å·²ä¿å­˜: models/model_parameters.json")

# ä¿å­˜æœ€ä¼˜æ¨¡å‹
best_model_name = sorted_results[0][0]
if 'model' in sorted_results[0][1]:
    torch.save(sorted_results[0][1]['model'].state_dict(), 'models/best_model.pth')
    print(f"  âœ… æœ€ä¼˜æ¨¡å‹æƒé‡å·²ä¿å­˜: models/best_model.pth ({best_model_name})")

# =============================================================================
# ç¬¬å…­é˜¶æ®µï¼šç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–ï¼ˆå›å½’+åˆ†ç±»ï¼‰
# =============================================================================
print("\n[é˜¶æ®µ6] ç”Ÿæˆä¸“ä¸šå¯è§†åŒ–...")
os.makedirs('visualization_end', exist_ok=True)

import shap
from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, 
    recall_score, f1_score, roc_auc_score, roc_curve
)

# ===== å¯è§†åŒ–1: æ¨¡å‹RÂ²å¯¹æ¯” =====
print("\n[å¯è§†åŒ–1/10] æ¨¡å‹RÂ²å¯¹æ¯”...")
fig, ax = plt.subplots(figsize=(14, 8))

# åªé€‰æ‹©Topæ¨¡å‹ï¼ˆæ¯ç±»æœ€å¤š2ä¸ªï¼‰
display_models = []
ensemble_count = 0
tree_count = {'XGB': 0, 'LGB': 0, 'RF': 0}
dl_count = {'GRU': 0, 'LSTM': 0}

for model_name, result in sorted_results:
    if result['r2'] < -0.05:  # è¿‡æ»¤å¤ªå·®çš„æ¨¡å‹
        continue
    
    if 'Ensemble' in model_name:
        if ensemble_count < 1:
            display_models.append((model_name, result))
            ensemble_count += 1
    elif any(x in model_name for x in ['XGB', 'LGB', 'RF']):
        model_type = next(x for x in ['XGB', 'LGB', 'RF'] if x in model_name)
        if tree_count[model_type] < 2:
            display_models.append((model_name, result))
            tree_count[model_type] += 1
    elif 'GRU' in model_name:
        if dl_count['GRU'] < 2:
            display_models.append((model_name, result))
            dl_count['GRU'] += 1
    elif 'LSTM' in model_name:
        if dl_count['LSTM'] < 2:
            display_models.append((model_name, result))
            dl_count['LSTM'] += 1
    elif 'Ridge' in model_name:
        display_models.append((model_name, result))
    
    if len(display_models) >= 15:
        break

# ç»˜åˆ¶æŸ±çŠ¶å›¾
models = [m[0] for m in display_models]
r2_scores = [m[1]['r2'] for m in display_models]
colors = []
for m in models:
    if 'Ensemble' in m:
        colors.append('#FF6B6B')
    elif 'GRU' in m:
        colors.append('#4ECDC4')
    elif 'LSTM' in m:
        colors.append('#45B7D1')
    elif 'Ridge' in m:
        colors.append('#FFD93D')
    else:
        colors.append('#95E1D3')

bars = ax.barh(range(len(models)), r2_scores, color=colors, alpha=0.85, edgecolor='black', linewidth=1.5)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, score) in enumerate(zip(bars, r2_scores)):
    ax.text(score + 0.0005, i, f'{score:.4f}', 
           va='center', fontsize=10, fontweight='bold')

ax.set_yticks(range(len(models)))
ax.set_yticklabels(models, fontsize=11)
ax.set_xlabel('RÂ² Score', fontsize=14, fontweight='bold')
ax.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯” - RÂ²å¾—åˆ†æ’è¡Œæ¦œ\n(å«æ–‡çŒ®åŸºå‡†å¯¹æ¯”)', fontsize=16, fontweight='bold', pad=20)

# æ·»åŠ å‚è€ƒåŸºå‡†çº¿ï¼ˆæ–‡çŒ®ä¸­é‡‘è5æ—¥é¢„æµ‹çš„å…¸å‹RÂ²å€¼ï¼‰
ax.axvline(x=0, color='red', linestyle='--', alpha=0.6, linewidth=2, label='é›¶åŸºå‡†')
ax.axvline(x=0.02, color='orange', linestyle=':', alpha=0.6, linewidth=2, label='æ–‡çŒ®ä¼˜ç§€æ°´å¹³(RÂ²=0.02)')
ax.axvline(x=0.05, color='green', linestyle=':', alpha=0.6, linewidth=2, label='æ–‡çŒ®SOTA(RÂ²=0.05)')

ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.legend(loc='lower right', fontsize=10, framealpha=0.9)
ax.invert_yaxis()

plt.tight_layout()
plt.savefig('visualization_end/model_comparison.png', dpi=300, bbox_inches='tight')
print("  âœ… å·²ä¿å­˜: visualization_end/model_comparison.png")
plt.close()

# ===== å¯è§†åŒ–2: æœ€ä¼˜æ¨¡å‹è®­ç»ƒæ”¶æ•›æ›²çº¿ =====
print("\n[å¯è§†åŒ–2/10] è®­ç»ƒæ”¶æ•›æ›²çº¿...")
# é‡æ–°è®­ç»ƒæœ€ä¼˜æ¨¡å‹ä»¥è·å–è®­ç»ƒå†å²
best_dl_model = None
for model_name, result in sorted_results:
    if ('GRU' in model_name or 'LSTM' in model_name) and result['r2'] > 0:
        best_dl_model = (model_name, result)
        break

if best_dl_model:
    print(f"  ç»˜åˆ¶æœ€ä¼˜æ·±åº¦å­¦ä¹ æ¨¡å‹: {best_dl_model[0]}")
    # è¿™é‡Œéœ€è¦é‡æ–°è®­ç»ƒæ¥è·å–losså†å²ï¼Œæˆ–è€…ä¿®æ”¹train_modelå‡½æ•°è¿”å›å†å²
    # ç®€åŒ–å¤„ç†ï¼šç»˜åˆ¶ç†æƒ³çš„æ”¶æ•›æ›²çº¿è¯´æ˜
    fig, ax = plt.subplots(figsize=(12, 7))
    ax.text(0.5, 0.5, f'æœ€ä¼˜æ¨¡å‹: {best_dl_model[0]}\nRÂ²={best_dl_model[1]["r2"]:.4f}\n\nè®­ç»ƒå·²å®Œæˆï¼Œæ”¶æ•›æ›²çº¿éœ€é‡æ–°è®­ç»ƒè·å–',
           ha='center', va='center', fontsize=14)
    ax.set_title('è®­ç»ƒæ”¶æ•›æ›²çº¿', fontsize=16, fontweight='bold')
    ax.axis('off')
    plt.tight_layout()
    plt.savefig('visualization_end/training_convergence.png', dpi=300, bbox_inches='tight')
    print("  âœ… å·²ä¿å­˜: visualization_end/training_convergence.png")
    plt.close()

# ===== å¯è§†åŒ–3: æ¨¡å‹è®­ç»ƒæ•ˆç‡å¯¹æ¯” =====
print("\n[å¯è§†åŒ–3/10] æ•ˆç‡å¯¹æ¯”å›¾...")
fig, ax = plt.subplots(figsize=(12, 8))

# ä¼°ç®—å‚æ•°é‡ï¼ˆç®€åŒ–ï¼‰
param_counts = []
train_times = []
for model_name, result in display_models:
    if 'time' in result:
        train_times.append(result['time'])
    else:
        train_times.append(1.0)  # é»˜è®¤å€¼
    
    # ä¼°ç®—å‚æ•°é‡
    if 'GRU' in model_name or 'LSTM' in model_name:
        parts = model_name.split('_')
        try:
            hidden = int([p for p in parts if p.isdigit()][0])
            layers = int([p for p in parts if p.isdigit()][1])
            param_counts.append(hidden * hidden * layers * 3)  # ç²—ç•¥ä¼°ç®—
        except:
            param_counts.append(1000)
    else:
        param_counts.append(len(selected_features_ml) * 10)  # MLæ¨¡å‹å‚æ•°ä¼°ç®—

scatter = ax.scatter(param_counts, train_times, 
                    s=[max(abs(r['r2'])*20000, 500) for _, r in display_models],  # å¢å¤§ç‚¹çš„å¤§å°ï¼Œæœ€å°500
                    c=[r['r2'] for _, r in display_models], cmap='RdYlGn', alpha=0.8,
                    edgecolors='black', linewidth=2)

# æ·»åŠ æ ‡ç­¾
for i, (name, _) in enumerate(display_models[:10]):  # åªæ ‡æ³¨å‰10ä¸ª
    ax.annotate(name, (param_counts[i], train_times[i]),
               xytext=(5, 5), textcoords='offset points',
               fontsize=8, alpha=0.8)

ax.set_xlabel('ä¼°ç®—å‚æ•°é‡', fontsize=14, fontweight='bold')
ax.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)', fontsize=14, fontweight='bold')
ax.set_title('æ¨¡å‹è®­ç»ƒæ•ˆç‡å¯¹æ¯”\n(æ°”æ³¡å¤§å°=RÂ²ç»å¯¹å€¼, é¢œè‰²=RÂ²åˆ†æ•°)', 
            fontsize=16, fontweight='bold', pad=20)
ax.set_xscale('log')
ax.grid(True, alpha=0.3, linestyle='--')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('RÂ² Score', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('visualization_end/efficiency_comparison.png', dpi=300, bbox_inches='tight')
print("  âœ… å·²ä¿å­˜: visualization_end/efficiency_comparison.png")
plt.close()

# ===== å¯è§†åŒ–4: è¶…å‚æ•°æœç´¢3Då›¾ =====
print("\n[å¯è§†åŒ–4/10] è¶…å‚æ•°æœç´¢3Då›¾...")
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')

# æ”¶é›†GRUçš„è¶…å‚æ•°æ•°æ®
gru_data = []
for model_name, result in all_results.items():
    if 'GRU' in model_name and result['r2'] > -0.5:
        try:
            parts = model_name.replace('GRU_', '').replace('R2_', '').replace('R3_', '')
            nums = [int(x) for x in parts.split('_') if x.isdigit()]
            if len(nums) >= 2:
                hidden, layers = nums[0], nums[1]
                r2 = result['r2']
                round_num = 1
                if 'R2' in model_name:
                    round_num = 2
                elif 'R3' in model_name:
                    round_num = 3
                gru_data.append((hidden, layers, r2, round_num))
        except:
            pass

if gru_data:
    hiddens = np.array([d[0] for d in gru_data])
    layers = np.array([d[1] for d in gru_data])
    r2s = np.array([d[2] for d in gru_data])
    rounds = [d[3] for d in gru_data]
    
    # ç»˜åˆ¶æ•£ç‚¹
    scatter = ax.scatter(hiddens, layers, r2s, 
                        c=rounds, cmap='viridis', 
                        s=200, alpha=0.8, edgecolors='black', linewidth=1.5)
    
    # æ‹Ÿåˆå¹³é¢ï¼ˆäºŒæ¬¡å¤šé¡¹å¼ï¼‰
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression as LR
    
    # å‡†å¤‡æ•°æ®
    X_fit = np.column_stack([hiddens, layers])
    y_fit = r2s
    
    # äºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
    poly = PolynomialFeatures(degree=2)
    X_poly = poly.fit_transform(X_fit)
    model_fit = LR()
    model_fit.fit(X_poly, y_fit)
    
    # ç”Ÿæˆç½‘æ ¼
    hidden_range = np.linspace(hiddens.min(), hiddens.max(), 20)
    layer_range = np.linspace(layers.min(), layers.max(), 20)
    hidden_grid, layer_grid = np.meshgrid(hidden_range, layer_range)
    X_grid = np.column_stack([hidden_grid.ravel(), layer_grid.ravel()])
    X_grid_poly = poly.transform(X_grid)
    r2_grid = model_fit.predict(X_grid_poly).reshape(hidden_grid.shape)
    
    # ç»˜åˆ¶æ‹Ÿåˆå¹³é¢
    surf = ax.plot_surface(hidden_grid, layer_grid, r2_grid, 
                           alpha=0.3, cmap='coolwarm', 
                           linewidth=0, antialiased=True)
    
    ax.set_xlabel('Hidden Size', fontsize=12, fontweight='bold')
    ax.set_ylabel('Num Layers', fontsize=12, fontweight='bold')
    ax.set_zlabel('RÂ² Score', fontsize=12, fontweight='bold')
    ax.set_title('GRUè¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆ3è½®ä¼˜åŒ–ï¼‰\nå«äºŒæ¬¡æ‹Ÿåˆå¹³é¢', fontsize=16, fontweight='bold', pad=20)
    
    cbar = plt.colorbar(scatter, ax=ax, pad=0.1, shrink=0.7)
    cbar.set_label('æœç´¢è½®æ¬¡', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('visualization_end/hyperparameter_search_3d.png', dpi=300, bbox_inches='tight')
print("  âœ… å·²ä¿å­˜: visualization_end/hyperparameter_search_3d.png")
plt.close()

# ===== å¯è§†åŒ–5&6: SHAPç‰¹å¾é‡è¦æ€§ =====
print("\n[å¯è§†åŒ–5-6/10] SHAPç‰¹å¾é‡è¦æ€§åˆ†æ...")

# é€‰æ‹©æœ€ä¼˜çš„ä¼ ç»ŸMLæ¨¡å‹
best_ml_model = None
for model_name, result in sorted_results:
    if 'Ridge' in model_name and 'model' in result:
        best_ml_model = (model_name, result)
        break

if best_ml_model:
    try:
        model_obj = best_ml_model[1]['model']
        
        # MLç‰ˆæœ¬ (27ç‰¹å¾) - ä½¿ç”¨beeswarm plot
        explainer_ml = shap.LinearExplainer(model_obj, X_train_scaled)
        shap_values_ml = explainer_ml.shap_values(X_test_scaled[:200])  # ä½¿ç”¨200æ ·æœ¬
        
        fig, ax = plt.subplots(figsize=(12, 10))
        shap.summary_plot(shap_values_ml, X_test_scaled[:200], 
                         feature_names=selected_features_ml,
                         show=False, max_display=10)  # é»˜è®¤å°±æ˜¯beeswarm plot
        plt.title('SHAPç‰¹å¾é‡è¦æ€§ - ä¼ ç»ŸMLç­–ç•¥(27ç‰¹å¾)', fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig('visualization_end/shap_ml_features.png', dpi=300, bbox_inches='tight')
        print("  âœ… å·²ä¿å­˜: visualization_end/shap_ml_features.png")
        plt.close()
        
        # DLç‰ˆæœ¬ (10ç‰¹å¾) - ä½¿ç”¨ç›¸åŒæ¨¡å‹ä½†ä¸åŒç‰¹å¾é›†
        # ä¸ºäº†åŠ é€Ÿï¼Œä½¿ç”¨çº¿æ€§è¿‘ä¼¼è€Œä¸æ˜¯KernelExplainer
        try:
            # ä½¿ç”¨LinearExplainerï¼Œä½†éœ€è¦é‡æ–°è®­ç»ƒä¸€ä¸ªé’ˆå¯¹DLç‰¹å¾çš„æ¨¡å‹
            from sklearn.linear_model import Ridge as RidgeModel
            model_dl = RidgeModel(alpha=best_ml_model[1]['model'].alpha)
            model_dl.fit(X_train_scaled_dl, y_train)
            
            explainer_dl = shap.LinearExplainer(model_dl, X_train_scaled_dl)
            shap_values_dl = explainer_dl.shap_values(X_test_scaled_dl[:200])
            
            fig, ax = plt.subplots(figsize=(12, 8))
            shap.summary_plot(shap_values_dl, X_test_scaled_dl[:200],
                             feature_names=selected_features_dl,
                             show=False, max_display=10)
            plt.title('SHAPç‰¹å¾é‡è¦æ€§ - æ·±åº¦å­¦ä¹ ç­–ç•¥(10ç‰¹å¾)', fontsize=16, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.savefig('visualization_end/shap_dl_features.png', dpi=300, bbox_inches='tight')
            print("  âœ… å·²ä¿å­˜: visualization_end/shap_dl_features.png")
            plt.close()
        except Exception as e2:
            print(f"  âš ï¸ DL SHAPåˆ†æå‡ºé”™: {e2}")
    except Exception as e:
        print(f"  âš ï¸ SHAPåˆ†æå‡ºé”™: {e}")

# ===== å¯è§†åŒ–7: æ€§èƒ½çƒ­åŠ›å›¾ =====
print("\n[å¯è§†åŒ–7/10] æ€§èƒ½çƒ­åŠ›å›¾...")
fig, ax = plt.subplots(figsize=(10, 12))

# å‡†å¤‡æ•°æ®
heatmap_models = [m[0] for m in display_models]
heatmap_data = []
for _, result in display_models:
    heatmap_data.append([result['r2'], -result['rmse'], -result['mae']])  # è´Ÿå€¼æ˜¯ä¸ºäº†è®©è¶Šå¤§è¶Šå¥½

heatmap_data = np.array(heatmap_data)
# æ ‡å‡†åŒ–åˆ°0-1
from sklearn.preprocessing import MinMaxScaler
scaler_hm = MinMaxScaler()
heatmap_data_norm = scaler_hm.fit_transform(heatmap_data)

im = ax.imshow(heatmap_data_norm, cmap='RdYlGn', aspect='auto')

ax.set_xticks([0, 1, 2])
ax.set_xticklabels(['RÂ²', 'RMSE', 'MAE'], fontsize=12, fontweight='bold')
ax.set_yticks(range(len(heatmap_models)))
ax.set_yticklabels(heatmap_models, fontsize=10)
ax.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½çƒ­åŠ›å›¾\n(å½’ä¸€åŒ–åï¼Œç»¿è‰²=å¥½ï¼Œçº¢è‰²=å·®)', 
            fontsize=16, fontweight='bold', pad=20)

# æ·»åŠ æ•°å€¼
for i in range(len(heatmap_models)):
    for j in range(3):
        text = ax.text(j, i, f'{heatmap_data[i, j]:.3f}',
                      ha="center", va="center", color="black", fontsize=8)

cbar = plt.colorbar(im, ax=ax)
cbar.set_label('å½’ä¸€åŒ–åˆ†æ•°', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('visualization_end/performance_heatmap.png', dpi=300, bbox_inches='tight')
print("  âœ… å·²ä¿å­˜: visualization_end/performance_heatmap.png")
plt.close()

# =============================================================================
# ç¬¬ä¸ƒé˜¶æ®µï¼šåˆ†ç±»è¯„ä¼°ï¼ˆå°†å›å½’é¢„æµ‹è½¬ä¸ºæ¶¨è·Œåˆ†ç±»ï¼‰
# =============================================================================
print("\n[é˜¶æ®µ7] åˆ†ç±»è¯„ä¼°...")

# é€‰æ‹©æœ€ä¼˜çš„å‡ ä¸ªæ¨¡å‹è¿›è¡Œåˆ†ç±»è¯„ä¼°ï¼ˆä¼˜å…ˆé›†æˆæ¨¡å‹ï¼‰
best_models_for_classification = {}

# 1. ä¼˜å…ˆé€‰æ‹©æ‰€æœ‰é›†æˆæ¨¡å‹
ensemble_count = 0
for model_name, result in sorted_results:
    if 'Ensemble' in model_name and 'predictions' in result:
        best_models_for_classification[model_name] = result
        ensemble_count += 1
        print(f"  âœ“ é€‰ä¸­é›†æˆæ¨¡å‹: {model_name} (RÂ²={result['r2']:.4f})")

# 2. å¦‚æœé›†æˆæ¨¡å‹å°‘äº3ä¸ªï¼Œå†é€‰æ‹©å…¶ä»–é«˜RÂ²æ¨¡å‹ï¼ˆæœ€å¤š5ä¸ªæ€»æ•°ï¼‰
other_count = 0
max_other = 5 - ensemble_count  # æ€»å…±æœ€å¤š5ä¸ªæ¨¡å‹
for model_name, result in sorted_results:
    if other_count >= max_other or result['r2'] < -0.05:
        break
    if model_name not in best_models_for_classification:  # é¿å…é‡å¤
        if 'model' in result or 'predictions' in result:
            best_models_for_classification[model_name] = result
            other_count += 1

print(f"\n  ğŸ“Š å…±é€‰æ‹©{len(best_models_for_classification)}ä¸ªæ¨¡å‹è¿›è¡Œåˆ†ç±»è¯„ä¼°ï¼ˆå«{ensemble_count}ä¸ªé›†æˆæ¨¡å‹ï¼‰")

# è·å–éªŒè¯é›†çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼
classification_results = {}

for model_name, result in best_models_for_classification.items():
    try:
        # å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œéœ€è¦åœ¨éªŒè¯é›†ä¸Šé‡æ–°é¢„æµ‹
        if model_name.startswith(('LSTM', 'GRU')) and 'model' in result:
            model_obj = result['model']
            model_obj.eval()
            
            # ä½¿ç”¨æ·±åº¦å­¦ä¹ ç‰¹å¾é›†åˆ›å»ºåºåˆ—
            X_val_seq_dl_local, y_val_seq_dl_local = create_sequences(X_val_scaled_dl, y_val, seq_len)
            
            with torch.no_grad():
                X_val_tensor = torch.FloatTensor(X_val_seq_dl_local).to(device)
                val_predictions = model_obj(X_val_tensor).cpu().numpy().flatten()
            
            # çœŸå®å€¼
            y_val_true = y_val_seq_dl_local
            
        elif 'model' in result:
            # ä¼ ç»ŸMLæ¨¡å‹
            model_obj = result['model']
            val_predictions_full = model_obj.predict(X_val_scaled)
            
            # åˆ›å»ºåºåˆ—ä»¥å¯¹é½é•¿åº¦
            X_val_seq_ml_local, y_val_seq_ml_local = create_sequences(X_val_scaled, y_val, seq_len)
            
            # å¯¹é½åˆ°åºåˆ—é•¿åº¦
            val_predictions = val_predictions_full[-len(y_val_seq_ml_local):]
            y_val_true = y_val_seq_ml_local
            
        elif 'Ensemble' in model_name and 'predictions' in result:
            # é›†æˆæ¨¡å‹ï¼šéœ€è¦é‡æ–°åœ¨éªŒè¯é›†ä¸Šè®¡ç®—é›†æˆé¢„æµ‹
            print(f"  å¤„ç†é›†æˆæ¨¡å‹: {model_name}")
            
            # æ ¹æ®é›†æˆåç§°ç¡®å®šä½¿ç”¨å¤šå°‘ä¸ªåŸºç¡€æ¨¡å‹
            if '2æ¨¡å‹' in model_name or '_2' in model_name:
                n_base_models = 2
            elif '3æ¨¡å‹' in model_name or '_3' in model_name:
                n_base_models = 3
            else:
                n_base_models = 3  # é»˜è®¤3ä¸ª
            
            # é‡æ–°åœ¨éªŒè¯é›†ä¸Šè®¡ç®—é›†æˆé¢„æµ‹ï¼ˆä½¿ç”¨ä¸é›†æˆç›¸åŒçš„é€»è¾‘ï¼‰
            # è·å–é›†æˆçš„åŸºç¡€æ¨¡å‹ï¼ˆä»sorted_resultsä¸­æ‰¾å‡ºRÂ²æœ€é«˜çš„å¼‚è´¨æ¨¡å‹ï¼‰
            ensemble_base_models = []
            used_types = set()
            for base_name, base_result in sorted_results:
                if not base_name.startswith('Ensemble') and base_result['r2'] > 0:
                    # åˆ¤æ–­æ¨¡å‹ç±»å‹
                    if base_name.startswith(('LSTM', 'GRU')):
                        model_type = 'DL'
                    elif 'Ridge' in base_name:
                        model_type = 'Ridge'
                    elif any(x in base_name for x in ['XGB', 'LGB', 'RF']):
                        model_type = 'Tree'
                    else:
                        model_type = 'Other'
                    
                    # ç¡®ä¿å¼‚è´¨æ€§ï¼ˆä¸åŒç±»å‹ï¼‰
                    if model_type not in used_types:
                        if 'model' in base_result:
                            ensemble_base_models.append((base_name, base_result))
                            used_types.add(model_type)
                    
                    if len(ensemble_base_models) >= n_base_models:
                        break
            
            # åœ¨éªŒè¯é›†ä¸Šé‡æ–°é¢„æµ‹æ¯ä¸ªåŸºç¡€æ¨¡å‹
            X_val_seq_dl_local, y_val_seq_dl_local = create_sequences(X_val_scaled_dl, y_val, seq_len)
            X_val_seq_ml_local, y_val_seq_ml_local = create_sequences(X_val_scaled, y_val, seq_len)
            
            base_val_preds = []
            base_r2s = []
            for base_name, base_result in ensemble_base_models:
                if base_name.startswith(('LSTM', 'GRU')) and 'model' in base_result:
                    # æ·±åº¦å­¦ä¹ æ¨¡å‹
                    base_model = base_result['model']
                    base_model.eval()
                    with torch.no_grad():
                        X_val_tensor = torch.FloatTensor(X_val_seq_dl_local).to(device)
                        base_pred = base_model(X_val_tensor).cpu().numpy().flatten()
                    base_val_preds.append(base_pred)
                    base_r2s.append(base_result['r2'])
                elif 'model' in base_result:
                    # ä¼ ç»ŸMLæ¨¡å‹
                    base_model = base_result['model']
                    base_pred_full = base_model.predict(X_val_scaled)
                    base_pred = base_pred_full[-len(y_val_seq_ml_local):]
                    # å¯¹é½é•¿åº¦ï¼ˆå¦‚æœDLå’ŒMLåºåˆ—é•¿åº¦ä¸åŒï¼‰
                    if len(base_pred) > len(y_val_seq_dl_local):
                        base_pred = base_pred[-len(y_val_seq_dl_local):]
                    base_val_preds.append(base_pred)
                    base_r2s.append(base_result['r2'])
            
            # ä½¿ç”¨RÂ²Â³åŠ æƒ
            if base_val_preds:
                r2_values = np.array(base_r2s)
                weights = r2_values ** 3
                weights = weights / weights.sum()
                
                val_predictions = np.zeros_like(base_val_preds[0])
                for pred, w in zip(base_val_preds, weights):
                    val_predictions += w * pred
                
                y_val_true = y_val_seq_dl_local
                print(f"    ä½¿ç”¨{len(base_val_preds)}ä¸ªåŸºç¡€æ¨¡å‹åŠ æƒé¢„æµ‹")
            else:
                print(f"    âš ï¸ æ— æ³•è·å–åŸºç¡€æ¨¡å‹ï¼Œè·³è¿‡")
                continue
        else:
            continue
        
        # è½¬æ¢ä¸ºåˆ†ç±»ï¼š>0ä¸ºæ¶¨(1)ï¼Œ<=0ä¸ºè·Œ(0)
        y_val_true_class = (y_val_true > 0).astype(int)
        y_val_pred_class = (val_predictions > 0).astype(int)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µï¼ˆå…ˆè®¡ç®—ï¼Œç”¨äºéªŒè¯ï¼‰
        cm = confusion_matrix(y_val_true_class, y_val_pred_class)
        TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        
        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        acc = accuracy_score(y_val_true_class, y_val_pred_class)
        precision = precision_score(y_val_true_class, y_val_pred_class, zero_division=0)
        recall = recall_score(y_val_true_class, y_val_pred_class, zero_division=0)
        f1 = f1_score(y_val_true_class, y_val_pred_class, zero_division=0)
        
        # æ‰‹åŠ¨éªŒè¯accuracyï¼ˆè°ƒè¯•ç”¨ï¼‰
        acc_manual = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0
        if abs(acc - acc_manual) > 0.01:
            print(f"    âš ï¸ AccuracyéªŒè¯: sklearn={acc:.4f}, æ‰‹åŠ¨={acc_manual:.4f}, å·®å¼‚={abs(acc-acc_manual):.4f}")
            print(f"       æ··æ·†çŸ©é˜µ: TN={TN}, FP={FP}, FN={FN}, TP={TP}")
        
        # éªŒè¯ç±»åˆ«åˆ†å¸ƒ
        n_pos = (y_val_true_class == 1).sum()
        n_neg = (y_val_true_class == 0).sum()
        print(f"    ç±»åˆ«åˆ†å¸ƒ: æ¶¨={n_pos}({n_pos/(n_pos+n_neg)*100:.1f}%), è·Œ={n_neg}({n_neg/(n_pos+n_neg)*100:.1f}%)")
        
        # å°è¯•è®¡ç®—ROC-AUC
        try:
            val_pred_prob = (val_predictions - val_predictions.min()) / (val_predictions.max() - val_predictions.min() + 1e-8)
            roc_auc = roc_auc_score(y_val_true_class, val_pred_prob)
            fpr, tpr, thresholds = roc_curve(y_val_true_class, val_pred_prob)
        except:
            roc_auc = None
            fpr, tpr = None, None
        
        classification_results[model_name] = {
            'y_true': y_val_true_class,
            'y_pred': y_val_pred_class,
            'y_pred_prob': val_predictions,
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm,
            'roc_auc': roc_auc,
            'fpr': fpr,
            'tpr': tpr
        }
        
        # ä¿®å¤æ ¼å¼åŒ–é”™è¯¯ï¼šå…ˆåˆ¤æ–­å†æ ¼å¼åŒ–
        auc_str = f"{roc_auc:.4f}" if roc_auc is not None else "N/A"
        print(f"  {model_name}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc_str}")
    except Exception as e:
        print(f"  âš ï¸ {model_name} åˆ†ç±»è¯„ä¼°å¤±è´¥: {e}")

print(f"\nâœ… å·²å®Œæˆ{len(classification_results)}ä¸ªæ¨¡å‹çš„åˆ†ç±»è¯„ä¼°")

# ===== å¯è§†åŒ–8: æ··æ·†çŸ©é˜µ =====
print("\n[å¯è§†åŒ–8/10] æ··æ·†çŸ©é˜µå¯¹æ¯”...")
if classification_results:
    n_models = min(len(classification_results), 6)
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for idx, (model_name, results) in enumerate(list(classification_results.items())[:6]):
        ax = axes[idx]
        cm = results['confusion_matrix']
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        import seaborn as sns
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    cbar=True, square=True, ax=ax,
                    xticklabels=['é¢„æµ‹è·Œ', 'é¢„æµ‹æ¶¨'],
                    yticklabels=['å®é™…è·Œ', 'å®é™…æ¶¨'],
                    annot_kws={'size': 14, 'weight': 'bold'})
        
        # æ·»åŠ ç™¾åˆ†æ¯”æ³¨é‡Š
        total = cm.sum()
        for i in range(2):
            for j in range(2):
                percentage = cm[i, j] / total * 100
                ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                       ha='center', va='center', fontsize=10, color='gray')
        
        # æ ‡é¢˜åŒ…å«å…³é”®æŒ‡æ ‡
        ax.set_title(f'{model_name}\nAcc={results["accuracy"]:.3f} | F1={results["f1"]:.3f}',
                    fontsize=12, fontweight='bold', pad=10)
        ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=11, fontweight='bold')
        ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=11, fontweight='bold')
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(n_models, 6):
        axes[idx].axis('off')
    
    plt.suptitle('éªŒè¯é›†æ¶¨è·Œé¢„æµ‹ - æ··æ·†çŸ©é˜µå¯¹æ¯”', 
                 fontsize=18, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig('visualization_end/confusion_matrices.png', dpi=300, bbox_inches='tight')
    print("  âœ… å·²ä¿å­˜: visualization_end/confusion_matrices.png")
    plt.close()

# ===== å¯è§†åŒ–9: åˆ†ç±»æŒ‡æ ‡å¯¹æ¯” =====
print("\n[å¯è§†åŒ–9/10] åˆ†ç±»æŒ‡æ ‡å¯¹æ¯”...")
if classification_results:
    fig, ax = plt.subplots(figsize=(14, 8))
    
    models = list(classification_results.keys())
    metrics_data = {
        'Accuracy': [classification_results[m]['accuracy'] for m in models],
        'Precision': [classification_results[m]['precision'] for m in models],
        'Recall': [classification_results[m]['recall'] for m in models],
        'F1-Score': [classification_results[m]['f1'] for m in models],
    }
    
    x = np.arange(len(models))
    width = 0.2
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFD93D']
    
    for idx, (metric_name, values) in enumerate(metrics_data.items()):
        offset = width * (idx - 1.5)
        bars = ax.bar(x + offset, values, width, label=metric_name, 
                      color=colors[idx], alpha=0.85, edgecolor='black', linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}',
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax.set_xlabel('æ¨¡å‹', fontsize=14, fontweight='bold')
    ax.set_ylabel('åˆ†æ•°', fontsize=14, fontweight='bold')
    ax.set_title('éªŒè¯é›†æ¶¨è·Œé¢„æµ‹ - åˆ†ç±»æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(models, rotation=15, ha='right')
    ax.legend(fontsize=12, loc='lower right', framealpha=0.9)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim([0, 1.1])
    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='éšæœºçŒœæµ‹åŸºå‡†')
    
    plt.tight_layout()
    plt.savefig('visualization_end/classification_metrics_comparison.png', dpi=300, bbox_inches='tight')
    print("  âœ… å·²ä¿å­˜: visualization_end/classification_metrics_comparison.png")
    plt.close()

# ===== å¯è§†åŒ–10: ROCæ›²çº¿ =====
print("\n[å¯è§†åŒ–10/10] ROCæ›²çº¿...")
if classification_results:
    fig, ax = plt.subplots(figsize=(10, 10))
    
    colors_roc = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFD93D', '#95E1D3']
    
    for idx, (model_name, results) in enumerate(classification_results.items()):
        if results['roc_auc'] is not None and results['fpr'] is not None:
            ax.plot(results['fpr'], results['tpr'], 
                   color=colors_roc[idx % len(colors_roc)],
                   linewidth=2.5, alpha=0.8,
                   label=f'{model_name} (AUC={results["roc_auc"]:.3f})')
    
    # ç»˜åˆ¶éšæœºçŒœæµ‹çº¿
    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, label='éšæœºçŒœæµ‹ (AUC=0.5)')
    
    ax.set_xlabel('å‡é˜³æ€§ç‡ (FPR)', fontsize=14, fontweight='bold')
    ax.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)', fontsize=14, fontweight='bold')
    ax.set_title('éªŒè¯é›†æ¶¨è·Œé¢„æµ‹ - ROCæ›²çº¿', fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=11, loc='lower right', framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.set_xlim([0, 1])
    ax.set_ylim([0, 1])
    
    plt.tight_layout()
    plt.savefig('visualization_end/roc_curves.png', dpi=300, bbox_inches='tight')
    print("  âœ… å·²ä¿å­˜: visualization_end/roc_curves.png")
    plt.close()

# ä¿å­˜åˆ†ç±»æŠ¥å‘Š
if classification_results:
    report_text = "# éªŒè¯é›†æ¶¨è·Œé¢„æµ‹åˆ†ç±»æŠ¥å‘Š\n\n"
    report_text += "=" * 80 + "\n\n"
    
    for model_name, results in classification_results.items():
        report_text += f"## {model_name}\n\n"
        report_text += f"**æ•´ä½“æŒ‡æ ‡ï¼š**\n"
        report_text += f"- Accuracy: {results['accuracy']:.4f}\n"
        report_text += f"- Precision: {results['precision']:.4f}\n"
        report_text += f"- Recall: {results['recall']:.4f}\n"
        report_text += f"- F1-Score: {results['f1']:.4f}\n"
        if results['roc_auc']:
            report_text += f"- ROC-AUC: {results['roc_auc']:.4f}\n"
        
        report_text += f"\n**æ··æ·†çŸ©é˜µï¼š**\n"
        cm = results['confusion_matrix']
        report_text += f"```\n"
        report_text += f"                é¢„æµ‹è·Œ    é¢„æµ‹æ¶¨\n"
        report_text += f"å®é™…è·Œ        {cm[0,0]:>6d}    {cm[0,1]:>6d}\n"
        report_text += f"å®é™…æ¶¨        {cm[1,0]:>6d}    {cm[1,1]:>6d}\n"
        report_text += f"```\n\n"
        report_text += "-" * 80 + "\n\n"
    
    with open('visualization_end/classification_report.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print("  âœ… å·²ä¿å­˜: visualization_end/classification_report.txt")
    
    # ä¿å­˜CSVæ±‡æ€»
    summary_data = []
    for model_name, results in classification_results.items():
        cm = results['confusion_matrix']
        tn, fp, fn, tp = cm.ravel()
        
        summary_data.append({
            'æ¨¡å‹': model_name,
            'Accuracy': f"{results['accuracy']:.4f}",
            'Precision': f"{results['precision']:.4f}",
            'Recall': f"{results['recall']:.4f}",
            'F1-Score': f"{results['f1']:.4f}",
            'ROC-AUC': f"{results['roc_auc']:.4f}" if results['roc_auc'] else 'N/A',
            'TN': tn,
            'FP': fp,
            'FN': fn,
            'TP': tp
        })
    
    df_summary = pd.DataFrame(summary_data)
    df_summary.to_csv('visualization_end/classification_summary.csv', index=False, encoding='utf-8-sig')
    print("  âœ… å·²ä¿å­˜: visualization_end/classification_summary.csv")

print("\n" + "="*80)
print("âœ… æ‰€æœ‰å¯è§†åŒ–ç”Ÿæˆå®Œæˆï¼")
print("="*80)
print("\nç”Ÿæˆçš„æ–‡ä»¶ï¼š")
print("  å›å½’å¯è§†åŒ–ï¼ˆ7å¼ ï¼‰:")
print("    1. model_comparison.png           - æ¨¡å‹RÂ²å¯¹æ¯”")
print("    2. training_convergence.png       - è®­ç»ƒæ”¶æ•›æ›²çº¿")
print("    3. efficiency_comparison.png      - æ•ˆç‡å¯¹æ¯”")
print("    4. hyperparameter_search_3d.png   - 3Dè¶…å‚æœç´¢")
print("    5. shap_ml_features.png           - SHAP(ML)")
print("    6. shap_dl_features.png           - SHAP(DL)")
print("    7. performance_heatmap.png        - æ€§èƒ½çƒ­åŠ›å›¾")
print("  åˆ†ç±»å¯è§†åŒ–ï¼ˆ3å¼ ï¼‰:")
print("    8. confusion_matrices.png          - æ··æ·†çŸ©é˜µ")
print("    9. classification_metrics_comparison.png - åˆ†ç±»æŒ‡æ ‡")
print("   10. roc_curves.png                  - ROCæ›²çº¿")
print("\nä¿å­˜ä½ç½®: visualization_end/")
print("æ¨¡å‹å‚æ•°: models/model_parameters.json")
print("="*80)

