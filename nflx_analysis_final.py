# -*- coding: utf-8 -*-
"""
Netflixè‚¡ç¥¨é¢„æµ‹ - æœ€ç»ˆç‰ˆï¼ˆå‚è€ƒLSTM-Stock-Predictorè®¾è®¡ç†å¿µï¼‰
å›å½’ç®€æ´æœ‰æ•ˆçš„æ¶æ„
"""

import os
import sys
import io
import warnings
import time
warnings.filterwarnings('ignore')

# UTF-8ç¼–ç 
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.decomposition import PCA

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# æ£€æŸ¥XGBoostæ˜¯å¦å¯ç”¨
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBoostæ¨¡å‹")

# é…ç½®
mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False
mpl.rcParams['figure.dpi'] = 200

os.makedirs('visualization_final', exist_ok=True)

RANDOM_STATE = 225  # ä¿®æ”¹éšæœºç§å­
np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_STATE)
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")

print("="*80)
print("Netflixè‚¡ç¥¨é¢„æµ‹ - æœ€ç»ˆç‰ˆ")
print(f"æ•°æ®: 2391è¡Œ Ã— 125ç‰¹å¾ â†’ ç®€åŒ–æ¶æ„")
print("="*80)

# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================

print("\n[1/5] æ•°æ®åŠ è½½...")
df = pd.read_csv('nflx_2014_2023.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date').reset_index(drop=True)

# ç›®æ ‡å˜é‡
df['next_5day_return'] = (df['close'].shift(-5) / df['close'] - 1)
df['return'] = df['close'].pct_change()

# âœ… ç²¾ç®€ç‰¹å¾å·¥ç¨‹ï¼ˆåŸºäºæˆåŠŸç»éªŒï¼‰
for lag in [5, 10, 15, 20]:  # åªä¿ç•™å…³é”®lag
    df[f'return_lag{lag}'] = df['return'].shift(lag)

for window in [5, 10, 20, 30]:  # å‡å°‘çª—å£æ•°é‡
    df[f'return_mean_{window}'] = df['return'].rolling(window).mean()
    df[f'return_std_{window}'] = df['return'].rolling(window).std()

# æŠ€æœ¯æŒ‡æ ‡
for indicator in ['rsi_14', 'macd', 'cci_14', 'atr_14']:
    if indicator in df.columns:
        df[f'{indicator}_momentum'] = df[indicator].diff()

# ä»·æ ¼ç›¸å¯¹ä½ç½®
df['close_to_sma50'] = (df['close'] - df['sma_50']) / df['sma_50']
df['close_to_sma100'] = (df['close'] - df['sma_100']) / df['sma_100']

# æ³¢åŠ¨ç‡
for window in [5, 10, 20]:
    df[f'volatility_{window}'] = df['return'].rolling(window).std()

# ä»·æ ¼åŠ¨é‡
for period in [5, 10, 20]:
    df[f'price_momentum_{period}'] = df['close'] / df['close'].shift(period) - 1

# æ—¶é—´ç‰¹å¾
df['month'] = df['date'].dt.month
df['dayofweek'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter

df = df.dropna().reset_index(drop=True)

print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

# =============================================================================
# æ•°æ®é›†åˆ’åˆ†
# =============================================================================

print("\n[2/5] æ•°æ®å¤„ç†...")

drop_cols = ['date', 'next_5day_return', 'open', 'high', 'low', 'close', 'return']
drop_cols = [col for col in drop_cols if col in df.columns]

X = df.drop(columns=drop_cols)
y = df['next_5day_return'].values

# æ—¶åºåˆ’åˆ†
train_size = int(len(X) * 0.70)
val_size = int(len(X) * 0.15)

X_train = X.values[:train_size]
X_val = X.values[train_size:train_size+val_size]
X_test = X.values[train_size+val_size:]

y_train = y[:train_size]
y_val = y[train_size:train_size+val_size]
y_test = y[train_size+val_size:]

print(f"è®­ç»ƒé›†: {X_train.shape[0]}, éªŒè¯é›†: {X_val.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
print(f"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}")

# ğŸš€ æ–°ç­–ç•¥ï¼šå®Œå…¨ä¸ä½¿ç”¨PCAï¼Œä¿ç•™åŸå§‹ç‰¹å¾ä¿¡æ¯
print(f"\nç‰¹å¾å¤„ç†ç­–ç•¥: ä¸ä½¿ç”¨PCAé™ç»´ï¼Œç›´æ¥æ ‡å‡†åŒ–ï¼ˆä¿ç•™æ‰€æœ‰{X_train.shape[1]}ç‰¹å¾ï¼‰")
print(f"  åŸå› : PCAç ´åäº†åŸå§‹ç‰¹å¾ç©ºé—´çš„ç»“æ„ï¼Œä¸é€‚åˆè‚¡ç¥¨é¢„æµ‹ä»»åŠ¡")

# åªä½¿ç”¨StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
print(f"  æ ‡å‡†åŒ–å®Œæˆ: {X_train.shape[1]}ç‰¹å¾ â†’ å‡å€¼0, æ ‡å‡†å·®1")

# =============================================================================
# ç®€æ´LSTMæ¨¡å‹ï¼ˆå‚è€ƒLSTM-Stock-Predictorï¼‰
# =============================================================================

class SimpleLSTM(nn.Module):
    """ç®€æ´LSTMï¼šå•å‘ + é€‚åº¦æ·±åº¦"""
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleLSTM, self).__init__()
        # âœ… å•å‘LSTMï¼ˆä¸æ˜¯åŒå‘ï¼ï¼‰
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        
        # ç®€æ´è¾“å‡ºå±‚
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

class SimpleGRU(nn.Module):
    """ç®€æ´GRUæ¨¡å‹"""
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                         batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        last_output = gru_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

def create_sequences(X, y, seq_len=10):
    """åˆ›å»ºåºåˆ—æ•°æ®"""
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len + 1):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len-1])
    return np.array(X_seq), np.array(y_seq)

# =============================================================================
# è®­ç»ƒå‡½æ•°
# =============================================================================

def train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
               epochs=50, lr=0.001, batch_size=32):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"\nè®­ç»ƒ {model_name}...")
    start_time = time.time()
    
    try:
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                                 shuffle=False, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                               shuffle=False, drop_last=True)
        
        # âœ… Huber Lossï¼ˆå‚è€ƒLSTM-Stock-Predictorï¼‰
        criterion = nn.HuberLoss(delta=1.0)
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                         factor=0.5, patience=5)
        
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        patience = 15
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            model.train()
            train_loss = 0
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    outputs = model(X_batch)
                    loss = criterion(outputs, y_batch)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            scheduler.step(val_loss)
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1}: Train={train_loss:.6f}, Val={val_loss:.6f}")
            
            if patience_counter >= patience:
                print(f"  Early stopping at epoch {epoch+1}")
                break
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # æµ‹è¯•
        model.eval()
        with torch.no_grad():
            predictions = model(X_test).cpu().numpy().flatten()
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        
        elapsed_time = time.time() - start_time
        print(f"  âœ… RÂ²={r2:.4f}, RMSE={rmse:.6f}, MAE={mae:.6f} ({elapsed_time:.1f}ç§’)")
        
        return {
            'model': model,
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions,
            'time': elapsed_time
        }
    except Exception as e:
        print(f"  âŒ {str(e)}")
        return None

# =============================================================================
# æ¨¡å‹è®­ç»ƒ
# =============================================================================

print("\n[3/5] æ¨¡å‹è®­ç»ƒ...")
print("="*80)

results = {}
input_size = X_train_scaled.shape[1]  # 30 (PCAå)

# âœ… ä½¿ç”¨seq_len=10ï¼ˆå‚è€ƒLSTM-Stock-Predictorçš„N_STEPSï¼‰
seq_len = 10
print(f"åºåˆ—é•¿åº¦: {seq_len}ï¼ˆå°æ•°æ®é›†é€‚ç”¨ï¼‰")

X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, seq_len)
X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, seq_len)

print(f"åºåˆ—æ•°æ®: è®­ç»ƒ={len(X_train_seq)}, éªŒè¯={len(X_val_seq)}, æµ‹è¯•={len(X_test_seq)}")

X_train_seq_tensor = torch.FloatTensor(X_train_seq).to(device)
X_val_seq_tensor = torch.FloatTensor(X_val_seq).to(device)
X_test_seq_tensor = torch.FloatTensor(X_test_seq).to(device)
y_train_seq_tensor = torch.FloatTensor(y_train_seq).unsqueeze(1).to(device)
y_val_seq_tensor = torch.FloatTensor(y_val_seq).unsqueeze(1).to(device)

# ===== å…ˆè®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹ =====
print("\nè®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼‰...")

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# Ridgeè¶…å‚æ•°ä¼˜åŒ–
print("  ä¼˜åŒ–Ridgeè¶…å‚æ•°...")
ridge_params = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]}
ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3, scoring='r2', n_jobs=-1)
ridge_grid.fit(X_train_scaled, y_train)
print(f"  â†’ Ridgeæœ€ä½³alpha={ridge_grid.best_params_['alpha']:.1f}, CV_RÂ²={ridge_grid.best_score_:.4f}")

baseline_models = {
    'Ridge': ridge_grid.best_estimator_,
    'Lasso': Lasso(alpha=0.01, max_iter=2000, random_state=RANDOM_STATE),  # è½»L1æ­£åˆ™åŒ–
    'ElasticNet': ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=RANDOM_STATE, max_iter=2000),
    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE),
    'DecisionTree': DecisionTreeRegressor(
        max_depth=5,                    # æµ…æ ‘ï¼ˆå‰ªæï¼‰
        min_samples_split=50,           # åˆ†è£‚æœ€å°æ ·æœ¬ï¼ˆæ—©åœï¼‰
        min_samples_leaf=20,            # å¶å­æœ€å°æ ·æœ¬ï¼ˆå‰ªæï¼‰
        min_impurity_decrease=0.001,   # æœ€å°çº¯åº¦æå‡ï¼ˆæ—©åœï¼‰
        ccp_alpha=0.01,                 # æˆæœ¬å¤æ‚åº¦å‰ªæ
        random_state=RANDOM_STATE
    ),
    'SVR': SVR(
        kernel='rbf',           # å¾„å‘åŸºæ ¸ï¼ˆéçº¿æ€§ï¼‰
        C=10.0,                 # æ­£åˆ™åŒ–å‚æ•°ï¼ˆå¢å¤§å®¹å¿åº¦ï¼‰
        epsilon=0.05,           # Îµç®¡ï¼ˆå¢å¤§é²æ£’æ€§ï¼‰
        gamma='scale',          # æ ¸ç³»æ•°ï¼ˆè‡ªåŠ¨ç¼©æ”¾ï¼‰
        max_iter=5000           # å¢åŠ è¿­ä»£æ¬¡æ•°
    ),
}

# æ·»åŠ XGBoostå’ŒLightGBMï¼ˆä¼˜åŒ–å‚æ•°ï¼Œé€‚é…å°æ•°æ®é›†ï¼‰
if XGBOOST_AVAILABLE:
    # æ•°æ®é‡çº§: ~1700æ ·æœ¬ Ã— 30ç‰¹å¾
    # ä½¿ç”¨ä¿å®ˆå‚æ•°é¿å…è¿‡æ‹Ÿåˆ
    baseline_models['XGBoost'] = xgb.XGBRegressor(
        n_estimators=50,        # å‡å°‘æ ‘æ•°é‡ï¼ˆå°æ•°æ®é›†ï¼‰
        max_depth=3,            # æµ…æ ‘ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
        learning_rate=0.05,     # ä½å­¦ä¹ ç‡
        subsample=0.8,          # è¡Œé‡‡æ ·
        colsample_bytree=0.8,   # åˆ—é‡‡æ ·
        min_child_weight=3,     # æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æƒé‡å’Œ
        reg_alpha=0.1,          # L1æ­£åˆ™åŒ–
        reg_lambda=1.0,         # L2æ­£åˆ™åŒ–
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbosity=0
    )

# æ·»åŠ LightGBMï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    import lightgbm as lgb
    baseline_models['LightGBM'] = lgb.LGBMRegressor(
        n_estimators=50,
        max_depth=4,
        learning_rate=0.05,
        num_leaves=15,           # å¶å­æ•°é‡ï¼ˆ2^4-1ï¼‰
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_samples=20,    # å¶å­æœ€å°æ ·æœ¬æ•°
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=-1
    )
except ImportError:
    print("  âš ï¸ LightGBMæœªå®‰è£…ï¼Œå°†è·³è¿‡LightGBMæ¨¡å‹")

baseline_results = {}

# è®­ç»ƒä¸éœ€è¦è¶…å‚æœç´¢çš„æ¨¡å‹
simple_models = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTree', 'SVR', 'GradientBoosting']
for name in simple_models:
    if name in baseline_models:
        model = baseline_models[name]
        print(f"\nè®­ç»ƒ {name}...")
        start_time = time.time()
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        elapsed_time = time.time() - start_time
        
        print(f"  âœ… RÂ²={r2:.4f}, RMSE={rmse:.6f}, MAE={mae:.6f} ({elapsed_time:.1f}ç§’)")
        
        baseline_results[name] = {
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions,
            'time': elapsed_time
        }

# === æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢ ===
print("\nğŸ” æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢...")

# === XGBoostæœç´¢ï¼ˆ8ä¸ªé…ç½®ï¼‰===
if XGBOOST_AVAILABLE:
    print("\n[XGBoost] æ¢ç´¢8ä¸ªå…³é”®é…ç½®ï¼ˆ1736æ ·æœ¬Ã—42ç‰¹å¾ï¼‰...")
    xgb_search_configs = [
        # (n_estimators, max_depth, learning_rate, subsample, colsample_bytree, min_child_weight, reg_alpha, reg_lambda)
        (50, 3, 0.05, 0.8, 0.8, 3, 0.1, 1.0),   # åŸºçº¿ï¼ˆä¿å®ˆï¼‰
        (100, 3, 0.05, 0.8, 0.8, 3, 0.1, 1.0),  # å¢åŠ æ ‘æ•°é‡
        (50, 4, 0.05, 0.8, 0.8, 3, 0.1, 1.0),   # å¢åŠ æ·±åº¦
        (50, 3, 0.1, 0.8, 0.8, 3, 0.1, 1.0),    # æé«˜å­¦ä¹ ç‡
        (50, 3, 0.05, 0.9, 0.9, 2, 0.05, 0.5),  # å‡å°‘æ­£åˆ™åŒ–
        (100, 4, 0.03, 0.8, 0.8, 4, 0.2, 1.5),  # æ›´ä¿å®ˆ
        (75, 3, 0.07, 0.85, 0.85, 3, 0.1, 1.0), # ä¸­ç­‰å‚æ•°
        (50, 5, 0.05, 0.7, 0.7, 5, 0.3, 2.0),   # æ·±æ ‘+å¼ºæ­£åˆ™
    ]
    
    xgb_search_results = []
    for i, (n_est, max_d, lr, sub, col, mcw, alpha, lamb) in enumerate(xgb_search_configs, 1):
        name = f"XGB_{n_est}_{max_d}"
        print(f"  [{i}/8] {name} (n={n_est}, d={max_d}, lr={lr})...", end=' ')
        model = xgb.XGBRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr,
            subsample=sub, colsample_bytree=col, min_child_weight=mcw,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            xgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ è·³è¿‡")
    
    xgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_xgb = xgb_search_results[:3]
    print(f"  âœ… Top 3 XGBoost: {[(n, r['r2']) for n, r in top_3_xgb]}")
    for name, result in top_3_xgb:
        baseline_results[name] = result

# === LightGBMæœç´¢ï¼ˆ8ä¸ªé…ç½®ï¼‰===
try:
    import lightgbm as lgb
    print("\n[LightGBM] æ¢ç´¢8ä¸ªå…³é”®é…ç½®...")
    lgb_search_configs = [
        # (n_estimators, max_depth, learning_rate, num_leaves, subsample, colsample_bytree, min_child_samples, reg_alpha, reg_lambda)
        (50, 4, 0.05, 15, 0.8, 0.8, 20, 0.1, 1.0),  # åŸºçº¿
        (100, 4, 0.05, 15, 0.8, 0.8, 20, 0.1, 1.0), # å¢åŠ æ ‘æ•°é‡
        (50, 5, 0.05, 20, 0.8, 0.8, 20, 0.1, 1.0),  # å¢åŠ æ·±åº¦å’Œå¶å­
        (50, 4, 0.1, 15, 0.8, 0.8, 20, 0.1, 1.0),   # æé«˜å­¦ä¹ ç‡
        (50, 4, 0.05, 10, 0.9, 0.9, 15, 0.05, 0.5), # å‡å°‘æ­£åˆ™åŒ–
        (100, 5, 0.03, 20, 0.8, 0.8, 25, 0.2, 1.5), # æ›´ä¿å®ˆ
        (75, 4, 0.07, 15, 0.85, 0.85, 20, 0.1, 1.0),# ä¸­ç­‰å‚æ•°
        (50, 6, 0.05, 25, 0.7, 0.7, 30, 0.3, 2.0),  # æ·±æ ‘+å¼ºæ­£åˆ™
    ]
    
    lgb_search_results = []
    for i, (n_est, max_d, lr, n_leaves, sub, col, mcs, alpha, lamb) in enumerate(lgb_search_configs, 1):
        name = f"LGB_{n_est}_{max_d}"
        print(f"  [{i}/8] {name} (n={n_est}, d={max_d}, lr={lr})...", end=' ')
        model = lgb.LGBMRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr, num_leaves=n_leaves,
            subsample=sub, colsample_bytree=col, min_child_samples=mcs,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            lgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ è·³è¿‡")
    
    lgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_lgb = lgb_search_results[:3]
    print(f"  âœ… Top 3 LightGBM: {[(n, r['r2']) for n, r in top_3_lgb]}")
    for name, result in top_3_lgb:
        baseline_results[name] = result
except ImportError:
    print("  âš ï¸ LightGBMæœªå®‰è£…ï¼Œè·³è¿‡")

# === RandomForestæœç´¢ï¼ˆ8ä¸ªé…ç½®ï¼‰===
print("\n[RandomForest] æ¢ç´¢8ä¸ªå…³é”®é…ç½®...")
rf_search_configs = [
    # (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features)
    (100, 10, 10, 5, 'sqrt'),   # åŸºçº¿
    (200, 10, 10, 5, 'sqrt'),   # å¢åŠ æ ‘æ•°é‡
    (100, 15, 10, 5, 'sqrt'),   # å¢åŠ æ·±åº¦
    (100, 10, 5, 3, 'sqrt'),    # å‡å°‘å‰ªæ
    (100, 8, 15, 8, 'sqrt'),    # æ›´ä¿å®ˆ
    (150, 12, 10, 5, 'log2'),   # log2ç‰¹å¾
    (100, 10, 10, 5, 0.5),      # 50%ç‰¹å¾
    (100, 20, 20, 10, 'sqrt'),  # æ·±æ ‘+å¼ºå‰ªæ
]

rf_search_results = []
for i, (n_est, max_d, mss, msl, max_f) in enumerate(rf_search_configs, 1):
    name = f"RF_{n_est}_{max_d}"
    print(f"  [{i}/8] {name} (n={n_est}, d={max_d})...", end=' ')
    model = RandomForestRegressor(
        n_estimators=n_est, max_depth=max_d,
        min_samples_split=mss, min_samples_leaf=msl,
        max_features=max_f,
        random_state=RANDOM_STATE, n_jobs=-1
    )
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.2:
        rf_search_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred,
            'time': 0
        }))
        print(f"RÂ²={r2:.4f} âœ…")
    else:
        print("âŒ è·³è¿‡")

rf_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_rf = rf_search_results[:3]
print(f"  âœ… Top 3 RandomForest: {[(n, r['r2']) for n, r in top_3_rf]}")
for name, result in top_3_rf:
    baseline_results[name] = result

print(f"\nâœ… æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢å®Œæˆ")

# ===== æ·±åº¦å­¦ä¹ æ¨¡å‹ =====
print("\nè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")

# ğŸ” æ¢ç´¢æ€§è¶…å‚æ•°æœç´¢ï¼ˆæ¯ç±»æ¨¡å‹â‰¤8æ¬¡ï¼Œè€ƒè™‘å°æ•°æ®é›†ç‰¹ç‚¹ï¼‰
print("\nğŸ” æ¢ç´¢æ€§è¶…å‚æ•°æœç´¢ï¼ˆ1736æ ·æœ¬Ã—42ç‰¹å¾ï¼‰...")

# === LSTMè¶…å‚æ•°æœç´¢ï¼ˆ8ä¸ªé…ç½®ï¼‰===
print("\n[LSTM] æ¢ç´¢8ä¸ªå…³é”®é…ç½®...")
lstm_search_configs = [
    # (hidden_size, num_layers, dropout, lr)
    (32, 1, 0.3, 0.001),   # å°æ¨¡å‹ï¼Œä½dropout
    (48, 1, 0.3, 0.001),   # ä¸­å°æ¨¡å‹
    (64, 1, 0.2, 0.001),   # ä¸­æ¨¡å‹ï¼Œä½dropout
    (32, 2, 0.4, 0.001),   # å°åŒå±‚ï¼Œé«˜dropout
    (48, 2, 0.4, 0.001),   # ä¸­åŒå±‚
    (32, 1, 0.3, 0.0005),  # å°æ¨¡å‹ï¼Œä½å­¦ä¹ ç‡
    (48, 1, 0.3, 0.0005),  # ä¸­æ¨¡å‹ï¼Œä½å­¦ä¹ ç‡
    (64, 2, 0.3, 0.0005),  # ä¸­åŒå±‚ï¼Œä½å­¦ä¹ ç‡
]

lstm_search_results = []
for i, (hs, nl, dp, lr) in enumerate(lstm_search_configs, 1):
    name = f"LSTM_{hs}_{nl}"
    print(f"  [{i}/8] {name} (h={hs}, l={nl}, d={dp:.1f}, lr={lr:.4f})...", end=' ')
    torch.manual_seed(RANDOM_STATE + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=50, lr=lr, batch_size=32)
    if result and result['r2'] > -0.1:  # åªä¿ç•™åˆç†ç»“æœ
        result['config'] = {'hidden_size': hs, 'num_layers': nl, 'dropout': dp, 'lr': lr}
        lstm_search_results.append((name, result))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        print("âŒ è·³è¿‡")

lstm_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm = lstm_search_results[:3]
print(f"  âœ… Top 3 LSTM: {[(n, r['r2']) for n, r in top_3_lstm]}")

# === GRUè¶…å‚æ•°æœç´¢ï¼ˆ8ä¸ªé…ç½®ï¼‰===
print("\n[GRU] æ¢ç´¢8ä¸ªå…³é”®é…ç½®...")
gru_search_configs = [
    # åŸºäºä¹‹å‰ç»éªŒï¼ŒGRU_48_3è¡¨ç°å¥½ï¼Œé‡ç‚¹æ¢ç´¢è¿™ä¸ªåŒºåŸŸ
    (48, 3, 0.35, 0.0005),  # æœ€ä¼˜é…ç½®å¾®è°ƒ
    (48, 3, 0.3, 0.0005),   # é™ä½dropout
    (48, 2, 0.35, 0.001),   # å‡å°‘å±‚æ•°
    (32, 2, 0.3, 0.001),    # å°æ¨¡å‹
    (64, 2, 0.3, 0.001),    # ä¸­æ¨¡å‹
    (48, 1, 0.25, 0.001),   # å•å±‚
    (80, 2, 0.4, 0.0001),   # å¤§æ¨¡å‹ï¼Œä½å­¦ä¹ ç‡
    (48, 3, 0.4, 0.001),    # é«˜dropout
]

gru_search_results = []
for i, (hs, nl, dp, lr) in enumerate(gru_search_configs, 1):
    name = f"GRU_{hs}_{nl}"
    print(f"  [{i}/8] {name} (h={hs}, l={nl}, d={dp:.1f}, lr={lr:.4f})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 100 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 100 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=50, lr=lr, batch_size=32)
    if result and result['r2'] > -0.1:
        result['config'] = {'hidden_size': hs, 'num_layers': nl, 'dropout': dp, 'lr': lr}
        gru_search_results.append((name, result))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        print("âŒ è·³è¿‡")

gru_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru = gru_search_results[:3]
print(f"  âœ… Top 3 GRU: {[(n, r['r2']) for n, r in top_3_gru]}")

# === ä¿ç•™Top 3åˆ°results ===
for name, result in top_3_lstm + top_3_gru:
    results[name] = result

print(f"\nâœ… å·²ä¿ç•™Top 3 LSTM + Top 3 GRUåˆ°æœ€ç»ˆç»“æœ")

# =============================================================================
# é›†æˆå­¦ä¹ 
# =============================================================================

print("\n[4/5] å¼‚è´¨æ¨¡å‹é›†æˆå­¦ä¹ ï¼ˆå¤šæ ·åŒ–ç»„åˆï¼‰...")

# åˆå¹¶æ‰€æœ‰æ¨¡å‹ç»“æœï¼Œé€‰æ‹©Topæ¨¡å‹ï¼ˆä¿®å¤ç»´åº¦é—®é¢˜ï¼‰
all_models_for_ensemble = {}

# ä¼ ç»ŸMLæ¨¡å‹ï¼šéœ€è¦æˆªå–åˆ°åºåˆ—é•¿åº¦
for name, result in baseline_results.items():
    if result['r2'] > 0.05:  # åªé€‰RÂ²>0.05çš„æ¨¡å‹
        predictions_aligned = result['predictions'][-len(y_test_seq):]
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': predictions_aligned,
            'type': 'ML'
        }

# æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šå·²ç»æ˜¯åºåˆ—é•¿åº¦
for name, result in results.items():
    if result['r2'] > 0.05:
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': result['predictions'],
            'type': 'DL'
        }

# å¼‚è´¨æ€§é›†æˆç­–ç•¥ï¼šåªé€‰æ‹©é«˜æ€§èƒ½æ¨¡å‹ï¼ˆRÂ² > 0.13ï¼‰
sorted_all = sorted(all_models_for_ensemble.items(), key=lambda x: x[1]['r2'], reverse=True)

# ç­–ç•¥ï¼šåªé€‰æ‹©RÂ²>0.13çš„æ¨¡å‹ï¼Œé¿å…å¼±æ¨¡å‹æ‹–ç´¯
R2_THRESHOLD = 0.13
selected_models = []
gru_selected = False

for model_name, result in sorted_all:
    # åªé€‰æ‹©æ€§èƒ½è¶…è¿‡é˜ˆå€¼çš„æ¨¡å‹
    if result['r2'] > R2_THRESHOLD:
        # GRUåªé€‰ä¸€ä¸ªï¼ˆæœ€ä½³çš„ï¼‰
        if 'GRU' in model_name:
            if not gru_selected:
                selected_models.append((model_name, result))
                gru_selected = True
        else:
            selected_models.append((model_name, result))
    
    if len(selected_models) >= 5:  # æœ€å¤š5ä¸ªæ¨¡å‹
        break

# è‡³å°‘ä¿ç•™Top 3æ¨¡å‹
if len(selected_models) < 3:
    for model_name, result in sorted_all:
        if (model_name, result) not in selected_models:
            if 'GRU' in model_name and gru_selected:
                continue
            selected_models.append((model_name, result))
            if len(selected_models) >= 3:
                break

if len(selected_models) >= 2:
    print(f"  ä½¿ç”¨{len(selected_models)}ä¸ªå¼‚è´¨æ¨¡å‹:")
    for i, (name, result) in enumerate(selected_models, 1):
        model_type_detail = "ä¼ ç»ŸML" if result['type'] == 'ML' else "æ·±åº¦å­¦ä¹ "
        print(f"    {i}. {name}: RÂ²={result['r2']:.4f} ({model_type_detail})")
    
    # === ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆï¼ˆå¼ºåŒ–ä¼˜ç§€æ¨¡å‹æƒé‡ï¼‰===
    print("\n  ã€ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆã€‘")
    r2_values = np.array([result['r2'] for _, result in selected_models])
    weights_r2 = r2_values ** 3  # ä½¿ç”¨RÂ²Â³åŠ æƒï¼Œè¿›ä¸€æ­¥æ”¾å¤§ä¼˜ç§€æ¨¡å‹è´¡çŒ®
    weights_r2 = weights_r2 / weights_r2.sum()
    print(f"  æƒé‡åˆ†é…ï¼ˆRÂ²Â³ï¼‰: {dict(zip([name for name, _ in selected_models], weights_r2.round(3)))}")
    
    ensemble_pred_r2 = np.zeros_like(selected_models[0][1]['predictions'])
    for (name, result), w in zip(selected_models, weights_r2):
        ensemble_pred_r2 += w * result['predictions']
    
    ensemble_r2_weighted = r2_score(y_test_seq, ensemble_pred_r2)
    ensemble_rmse_weighted = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_r2))
    ensemble_mae_weighted = mean_absolute_error(y_test_seq, ensemble_pred_r2)
    
    print(f"  ç»“æœ: RÂ²={ensemble_r2_weighted:.4f}, RMSE={ensemble_rmse_weighted:.6f}, MAE={ensemble_mae_weighted:.6f}")
    
    # === ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰ ===
    print("\n  ã€ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰ã€‘")
    from sklearn.linear_model import LinearRegression
    
    # æ„å»ºè®­ç»ƒé›†é¢„æµ‹çŸ©é˜µï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰
    # X_val_scaled å·²ç»æ˜¯PCA+æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆ30ç‰¹å¾ï¼‰
    X_val_seq, y_val_seq_meta = create_sequences(X_val_scaled, y_val, seq_len)
    
    stacking_train_preds = []
    for name, result in selected_models:
        if name in baseline_results:
            # ä¼ ç»ŸMLæ¨¡å‹ï¼ˆä½¿ç”¨X_val_scaledï¼Œå·²PCA+æ ‡å‡†åŒ–ï¼‰
            model_obj = result.get('model')
            if model_obj is not None:
                val_pred = model_obj.predict(X_val_scaled)[-len(y_val_seq_meta):]
                stacking_train_preds.append(val_pred)
        else:
            # æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä½¿ç”¨åºåˆ—æ•°æ®ï¼‰
            model_obj = result.get('model')
            if model_obj is not None:
                model_obj.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val_seq).to(device)
                    val_pred = model_obj(X_val_tensor).cpu().numpy().flatten()
                    stacking_train_preds.append(val_pred)
    
    # æ„å»ºæµ‹è¯•é›†é¢„æµ‹çŸ©é˜µ
    stacking_test_preds = []
    for name, result in selected_models:
        test_pred = result['predictions']
        stacking_test_preds.append(test_pred)
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLinearRegressionï¼‰
    print(f"  éªŒè¯é›†é¢„æµ‹æ•°: {len(stacking_train_preds)}")
    if len(stacking_train_preds) >= len(selected_models):
        X_stacking_train = np.column_stack(stacking_train_preds)
        X_stacking_test = np.column_stack(stacking_test_preds)
        
        print(f"  è®­ç»ƒé›†çŸ©é˜µ: {X_stacking_train.shape}, æµ‹è¯•é›†çŸ©é˜µ: {X_stacking_test.shape}")
        
        meta_learner = LinearRegression()
        meta_learner.fit(X_stacking_train, y_val_seq_meta)
        
        # è·å–æƒé‡
        weights_ols = meta_learner.coef_
        weights_ols = np.maximum(weights_ols, 0)  # éè´Ÿçº¦æŸ
        if weights_ols.sum() > 0:
            weights_ols = weights_ols / weights_ols.sum()  # å½’ä¸€åŒ–
        
        print(f"  æƒé‡åˆ†é…: {dict(zip([name for name, _ in selected_models], weights_ols.round(3)))}")
        
        # æœ€å°äºŒä¹˜æ³•é¢„æµ‹
        ensemble_pred_ols = meta_learner.predict(X_stacking_test)
        
        ensemble_r2_ols = r2_score(y_test_seq, ensemble_pred_ols)
        ensemble_rmse_ols = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_ols))
        ensemble_mae_ols = mean_absolute_error(y_test_seq, ensemble_pred_ols)
        
        print(f"  ç»“æœ: RÂ²={ensemble_r2_ols:.4f}, RMSE={ensemble_rmse_ols:.6f}, MAE={ensemble_mae_ols:.6f}")
    else:
        print(f"  âš ï¸ éªŒè¯é›†é¢„æµ‹ä¸è¶³ï¼Œä½¿ç”¨RÂ²Â²åŠ æƒç»“æœä½œä¸ºæœ€å°äºŒä¹˜æ³•ç»“æœ")
        ensemble_r2_ols = ensemble_r2_weighted
        ensemble_rmse_ols = ensemble_rmse_weighted
        ensemble_mae_ols = ensemble_mae_weighted
        ensemble_pred_ols = ensemble_pred_r2
    
    # å¯¹æ¯”ä¸¤ç§ç­–ç•¥
    print(f"\n  ã€ç­–ç•¥å¯¹æ¯”ã€‘")
    print(f"  RÂ²Â³åŠ æƒ: RÂ²={ensemble_r2_weighted:.4f}, RMSE={ensemble_rmse_weighted:.6f}")
    print(f"  æœ€å°äºŒä¹˜: RÂ²={ensemble_r2_ols:.4f}, RMSE={ensemble_rmse_ols:.6f}")
    print(f"  RidgeåŸºå‡†: RÂ²={sorted_all[0][1]['r2']:.4f}")
    
    # ä¿å­˜ä¸¤ç§ç­–ç•¥çš„ç»“æœ
    results['Ensemble_R2Â³'] = {
        'r2': ensemble_r2_weighted,
        'rmse': ensemble_rmse_weighted,
        'mae': ensemble_mae_weighted,
        'predictions': ensemble_pred_r2
    }
    results['Ensemble_OLS'] = {
        'r2': ensemble_r2_ols,
        'rmse': ensemble_rmse_ols,
        'mae': ensemble_mae_ols,
        'predictions': ensemble_pred_ols
    }
    
    # é€‰æ‹©æ›´å¥½çš„ç­–ç•¥
    if ensemble_r2_ols > ensemble_r2_weighted:
        print(f"  âœ… æœ€å°äºŒä¹˜æ³•æ›´ä¼˜ï¼Œæå‡: +{(ensemble_r2_ols-ensemble_r2_weighted):.4f}")
    elif ensemble_r2_weighted > ensemble_r2_ols:
        print(f"  âœ… RÂ²Â²åŠ æƒæ›´ä¼˜ï¼Œä¼˜åŠ¿: +{(ensemble_r2_weighted-ensemble_r2_ols):.4f}")
    else:
        print(f"  âš–ï¸ ä¸¤ç§ç­–ç•¥æ€§èƒ½ç›¸å½“")
    
    print(f"\n  ç»„åˆ: {len([m for m in selected_models if m[1]['type']=='ML'])}ä¸ªä¼ ç»ŸML + {len([m for m in selected_models if m[1]['type']=='DL'])}ä¸ªæ·±åº¦å­¦ä¹ ")

# =============================================================================
# ç»“æœæ±‡æ€»
# =============================================================================

print("\n[5/5] ç»“æœæ±‡æ€»...")
print("="*80)
print("æœ€ç»ˆç‰ˆæ¨¡å‹ç»“æœ")
print("="*80)

# åˆå¹¶åŸºçº¿æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç»“æœ
all_results = {**baseline_results, **results}
sorted_results = sorted(all_results.items(), key=lambda x: x[1]['r2'], reverse=True)

print(f"\n{'æ¨¡å‹':<20} {'RÂ²':<10} {'RMSE':<12} {'MAE':<12} {'ç±»å‹':<10}")
print("-" * 70)
for model_name, result in sorted_results:
    model_type = "æ·±åº¦å­¦ä¹ " if model_name in results else "ä¼ ç»ŸML"
    print(f"{model_name:<20} {result['r2']:>8.4f}  {result['rmse']:>10.6f}  {result['mae']:>10.6f}  {model_type}")

best_model = sorted_results[0][0]
best_r2 = sorted_results[0][1]['r2']
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (RÂ²={best_r2:.4f})")

# å¯¹æ¯”åˆ†æ
print("\n" + "="*80)
print("æ¨¡å‹ç±»å‹å¯¹æ¯”")
print("="*80)
dl_models = {k: v for k, v in results.items()}
ml_models = {k: v for k, v in baseline_results.items()}

if dl_models:
    dl_avg_r2 = np.mean([v['r2'] for v in dl_models.values()])
    print(f"æ·±åº¦å­¦ä¹ å¹³å‡RÂ²: {dl_avg_r2:.4f}")
if ml_models:
    ml_avg_r2 = np.mean([v['r2'] for v in ml_models.values()])
    print(f"ä¼ ç»ŸMLå¹³å‡RÂ²: {ml_avg_r2:.4f}")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# RÂ²å¯¹æ¯”ï¼ˆTop 10ï¼‰
ax = axes[0, 0]
top_10 = sorted_results[:10]
models_top10 = [m[0][:15] for m in top_10]
r2s_top10 = [m[1]['r2'] for m in top_10]
colors = ['green' if r2 > 0.1 else 'skyblue' if r2 > 0 else 'lightcoral' for r2 in r2s_top10]
ax.barh(models_top10, r2s_top10, color=colors)
ax.set_xlabel('RÂ² Score', fontsize=10)
ax.set_title('Top 10 æ¨¡å‹RÂ²å¯¹æ¯”', fontsize=11, fontweight='bold')
ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)
ax.axvline(x=0.1, color='green', linestyle='--', alpha=0.5, label='RÂ²=0.1')
ax.legend(fontsize=8)
ax.grid(axis='x', alpha=0.3)
ax.tick_params(labelsize=8)

# RMSEå¯¹æ¯”
ax = axes[0, 1]
rmses_top10 = [m[1]['rmse'] for m in top_10]
ax.barh(models_top10, rmses_top10, color='lightcoral')
ax.set_xlabel('RMSE', fontsize=10)
ax.set_title('Top 10 æ¨¡å‹RMSEå¯¹æ¯”', fontsize=11, fontweight='bold')
ax.grid(axis='x', alpha=0.3)
ax.tick_params(labelsize=8)

# æ¨¡å‹ç±»å‹å¯¹æ¯”ï¼ˆç®±çº¿å›¾ï¼‰
ax = axes[0, 2]
dl_r2s = [v['r2'] for k, v in results.items()]
ml_r2s = [v['r2'] for k, v in baseline_results.items()]
ax.boxplot([dl_r2s, ml_r2s], labels=['æ·±åº¦å­¦ä¹ ', 'ä¼ ç»ŸML'])
ax.set_ylabel('RÂ² Score', fontsize=10)
ax.set_title('æ¨¡å‹ç±»å‹RÂ²åˆ†å¸ƒ', fontsize=11, fontweight='bold')
ax.grid(axis='y', alpha=0.3)
ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
ax.tick_params(labelsize=9)

# é¢„æµ‹vså®é™…
ax = axes[1, 0]
best_pred = sorted_results[0][1]['predictions']
# å¦‚æœæ˜¯åºåˆ—æ¨¡å‹ï¼Œä½¿ç”¨y_test_seqï¼Œå¦åˆ™ä½¿ç”¨y_test
y_test_plot = y_test_seq if best_model in results else y_test
best_pred_plot = best_pred[:len(y_test_plot)] if len(best_pred) > len(y_test_plot) else best_pred
y_test_plot = y_test_plot[:len(best_pred_plot)]

ax.scatter(y_test_plot, best_pred_plot, alpha=0.5, s=20)
ax.plot([y_test_plot.min(), y_test_plot.max()], 
        [y_test_plot.min(), y_test_plot.max()], 
        'r--', lw=2, label='å®Œç¾é¢„æµ‹')
ax.set_xlabel('å®é™…5æ—¥æ”¶ç›Šç‡', fontsize=10)
ax.set_ylabel('é¢„æµ‹5æ—¥æ”¶ç›Šç‡', fontsize=10)
ax.set_title(f'æœ€ä½³æ¨¡å‹é¢„æµ‹ ({best_model}, RÂ²={best_r2:.4f})', fontsize=11, fontweight='bold')
ax.legend(fontsize=9)
ax.grid(True, alpha=0.3)
ax.tick_params(labelsize=9)

# æ®‹å·®åˆ†å¸ƒ
ax = axes[1, 1]
residuals = y_test_plot - best_pred_plot
ax.hist(residuals, bins=40, color='green', alpha=0.7, edgecolor='black')
ax.axvline(x=0, color='red', linestyle='--', lw=2)
ax.set_xlabel('æ®‹å·®', fontsize=10)
ax.set_ylabel('é¢‘æ•°', fontsize=10)
ax.set_title(f'æ®‹å·®åˆ†å¸ƒ (MAE={sorted_results[0][1]["mae"]:.4f})', fontsize=11, fontweight='bold')
ax.grid(True, alpha=0.3)
ax.tick_params(labelsize=9)

# ç‰ˆæœ¬å¯¹æ¯”ï¼ˆå†å²ï¼‰
ax = axes[1, 2]
version_names = ['V3\n(åˆ†ç±»)', 'V4\n(é›†æˆ)', 'V5å¢å¼º\n(å›å½’)', 'V5æœ€ç»ˆ\n(ç®€åŒ–)']
version_scores = [0.5227, 0.5387, 0.0418, best_r2]  # å‡†ç¡®ç‡ â†’ RÂ²
version_colors = ['lightblue', 'skyblue', 'orange', 'green']
bars = ax.bar(version_names, version_scores, color=version_colors, alpha=0.8, edgecolor='black')
ax.set_ylabel('æ€§èƒ½æŒ‡æ ‡', fontsize=10)
ax.set_title('ç‰ˆæœ¬æ¼”è¿›å¯¹æ¯”', fontsize=11, fontweight='bold')
ax.grid(axis='y', alpha=0.3)
for i, (bar, score) in enumerate(zip(bars, version_scores)):
    height = bar.get_height()
    label = f'{score:.2%}' if i < 2 else f'RÂ²={score:.3f}'
    ax.text(bar.get_x() + bar.get_width()/2., height,
            label, ha='center', va='bottom', fontsize=9)
ax.tick_params(labelsize=8)

plt.tight_layout()
plt.savefig('visualization_final/final_results.png', dpi=200, bbox_inches='tight')
print(f"\nğŸ“Š å¯è§†åŒ–å·²ä¿å­˜: visualization_final/final_results.png")

# é¢å¤–å¯è§†åŒ–ï¼šå¯¹æ¯”å›¾
fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))

# æ·±åº¦å­¦ä¹  vs ä¼ ç»ŸML
ax = axes2[0]
categories = ['æ·±åº¦å­¦ä¹ ', 'ä¼ ç»ŸML']
avg_r2 = [np.mean([v['r2'] for v in results.values()]), 
          np.mean([v['r2'] for v in baseline_results.values()])]
max_r2 = [max([v['r2'] for v in results.values()]), 
          max([v['r2'] for v in baseline_results.values()])]
min_r2 = [min([v['r2'] for v in results.values()]), 
          min([v['r2'] for v in baseline_results.values()])]

x = np.arange(len(categories))
width = 0.25
ax.bar(x - width, avg_r2, width, label='å¹³å‡RÂ²', color='skyblue')
ax.bar(x, max_r2, width, label='æœ€ä½³RÂ²', color='green')
ax.bar(x + width, min_r2, width, label='æœ€å·®RÂ²', color='lightcoral')
ax.set_ylabel('RÂ² Score')
ax.set_title('æ·±åº¦å­¦ä¹  vs ä¼ ç»Ÿæœºå™¨å­¦ä¹ ')
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)

# æ—¶é—´åºåˆ—é¢„æµ‹å¯è§†åŒ–ï¼ˆæœ€å50ä¸ªç‚¹ï¼‰
ax = axes2[1]
n_show = min(50, len(y_test_plot))

# è·å–æµ‹è¯•é›†å¯¹åº”çš„æ—¥æœŸ
test_dates = df.iloc[train_size + val_size:train_size + val_size + len(y_test)]['date'].values
if best_model in results:  # åºåˆ—æ¨¡å‹éœ€è¦å‡å»seq_len
    test_dates = test_dates[seq_len:]
test_dates_plot = test_dates[-n_show:]

ax.plot(test_dates_plot, y_test_plot[-n_show:], 'o-', label='å®é™…å€¼', alpha=0.7, markersize=4)
ax.plot(test_dates_plot, best_pred_plot[-n_show:], 's-', label='é¢„æµ‹å€¼', alpha=0.7, markersize=4)
ax.set_xlabel('æ—¥æœŸ')
ax.set_ylabel('5æ—¥æ”¶ç›Šç‡')
ax.set_title(f'é¢„æµ‹vså®é™…ï¼ˆæœ€å{n_show}ä¸ªç‚¹ï¼‰')
ax.legend()
ax.grid(True, alpha=0.3)
ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)
# æ—‹è½¬æ—¥æœŸæ ‡ç­¾
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
ax.tick_params(labelsize=8)

plt.tight_layout()
plt.savefig('visualization_final/comparison_analysis.png', dpi=200, bbox_inches='tight')
print(f"ğŸ“Š å¯¹æ¯”åˆ†æå·²ä¿å­˜: visualization_final/comparison_analysis.png")

# ä¿å­˜æœ€ä½³æ¨¡å‹
best_model_obj = sorted_results[0][1].get('model')
if best_model_obj:
    torch.save(best_model_obj.state_dict(), 'best_model_final.pth')
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: best_model_final.pth")

print("\n" + "="*80)
print("âœ… æœ€ç»ˆç‰ˆè®­ç»ƒå®Œæˆï¼")
print("="*80)

