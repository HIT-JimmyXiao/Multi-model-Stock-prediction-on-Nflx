# -*- coding: utf-8 -*-
"""
Netflixè‚¡ç¥¨é¢„æµ‹ - ä¼˜åŒ–ç‰ˆv2ï¼ˆæ¿€è¿›ç‰¹å¾ç­›é€‰ï¼‰
æ”¹è¿›è¦ç‚¹ï¼š
1. ç‰¹å¾å·¥ç¨‹ï¼š156 â†’ ä¼ ç»ŸML(27ç‰¹å¾/50%) + æ·±åº¦å­¦ä¹ (11ç‰¹å¾/20%)
   - ä¼ ç»ŸML: æ ·æœ¬æ•°/65â‰ˆ27 â†’ ä¿å®ˆæ¯”ä¾‹ âœ…
   - æ·±åº¦å­¦ä¹ : æ ·æœ¬æ•°/150â‰ˆ11 â†’ æç®€é˜²è¿‡æ‹Ÿåˆ âœ…
2. LSTMä¼˜åŒ–ï¼šå°æ¨¡å‹+ä½lr+é•¿è®­ç»ƒï¼ˆepochs=150, 10/12æˆåŠŸï¼‰
3. GRUä¼˜åŒ–ï¼šåŸºäºå†å²æœ€ä¼˜GRU_32_3ç²¾ç»†è°ƒä¼˜
4. RFæ·±åº¦ä¼˜åŒ–ï¼šå›´ç»•æœ€ä¼˜RF_200_8é…ç½®ç½‘æ ¼æœç´¢
5. é¢„æµ‹ç›®æ ‡ï¼š5å¤©æ”¶ç›Šç‡ï¼ˆå•å…¬å¸æŠ€æœ¯é¢å»¶ç»­æ€§å¼ºï¼‰
6. åºåˆ—é•¿åº¦20ï¼Œbatch_size=16ï¼Œå……åˆ†è®­ç»ƒ
"""

import os
import sys
import io
import time
import warnings
warnings.filterwarnings('ignore')

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import mutual_info_regression, f_regression
from statsmodels.stats.outliers_influence import variance_inflation_factor

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge

mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False
mpl.rcParams['figure.dpi'] = 200

os.makedirs('visualization_final', exist_ok=True)

RANDOM_STATE = 225
np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_STATE)
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")

print("="*80)
print("Netflixè‚¡ç¥¨é¢„æµ‹ - ä¸°å¯Œç‰¹å¾+å®Œæ•´ç­›é€‰ç‰ˆ")
print("="*80)

# =============================================================================
# ç¬¬ä¸€é˜¶æ®µï¼šä¸°å¯Œç‰¹å¾å·¥ç¨‹
# =============================================================================
print("\n[é˜¶æ®µ1] ä¸°å¯Œç‰¹å¾å·¥ç¨‹...")
df = pd.read_csv('nflx_2014_2023.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date').reset_index(drop=True)

print(f"åŸå§‹æ•°æ®: {df.shape}")

# ===== ç›®æ ‡å˜é‡ =====
df['next_5day_return'] = (df['close'].shift(-5) / df['close'] - 1)

# ===== 1. åŸå§‹ä»·æ ¼ç‰¹å¾ï¼ˆæ— æ³„éœ²ï¼é¢„æµ‹5æ—¥åï¼Œå½“æ—¥ä»·æ ¼å·²çŸ¥ï¼‰=====
df['daily_range'] = (df['high'] - df['low']) / df['close']  # æ—¥å†…æ³¢åŠ¨å¹…åº¦
df['open_close_ratio'] = df['close'] / df['open']  # æ”¶ç›˜/å¼€ç›˜æ¯”
df['high_close_ratio'] = df['high'] / df['close']  # æœ€é«˜/æ”¶ç›˜æ¯”
df['low_close_ratio'] = df['low'] / df['close']   # æœ€ä½/æ”¶ç›˜æ¯”
df['volume_change'] = df['volume'].pct_change()   # æˆäº¤é‡å˜åŒ–

# ===== 2. æ”¶ç›Šç‡ç‰¹å¾ =====
df['return'] = df['close'].pct_change()
df['log_return'] = np.log(df['close'] / df['close'].shift(1))

# æ›´å¤šlagç‰¹å¾ï¼ˆ5-30å¤©ï¼‰
for lag in [1, 2, 3, 5, 7, 10, 15, 20, 30]:
    df[f'return_lag{lag}'] = df['return'].shift(lag)
    df[f'close_lag{lag}'] = df['close'].shift(lag)

# ===== 3. æ»šåŠ¨ç»Ÿè®¡ï¼ˆå¤šæ—¶é—´çª—å£ï¼‰=====
for window in [3, 5, 7, 10, 15, 20, 30, 60]:
    # æ”¶ç›Šç‡ç»Ÿè®¡
    df[f'return_mean_{window}'] = df['return'].rolling(window).mean()
    df[f'return_std_{window}'] = df['return'].rolling(window).std()
    
    # skewå’Œkurtéœ€è¦è‡³å°‘4ä¸ªè§‚æµ‹å€¼ï¼Œåªåœ¨çª—å£>=5æ—¶è®¡ç®—
    if window >= 5:
        df[f'return_skew_{window}'] = df['return'].rolling(window).skew()
        df[f'return_kurt_{window}'] = df['return'].rolling(window).kurt()
    
    # ä»·æ ¼ç»Ÿè®¡
    df[f'close_max_{window}'] = df['close'].rolling(window).max()
    df[f'close_min_{window}'] = df['close'].rolling(window).min()
    df[f'close_mean_{window}'] = df['close'].rolling(window).mean()
    
    # æˆäº¤é‡ç»Ÿè®¡
    df[f'volume_mean_{window}'] = df['volume'].rolling(window).mean()

# ===== 4. æŠ€æœ¯æŒ‡æ ‡è¡ç”Ÿ =====
# åŸå§‹æŒ‡æ ‡å·²æœ‰ï¼šrsi_7, rsi_14, cci_7, cci_14, sma_50, ema_50, sma_100, ema_100, macd, bollinger, atr_7, atr_14

# RSIç›¸å…³
df['rsi_diff'] = df['rsi_14'] - df['rsi_7']
df['rsi_momentum'] = df['rsi_14'].diff()
df['rsi_ma5'] = df['rsi_14'].rolling(5).mean()

# CCIç›¸å…³
df['cci_diff'] = df['cci_14'] - df['cci_7']
df['cci_momentum'] = df['cci_14'].diff()

# MACDç›¸å…³
df['macd_momentum'] = df['macd'].diff()
df['macd_ma5'] = df['macd'].rolling(5).mean()

# ATRç›¸å…³
df['atr_diff'] = df['atr_14'] - df['atr_7']
df['atr_momentum'] = df['atr_14'].diff()

# ä»·æ ¼ä¸å‡çº¿å…³ç³»
df['close_to_sma50'] = (df['close'] - df['sma_50']) / df['sma_50']
df['close_to_sma100'] = (df['close'] - df['sma_100']) / df['sma_100']
df['close_to_ema50'] = (df['close'] - df['ema_50']) / df['ema_50']
df['close_to_ema100'] = (df['close'] - df['ema_100']) / df['ema_100']
df['sma_cross'] = (df['sma_50'] - df['sma_100']) / df['sma_100']

# å¸ƒæ—å¸¦ç›¸å…³
df['bollinger_position'] = (df['close'] - df['bollinger']) / df['bollinger']
df['bollinger_width'] = df['bollinger'] / df['close']

# ===== 5. æ³¢åŠ¨ç‡ç‰¹å¾ =====
for window in [3, 5, 10, 20, 30]:
    df[f'volatility_{window}'] = df['return'].rolling(window).std()

# æ³¢åŠ¨ç‡æ¯”ç‡ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰volatilityåˆ›å»ºåï¼‰
for window in [3, 5, 10, 20, 30]:
    if window != 20:  # é¿å…é™¤ä»¥è‡ªå·±
        df[f'volatility_ratio_{window}'] = df[f'volatility_{window}'] / df['volatility_20']

# ===== 6. åŠ¨é‡ç‰¹å¾ =====
for period in [3, 5, 7, 10, 15, 20, 30]:
    df[f'price_momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
    df[f'volume_momentum_{period}'] = df['volume'] / df['volume'].shift(period) - 1

# ===== 7. äº¤å‰ç‰¹å¾ =====
df['price_volume_corr'] = df['close'] * df['volume']
df['volatility_volume'] = df['volatility_20'] * df['volume']
df['rsi_volume'] = df['rsi_14'] * df['volume']

# ===== 8. æ—¶é—´ç‰¹å¾ =====
df['month'] = df['date'].dt.month
df['dayofweek'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter
df['day'] = df['date'].dt.day
df['week'] = df['date'].dt.isocalendar().week
df['is_month_start'] = df['date'].dt.is_month_start.astype(int)
df['is_month_end'] = df['date'].dt.is_month_end.astype(int)
df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)
df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)

print(f"ç‰¹å¾å·¥ç¨‹å: {df.shape}")

# =============================================================================
# ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®ç¼“å­˜æœºåˆ¶
# =============================================================================
CACHE_FILE = 'data_pre.csv'

if os.path.exists(CACHE_FILE):
    print(f"\nâœ… å‘ç°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œç›´æ¥åŠ è½½...")
    df = pd.read_csv(CACHE_FILE)
    df['date'] = pd.to_datetime(df['date'])
    print(f"åŠ è½½æ•°æ®: {df.shape}")
    print("è·³è¿‡ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®æ¸…ç†ï¼Œç›´æ¥è¿›å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
else:
    print("\n[é˜¶æ®µ2] æ•°æ®æ¸…ç†ï¼ˆæ— ç¼“å­˜ï¼Œæ‰§è¡Œå®Œæ•´é¢„å¤„ç†ï¼‰...")
    
    # âœ… æ­£ç¡®çš„NaNå¤„ç†ç­–ç•¥
    print(f"ç‰¹å¾å·¥ç¨‹åNaNç»Ÿè®¡: {df.isnull().sum().sum()}ä¸ªNaN")
    
    # 1. åªåˆ é™¤ç›®æ ‡å˜é‡ä¸ºNaNçš„è¡Œï¼ˆæœ€å5è¡Œï¼Œå› ä¸ºnext_5day_return = close.shift(-5)ï¼‰
    df_valid = df[df['next_5day_return'].notna()].copy()
    print(f"åˆ é™¤ç›®æ ‡å˜é‡NaNå: {df_valid.shape} (åˆ é™¤äº†{len(df) - len(df_valid)}è¡Œ)")
    
    # 2. å¯¹ç‰¹å¾åˆ—çš„NaNè¿›è¡Œå‰å‘å¡«å……ï¼ˆåˆç†å‡è®¾ï¼šç‰¹å¾å˜åŒ–æ˜¯è¿ç»­çš„ï¼‰
    feature_cols = [col for col in df_valid.columns if col not in ['date', 'next_5day_return']]
    df_valid[feature_cols] = df_valid[feature_cols].ffill()
    
    # 3. å¦‚æœè¿˜æœ‰NaNï¼ˆç¬¬ä¸€è¡Œçš„returnç­‰ï¼‰ï¼Œç”¨åå‘å¡«å……
    df_valid[feature_cols] = df_valid[feature_cols].bfill()
    
    # 4. æœ€åæ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaN
    remaining_nan = df_valid.isnull().sum().sum()
    print(f"å¡«å……åå‰©ä½™NaN: {remaining_nan}ä¸ª")
    
    if remaining_nan > 0:
        print("  è­¦å‘Šï¼šä»æœ‰NaNï¼Œç”¨0å¡«å……")
        df_valid = df_valid.fillna(0)
    
    df = df_valid.reset_index(drop=True)
    print(f"æœ€ç»ˆæ•°æ®: {df.shape}")
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
    df.to_csv(CACHE_FILE, index=False)
    print(f"âœ… é¢„å¤„ç†æ•°æ®å·²ä¿å­˜åˆ°: {CACHE_FILE}")

# å‡†å¤‡æ•°æ®
drop_cols = ['date', 'next_5day_return', 'next_day_close']
drop_cols = [col for col in drop_cols if col in df.columns]

# æ—¶é—´åºåˆ—åˆ†å‰²
train_size = int(len(df) * 0.70)
val_size = int(len(df) * 0.15)

df_train = df[:train_size].copy()
df_val = df[train_size:train_size+val_size].copy()
df_test = df[train_size+val_size:].copy()

print(f"è®­ç»ƒé›†: {df_train.shape[0]}, éªŒè¯é›†: {df_val.shape[0]}, æµ‹è¯•é›†: {df_test.shape[0]}")

X_train_raw = df_train.drop(columns=drop_cols)
y_train = df_train['next_5day_return'].values

feature_names = X_train_raw.columns.tolist()
print(f"åˆå§‹ç‰¹å¾æ•°: {len(feature_names)}")

# ===== å®½æ¾è¿‡æ»¤ç­–ç•¥ï¼šä¿ç•™æ›´å¤šåŸå§‹ç‰¹å¾ =====
print("\n[ç‰¹å¾è¿‡æ»¤ç­–ç•¥] å®½æ¾æ¨¡å¼ï¼ˆä¿ç•™æ›´å¤šä¿¡æ¯ï¼‰...")

# ===== Step 1: ç¼ºå¤±å€¼è¿‡æ»¤ï¼ˆé˜ˆå€¼90%ï¼Œéå¸¸å®½æ¾ï¼‰=====
print("\n[Step 1] ç¼ºå¤±å€¼è¿‡æ»¤ï¼ˆé˜ˆå€¼90%ï¼‰...")
missing_ratio = X_train_raw.isnull().mean()
valid_features = missing_ratio[missing_ratio < 0.9].index.tolist()
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(feature_names) - len(valid_features)})")

X_train_filtered = X_train_raw[valid_features]

# ===== Step 2: æ–¹å·®è¿‡æ»¤ï¼ˆé˜ˆå€¼0.0001ï¼Œæ›´å®½æ¾ï¼‰=====
print("\n[Step 2] æ–¹å·®è¿‡æ»¤ï¼ˆé˜ˆå€¼0.001ï¼‰...")
variances = X_train_filtered.var()
valid_features = variances[variances > 0.001].index.tolist()
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(X_train_filtered.columns) - len(valid_features)})")

X_train_filtered = X_train_filtered[valid_features]

# ===== Step 3: ç›¸å…³æ€§è¿‡æ»¤ï¼ˆåˆ é™¤é«˜ç›¸å…³ç‰¹å¾å¯¹ï¼‰=====
print("\n[Step 3] ç›¸å…³æ€§è¿‡æ»¤ï¼ˆé˜ˆå€¼0.95ï¼‰...")
corr_matrix = X_train_filtered.corr().abs()
upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
try:
    corr_with_y = {}
    for col in X_train_filtered.columns:
        try:
            corr = np.corrcoef(X_train_filtered[col].values, y_train)[0, 1]
            corr_with_y[col] = abs(corr) if not np.isnan(corr) else 0.0
        except:
            corr_with_y[col] = 0.0
except Exception as e:
    print(f"  è­¦å‘Šï¼šè®¡ç®—ä¸ç›®æ ‡ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
    corr_with_y = {col: 0.0 for col in X_train_filtered.columns}

# æ‰¾å‡ºé«˜ç›¸å…³ç‰¹å¾å¯¹
to_drop = set()
for column in upper_triangle.columns:
    if upper_triangle[column].max() > 0.95:
        high_corr_features = upper_triangle.index[upper_triangle[column] > 0.95].tolist()
        if len(high_corr_features) > 0:
            # è·å–æ‰€æœ‰ç›¸å…³ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
            all_features = high_corr_features + [column]
            feature_corrs = [(f, corr_with_y.get(f, 0.0)) for f in all_features]
            # æŒ‰ä¸ç›®æ ‡çš„ç›¸å…³æ€§æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„ï¼Œåˆ é™¤å…¶ä»–
            feature_corrs.sort(key=lambda x: x[1], reverse=True)
            to_drop.update([f for f, _ in feature_corrs[1:]])  # åˆ é™¤é™¤äº†æœ€ç›¸å…³çš„

to_drop = list(to_drop)
valid_features = [f for f in X_train_filtered.columns if f not in to_drop]
print(f"  ä¿ç•™ç‰¹å¾: {len(valid_features)} (åˆ é™¤ {len(to_drop)})")

X_train_filtered = X_train_filtered[valid_features]

# ===== Step 4: VIFè¿‡æ»¤ï¼ˆå®½æ¾é˜ˆå€¼ï¼‰=====
print("\n[Step 4] VIFè¿‡æ»¤ï¼ˆé˜ˆå€¼10ï¼Œå®½æ¾ï¼‰...")
current_features = X_train_filtered.columns.tolist()

# æ ‡å‡†åŒ–ï¼ˆVIFéœ€è¦ï¼‰
scaler_vif = StandardScaler()
X_scaled = scaler_vif.fit_transform(X_train_filtered)
X_scaled_df = pd.DataFrame(X_scaled, columns=current_features)

iteration = 0
max_iterations = 30  # å‡å°‘è¿­ä»£æ¬¡æ•°

while len(current_features) > 42:  # ä¿ç•™æ›´å¤šç‰¹å¾ï¼ˆè‡³å°‘42ä¸ªï¼‰
    iteration += 1
    if iteration > max_iterations:
        print(f"  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢")
        break
    
    # è®¡ç®—VIF
    vif_data = pd.DataFrame()
    vif_data["Feature"] = current_features
    vif_data["VIF"] = [variance_inflation_factor(X_scaled_df[current_features].values, i) 
                       for i in range(len(current_features))]
    
    max_vif = vif_data["VIF"].max()
    
    if max_vif > 10:  # æ›´å®½æ¾çš„VIFé˜ˆå€¼
        # åˆ é™¤VIFæœ€å¤§çš„ç‰¹å¾
        feature_to_drop = vif_data.loc[vif_data["VIF"].idxmax(), "Feature"]
        current_features.remove(feature_to_drop)
        print(f"  è¿­ä»£{iteration}: åˆ é™¤ {feature_to_drop} (VIF={max_vif:.2f}), å‰©ä½™{len(current_features)}ä¸ªç‰¹å¾")
    else:
        print(f"  æ‰€æœ‰ç‰¹å¾VIF < 10ï¼Œåœæ­¢")
        break

print(f"  æœ€ç»ˆä¿ç•™ç‰¹å¾: {len(current_features)}")
X_train_filtered = X_train_filtered[current_features]

# ===== Step 5: äº’ä¿¡æ¯ç­›é€‰ï¼ˆåŒç‰ˆæœ¬ï¼šä¼ ç»ŸML vs æ·±åº¦å­¦ä¹ ï¼‰=====
print("\n[Step 5] äº’ä¿¡æ¯ç­›é€‰ï¼ˆç”Ÿæˆä¸¤å¥—ç‰¹å¾é›†ï¼‰...")
# æ ‡å‡†åŒ–
scaler_mi = StandardScaler()
X_scaled_mi = scaler_mi.fit_transform(X_train_filtered)

# è®¡ç®—äº’ä¿¡æ¯
mi_scores = mutual_info_regression(X_scaled_mi, y_train, random_state=RANDOM_STATE)
mi_scores_df = pd.DataFrame({'feature': X_train_filtered.columns, 'mi_score': mi_scores})
mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)

# ç‰ˆæœ¬1ï¼šä¼ ç»ŸMLï¼ˆä¿ç•™50%ï¼Œ~27ç‰¹å¾ï¼‰- æ›´æ¿€è¿›ç­›é€‰
n_keep_ml = int(len(mi_scores_df) * 0.50)
selected_features_ml = mi_scores_df.head(n_keep_ml)['feature'].tolist()
print(f"  [ä¼ ç»ŸML] ä¿ç•™top 50%: {len(selected_features_ml)}ç‰¹å¾ (æ ·æœ¬æ•°/10={len(y_train)//10})")
print(f"    Top 10: {selected_features_ml[:10]}")

# ç‰ˆæœ¬2ï¼šæ·±åº¦å­¦ä¹ ï¼ˆä¿ç•™20%ï¼Œ~11ç‰¹å¾ï¼‰- æç®€æ¨¡å¼ï¼Œé¿å…è¿‡æ‹Ÿåˆ
n_keep_dl = int(len(mi_scores_df) * 0.20)
selected_features_dl = mi_scores_df.head(n_keep_dl)['feature'].tolist()
print(f"  [æ·±åº¦å­¦ä¹ ] ä¿ç•™top 20%: {len(selected_features_dl)}ç‰¹å¾ (æ ·æœ¬æ•°/50={len(y_train)//50})")
print(f"    æ·±åº¦å­¦ä¹ ç‰¹å¾: {selected_features_dl}")

X_train_final = X_train_filtered[selected_features_ml]  # ä¼ ç»ŸMLç”¨
X_train_final_dl = X_train_filtered[selected_features_dl]  # æ·±åº¦å­¦ä¹ ç”¨

print(f"\nâœ… ç‰¹å¾è¿‡æ»¤å®Œæˆ: {len(feature_names)} â†’ ML:{X_train_final.shape[1]} / DL:{X_train_final_dl.shape[1]}")

# =============================================================================
# ç¬¬ä¸‰é˜¶æ®µï¼šå‡†å¤‡æœ€ç»ˆæ•°æ®ï¼ˆä¼ ç»ŸMLå’Œæ·±åº¦å­¦ä¹ åˆ†åˆ«å‡†å¤‡ï¼‰
# =============================================================================
print("\n[é˜¶æ®µ3] å‡†å¤‡è®­ç»ƒæ•°æ®...")

# ===== ä¼ ç»ŸMLæ•°æ®é›†ï¼ˆ80%ç‰¹å¾ï¼‰=====
X_val_final = df_val[selected_features_ml]
X_test_final = df_test[selected_features_ml]

# ===== æ·±åº¦å­¦ä¹ æ•°æ®é›†ï¼ˆ50%ç‰¹å¾ï¼‰=====
X_val_final_dl = df_val[selected_features_dl]
X_test_final_dl = df_test[selected_features_dl]

# ç›®æ ‡å˜é‡ï¼ˆä¸¤å¥—å…±ç”¨ï¼‰
y_val = df_val['next_5day_return'].values
y_test = df_test['next_5day_return'].values

print(f"è®­ç»ƒé›†: {X_train_final.shape[0]}, éªŒè¯é›†: {X_val_final.shape[0]}, æµ‹è¯•é›†: {X_test_final.shape[0]}")
print(f"ä¼ ç»ŸMLç‰¹å¾æ•°: {X_train_final.shape[1]}")
print(f"æ·±åº¦å­¦ä¹ ç‰¹å¾æ•°: {X_train_final_dl.shape[1]}")

# ===== ä¼ ç»ŸMLæ ‡å‡†åŒ– =====
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_final)
X_val_scaled = scaler.transform(X_val_final)
X_test_scaled = scaler.transform(X_test_final)

# ===== æ·±åº¦å­¦ä¹ æ ‡å‡†åŒ– =====
scaler_dl = StandardScaler()
X_train_scaled_dl = scaler_dl.fit_transform(X_train_final_dl)
X_val_scaled_dl = scaler_dl.transform(X_val_final_dl)
X_test_scaled_dl = scaler_dl.transform(X_test_final_dl)

# =============================================================================
# ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–
# =============================================================================

# æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰
class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

class SimpleGRU(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(SimpleGRU, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                         batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        last_output = gru_out[:, -1, :]
        last_output = self.dropout(last_output)
        return self.fc(last_output)

def create_sequences(X, y, seq_len=10):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len + 1):
        X_seq.append(X[i:i+seq_len])
        y_seq.append(y[i+seq_len-1])
    return np.array(X_seq), np.array(y_seq)

def train_model(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test,
               epochs=100, lr=0.001, batch_size=16):
    """è®­ç»ƒæ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    try:
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        
        # é™ä½batch_sizeåˆ°16ï¼Œä¸drop_lastä»¥ä½¿ç”¨æ‰€æœ‰æ•°æ®
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)
        
        criterion = nn.HuberLoss(delta=1.0)
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # å¢åŠ æ­£åˆ™åŒ–
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8)
        
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        patience = 20  # å¢åŠ è€å¿ƒ
        
        for epoch in range(epochs):
            model.train()
            train_loss = 0
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    outputs = model(X_batch)
                    loss = criterion(outputs, y_batch)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            scheduler.step(val_loss)
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        model.eval()
        with torch.no_grad():
            predictions = model(X_test).cpu().numpy().flatten()
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        
        return {
            'model': model,
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions
        }
    except Exception as e:
        print(f"  âŒ {str(e)}")
        return None

print("\n[é˜¶æ®µ4] æ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–...")
print("="*80)

# å‡†å¤‡åºåˆ—æ•°æ®ï¼ˆæ·±åº¦å­¦ä¹ ä½¿ç”¨50%ç‰¹å¾é›†ï¼‰
seq_len = 20  # å¢åŠ åºåˆ—é•¿åº¦ï¼Œæ•æ‰æ›´é•¿æœŸè¶‹åŠ¿
input_size = X_train_scaled_dl.shape[1]  # ä½¿ç”¨æ·±åº¦å­¦ä¹ ç‰¹å¾é›†

print(f"åºåˆ—é•¿åº¦: {seq_len}, ç‰¹å¾ç»´åº¦: {input_size} (DLä¸“ç”¨)")

X_train_seq, y_train_seq = create_sequences(X_train_scaled_dl, y_train, seq_len)
X_val_seq, y_val_seq = create_sequences(X_val_scaled_dl, y_val, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_scaled_dl, y_test, seq_len)

print(f"åºåˆ—æ•°æ®é‡: è®­ç»ƒ={len(X_train_seq)}, éªŒè¯={len(X_val_seq)}, æµ‹è¯•={len(X_test_seq)}")

X_train_seq_tensor = torch.FloatTensor(X_train_seq).to(device)
X_val_seq_tensor = torch.FloatTensor(X_val_seq).to(device)
X_test_seq_tensor = torch.FloatTensor(X_test_seq).to(device)
y_train_seq_tensor = torch.FloatTensor(y_train_seq).unsqueeze(1).to(device)
y_val_seq_tensor = torch.FloatTensor(y_val_seq).unsqueeze(1).to(device)

all_results = {}

# ===== å®Œæ•´åŸºçº¿æ¨¡å‹è®­ç»ƒ =====
print("\nè®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼‰...")

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# Ridgeè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ›´æ¿€è¿›çš„æœç´¢ï¼‰
print("  ä¼˜åŒ–Ridgeè¶…å‚æ•°...")
ridge_params = {'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]}
ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3, scoring='r2', n_jobs=-1)
ridge_grid.fit(X_train_scaled, y_train)
print(f"  â†’ Ridgeæœ€ä½³alpha={ridge_grid.best_params_['alpha']:.2f}, CV_RÂ²={ridge_grid.best_score_:.4f}")

baseline_models = {
    'Ridge': ridge_grid.best_estimator_,
    'Lasso': Lasso(alpha=0.01, max_iter=3000, random_state=RANDOM_STATE),
    'ElasticNet': ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=RANDOM_STATE, max_iter=3000),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE),
    'DecisionTree': DecisionTreeRegressor(
        max_depth=8, min_samples_split=30, min_samples_leaf=15,
        min_impurity_decrease=0.0005, ccp_alpha=0.005, random_state=RANDOM_STATE
    ),
    'SVR': SVR(kernel='rbf', C=15.0, epsilon=0.03, gamma='scale', max_iter=8000),
}

baseline_results = {}

# è®­ç»ƒç®€å•æ¨¡å‹
simple_models = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTree', 'SVR', 'GradientBoosting']
for name in simple_models:
    if name in baseline_models:
        model = baseline_models[name]
        print(f"\nè®­ç»ƒ {name}...")
        start_time = time.time()
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
        
        r2 = r2_score(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        elapsed_time = time.time() - start_time
        
        print(f"  âœ… RÂ²={r2:.4f}, RMSE={rmse:.6f}, MAE={mae:.6f} ({elapsed_time:.1f}ç§’)")
        
        baseline_results[name] = {
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'predictions': predictions,
            'time': elapsed_time,
            'model': model
        }

all_results.update(baseline_results)

# === æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢ï¼ˆæ¿€è¿›æ¢ç´¢ï¼‰===
print("\nğŸ” æ ‘æ¨¡å‹è¶…å‚æ•°æœç´¢ï¼ˆæ¿€è¿›æ¨¡å¼ï¼‰...")

# === XGBoostæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼Œæ›´æ¿€è¿›ï¼‰===
if XGBOOST_AVAILABLE:
    print("\n[XGBoost] æç®€æœç´¢ï¼ˆæµ…å±‚+å¼ºæ­£åˆ™ï¼‰...")
    xgb_search_configs = [
        # åŸºäº50_3æœ€ä¼˜ï¼Œæ¢ç´¢æ›´æµ…ã€æ›´å°‘æ ‘ã€æ›´å¼ºæ­£åˆ™
        # (n_estimators, max_depth, learning_rate, subsample, colsample_bytree, min_child_weight, reg_alpha, reg_lambda)
        (50, 3, 0.05, 0.8, 0.8, 3, 0.1, 1.0),     # ä¿ç•™æœ€ä¼˜baseline
        (40, 3, 0.05, 0.8, 0.8, 3, 0.1, 1.0),     # å‡å°‘æ ‘
        (30, 3, 0.05, 0.8, 0.8, 3, 0.1, 1.0),     # æ›´å°‘æ ‘
        (50, 2, 0.05, 0.8, 0.8, 3, 0.1, 1.0),     # æ›´æµ…depth=2
        (40, 2, 0.05, 0.8, 0.8, 4, 0.15, 1.5),    # è¶…æµ…+å¼ºæ­£åˆ™
        (30, 2, 0.05, 0.7, 0.7, 5, 0.2, 2.0),     # æç®€+æå¼ºæ­£åˆ™
        (50, 3, 0.03, 0.8, 0.8, 3, 0.1, 1.0),     # é™ä½lr
        (50, 3, 0.05, 0.7, 0.7, 4, 0.15, 1.5),    # é™subsample+å¼ºæ­£åˆ™
        (60, 3, 0.04, 0.8, 0.8, 3, 0.1, 1.0),     # ç•¥å¤šæ ‘+ä½lr
        (50, 3, 0.05, 0.8, 0.8, 5, 0.2, 2.0),     # é«˜min_child_weight
        (45, 2, 0.04, 0.75, 0.75, 4, 0.15, 1.5),  # ç»¼åˆä¿å®ˆ
        (35, 3, 0.05, 0.8, 0.8, 4, 0.12, 1.2),    # å¹³è¡¡ç»„åˆ
    ]
    
    xgb_search_results = []
    for i, (n_est, max_d, lr, sub, col, mcw, alpha, lamb) in enumerate(xgb_search_configs, 1):
        name = f"XGB_{n_est}_{max_d}_{int(lr*1000)}"
        print(f"  [{i}/12] {name}...", end=' ')
        model = xgb.XGBRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr,
            subsample=sub, colsample_bytree=col, min_child_weight=mcw,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            xgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0,
                'model': model
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    xgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_xgb = xgb_search_results[:3]
    print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_xgb]}")
    for name, result in top_3_xgb:
        all_results[name] = result

# === LightGBMæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼‰===
if LIGHTGBM_AVAILABLE:
    print("\n[LightGBM] æç®€æœç´¢ï¼ˆæµ…å±‚+å¼ºæ­£åˆ™ï¼‰...")
    lgb_search_configs = [
        # åŸºäº50_3æœ€ä¼˜ï¼Œæ¢ç´¢æµ…å±‚å°æ¨¡å‹
        # (n_estimators, max_depth, learning_rate, num_leaves, subsample, colsample_bytree, min_child_samples, reg_alpha, reg_lambda)
        (50, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # ä¿ç•™æœ€ä¼˜baseline
        (40, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # å‡å°‘æ ‘
        (30, 3, 0.05, 10, 0.8, 0.8, 20, 0.1, 1.0),   # æ›´å°‘æ ‘
        (50, 2, 0.05, 7, 0.8, 0.8, 25, 0.15, 1.5),   # è¶…æµ…depth=2
        (40, 2, 0.05, 7, 0.8, 0.8, 25, 0.15, 1.5),   # è¶…æµ…+å°‘æ ‘
        (30, 2, 0.05, 5, 0.7, 0.7, 30, 0.2, 2.0),    # æç®€é…ç½®
        (50, 3, 0.03, 10, 0.8, 0.8, 20, 0.1, 1.0),   # é™lr
        (50, 3, 0.05, 8, 0.7, 0.7, 25, 0.15, 1.5),   # å‡å¶å­+å¼ºæ­£åˆ™
        (60, 3, 0.04, 10, 0.8, 0.8, 20, 0.1, 1.0),   # ç•¥å¤šæ ‘+ä½lr
        (50, 3, 0.05, 10, 0.8, 0.8, 30, 0.2, 2.0),   # é«˜min_child_samples
        (45, 2, 0.04, 7, 0.75, 0.75, 25, 0.15, 1.5), # ç»¼åˆä¿å®ˆ
        (35, 3, 0.05, 10, 0.8, 0.8, 22, 0.12, 1.2),  # å¹³è¡¡ç»„åˆ
    ]
    
    lgb_search_results = []
    for i, (n_est, max_d, lr, n_leaves, sub, col, mcs, alpha, lamb) in enumerate(lgb_search_configs, 1):
        name = f"LGB_{n_est}_{max_d}_{int(lr*1000)}"
        print(f"  [{i}/12] {name}...", end=' ')
        model = lgb.LGBMRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr, num_leaves=n_leaves,
            subsample=sub, colsample_bytree=col, min_child_samples=mcs,
            reg_alpha=alpha, reg_lambda=lamb,
            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            lgb_search_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred,
                'time': 0,
                'model': model
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    lgb_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_lgb = lgb_search_results[:3]
    print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_lgb]}")
    for name, result in top_3_lgb:
        all_results[name] = result

# === RandomForestæœç´¢ï¼ˆ12ä¸ªé…ç½®ï¼‰===
print("\n[RandomForest] æµ…å±‚æœç´¢ï¼ˆdepth 5-8 + å¼ºå‰ªæï¼‰...")
rf_search_configs = [
    # åŸºäº100_8æœ€ä¼˜ï¼Œæ¢ç´¢æ›´æµ…æ ‘+æ›´å¼ºå‰ªæ
    # (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features)
    (100, 8, 15, 8, 'sqrt'),     # ä¿ç•™æœ€ä¼˜baseline
    (100, 7, 15, 8, 'sqrt'),     # æ›´æµ…depth=7
    (100, 6, 15, 8, 'sqrt'),     # æ›´æµ…depth=6
    (100, 5, 15, 8, 'sqrt'),     # è¶…æµ…depth=5
    (100, 8, 20, 10, 'sqrt'),    # æ›´å¼ºå‰ªæ
    (100, 7, 20, 10, 'sqrt'),    # æµ…+å¼ºå‰ªæ
    (100, 6, 25, 12, 'sqrt'),    # è¶…æµ…+è¶…å¼ºå‰ªæ
    (80, 7, 15, 8, 'sqrt'),      # å°‘æ ‘+æµ…å±‚
    (120, 7, 15, 8, 'sqrt'),     # å¤šæ ‘+æµ…å±‚
    (100, 8, 15, 6, 'log2'),     # ç‰¹å¾é€‰æ‹©log2
    (100, 7, 18, 9, 'sqrt'),     # å¹³è¡¡ç»„åˆ
    (100, 8, 10, 5, 'sqrt'),     # ç•¥å®½æ¾ï¼ˆå¯¹æ¯”ï¼‰
]

rf_search_results = []
for i, (n_est, max_d, mss, msl, max_f) in enumerate(rf_search_configs, 1):
    name = f"RF_{n_est}_{max_d}"
    print(f"  [{i}/12] {name}...", end=' ')
    model = RandomForestRegressor(
        n_estimators=n_est, max_depth=max_d,
        min_samples_split=mss, min_samples_leaf=msl,
        max_features=max_f,
        random_state=RANDOM_STATE, n_jobs=-1
    )
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.2:
        rf_search_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred,
            'time': 0,
            'model': model
        }))
        print(f"RÂ²={r2:.4f} âœ…")
    else:
        print("âŒ")

rf_search_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_rf = rf_search_results[:3]
print(f"  âœ… Top 3: {[(n, r['r2']) for n, r in top_3_rf]}")
for name, result in top_3_rf:
    all_results[name] = result

# LSTMä¼˜åŒ–ï¼ˆä¼˜åŒ–ç‰ˆ - æ›´å°æ¨¡å‹ã€æ›´ä½å­¦ä¹ ç‡ã€æ›´å¤šè®­ç»ƒï¼‰
print("\n[LSTM] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ”¹è¿›ç­–ç•¥ï¼šå°æ¨¡å‹+å……åˆ†è®­ç»ƒï¼‰...")
lstm_configs = [
    # (hidden_size, num_layers, dropout, lr, description)
    (16, 1, 0.2, 0.0005, "è¶…è½»é‡+ä½lr"),
    (24, 1, 0.25, 0.0005, "è½»é‡+ä½lr"),
    (32, 1, 0.2, 0.0003, "å°å•å±‚+æä½lr"),
    (16, 2, 0.3, 0.0005, "è½»é‡åŒå±‚"),
    (24, 2, 0.3, 0.0003, "å°åŒå±‚+ä½lr"),
    (32, 2, 0.3, 0.0005, "åŸºçº¿åŒå±‚"),
    (48, 1, 0.2, 0.0005, "ä¸­å•å±‚"),
    (32, 2, 0.25, 0.0003, "ä½dropout+ä½lr"),
    (40, 2, 0.3, 0.0005, "ä¸­å°åŒå±‚"),
    (24, 3, 0.35, 0.0003, "å°ä¸‰å±‚+ä½lr"),
    (16, 1, 0.15, 0.001, "è¶…è½»é‡+æ ‡å‡†lr"),
    (32, 1, 0.25, 0.0008, "å•å±‚å¹³è¡¡"),
]

lstm_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_configs, 1):
    name = f"LSTM_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=150, lr=lr, batch_size=16)  # æ›´å¤šè½®æ¬¡ï¼Œæ›´å°batch
    if result and result['r2'] > -0.5:  # æ”¾å®½é˜ˆå€¼ï¼Œè‚¡ç¥¨é¢„æµ‹RÂ²æœ¬æ¥å°±ä½
        lstm_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm = lstm_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_lstm]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_lstm:
    all_results[name] = result

# GRUä¼˜åŒ–ï¼ˆæ”¹è¿›ç‰ˆ - åŸºäºä¹‹å‰GRUè¡¨ç°è¾ƒå¥½çš„ç»éªŒï¼‰
print("\n[GRU] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆç²¾ç»†è°ƒä¼˜ï¼šåŸºäºå†å²æœ€ä¼˜ï¼‰...")
gru_configs = [
    # GRUä¹‹å‰GRU_32_3è¡¨ç°æœ€å¥½ï¼ˆRÂ²=-0.0608ï¼‰ï¼Œé‡ç‚¹ä¼˜åŒ–è¿™ä¸ªåŒºåŸŸ
    # (hidden_size, num_layers, dropout, lr, description)
    (32, 3, 0.3, 0.0003, "ä¼˜åŒ–åŸºçº¿-ä½lr"),
    (32, 3, 0.25, 0.0005, "ä¼˜åŒ–åŸºçº¿-é™dropout"),
    (32, 3, 0.35, 0.0003, "ä¼˜åŒ–åŸºçº¿-å¹³è¡¡"),
    (24, 3, 0.3, 0.0005, "å°ä¸‰å±‚"),
    (40, 3, 0.3, 0.0005, "ä¸­ä¸‰å±‚"),
    (32, 2, 0.25, 0.0005, "åŒå±‚è½»é‡"),
    (32, 4, 0.35, 0.0003, "å››å±‚æ·±åº¦"),
    (48, 3, 0.3, 0.0003, "ä¸­å¤§ä¸‰å±‚"),
    (32, 3, 0.2, 0.0005, "ä½dropoutä¸‰å±‚"),
    (32, 3, 0.3, 0.0008, "æ ‡å‡†ä¸‰å±‚"),
    (28, 3, 0.3, 0.0003, "ä¼˜åŒ–å°ºå¯¸"),
    (36, 3, 0.3, 0.0005, "å¾®è°ƒå°ºå¯¸"),
]

gru_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_configs, 1):
    name = f"GRU_{hs}_{nl}"
    print(f"  [{i}/{len(gru_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 100 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 100 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=150, lr=lr, batch_size=16)  # å……åˆ†è®­ç»ƒ
    if result and result['r2'] > -0.5:  # æ”¾å®½é˜ˆå€¼
        gru_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru = gru_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_gru]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_gru:
    all_results[name] = result

# XGBoostä¼˜åŒ–
if XGBOOST_AVAILABLE:
    print("\n[XGBoost] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆä¸°å¯Œç‰¹å¾ â†’ å¢åŠ æ ‘æ·±åº¦ï¼‰...")
    xgb_configs = [
        (50, 5, 0.05, "åŸºå‡†"),
        (50, 6, 0.05, "å¢åŠ æ·±åº¦"),
        (50, 7, 0.05, "æ›´æ·±"),
        (75, 6, 0.04, "å¢åŠ æ ‘æ•°+é™lr"),
        (100, 5, 0.03, "æ›´å¤šæ ‘+ä½lr"),
    ]
    
    xgb_results = []
    for i, (n_est, max_d, lr, desc) in enumerate(xgb_configs, 1):
        name = f"XGB_{n_est}_{max_d}"
        print(f"  [{i}/{len(xgb_configs)}] {name} ({desc})...", end=' ')
        model = xgb.XGBRegressor(
            n_estimators=n_est, max_depth=max_d, learning_rate=lr,
            subsample=0.8, colsample_bytree=0.8, min_child_weight=3,
            reg_alpha=0.1, reg_lambda=1.0,
            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0
        )
        model.fit(X_train_scaled, y_train)
        pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, pred)
        
        if r2 > -0.2:
            xgb_results.append((name, {
                'r2': r2,
                'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                'mae': mean_absolute_error(y_test, pred),
                'predictions': pred
            }))
            print(f"RÂ²={r2:.4f} âœ…")
        else:
            print("âŒ")
    
    xgb_results.sort(key=lambda x: x[1]['r2'], reverse=True)
    top_3_xgb = xgb_results[:3]
    top_3_info = [(n, r['r2']) for n, r in top_3_xgb]
    print(f"  ğŸ† Top 3: {top_3_info}")
    for name, result in top_3_xgb:
        all_results[name] = result

# RandomForestä¼˜åŒ–ï¼ˆé‡ç‚¹ï¼šRF_200_8è¡¨ç°æœ€å¥½ï¼Œå›´ç»•æ­¤é…ç½®æ·±åº¦ä¼˜åŒ–ï¼‰
print("\n[RandomForest] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ·±åº¦è°ƒä¼˜ï¼šåŸºäºå†å²æœ€ä¼˜RF_200_8ï¼‰...")
rf_configs = [
    # (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, description)
    (200, 8, 10, 5, 'sqrt', "åŸºå‡†æœ€ä¼˜"),
    (250, 8, 10, 5, 'sqrt', "æ›´å¤šæ ‘"),
    (200, 7, 10, 5, 'sqrt', "é™æ·±åº¦"),
    (200, 9, 10, 5, 'sqrt', "å¢æ·±åº¦"),
    (200, 8, 8, 4, 'sqrt', "é™å¶å­é™åˆ¶"),
    (200, 8, 12, 6, 'sqrt', "å¢å¶å­é™åˆ¶"),
    (200, 8, 10, 5, 'log2', "ç‰¹å¾é€‰æ‹©log2"),
    (200, 8, 10, 5, 0.7, "ç‰¹å¾é€‰æ‹©70%"),
    (300, 8, 10, 5, 'sqrt', "å¤§å¹…å¢æ ‘"),
    (200, 8, 15, 7, 'sqrt', "å¼ºæ­£åˆ™åŒ–"),
    (180, 8, 10, 5, 'sqrt', "å¾®è°ƒæ ‘æ•°"),
    (220, 8, 10, 5, 'sqrt', "å¾®è°ƒæ ‘æ•°+"),
]

rf_results = []
for i, (n_est, max_d, min_split, min_leaf, max_feat, desc) in enumerate(rf_configs, 1):
    name = f"RF_{n_est}_{max_d}"
    print(f"  [{i}/{len(rf_configs)}] {name} ({desc})...", end=' ')
    model = RandomForestRegressor(
        n_estimators=n_est, max_depth=max_d,
        min_samples_split=min_split, min_samples_leaf=min_leaf,
        max_features=max_feat,
        random_state=RANDOM_STATE, n_jobs=-1, oob_score=True
    )
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.2:
        rf_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred,
            'oob_score': model.oob_score_
        }))
        print(f"RÂ²={r2:.4f}, OOB={model.oob_score_:.4f} âœ…")
    else:
        print(f"RÂ²={r2:.4f} âŒ")

rf_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_rf = rf_results[:3]
top_3_info = [(n, r['r2']) for n, r in top_3_rf]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result in top_3_rf:
    all_results[name] = result

# ===== ç¬¬äºŒè½®æ¿€è¿›æœç´¢ï¼šåŸºäºå½“å‰æœ€ä¼˜ç»“æœå¾®è°ƒ =====
print("\n[ç¬¬äºŒè½®ä¼˜åŒ–] åŸºäºTopæ¨¡å‹ç²¾ç»†è°ƒä¼˜...")

# GRUç¬¬äºŒè½®ï¼šå›´ç»•GRU_32_3(RÂ²=0.0086)ç²¾ç»†æœç´¢
print("\n[GRU Round 2] å›´ç»•æœ€ä¼˜GRU_32_3æ·±åº¦ä¼˜åŒ–...")
gru_round2_configs = [
    # å›´ç»•32_3å¾®è°ƒ
    (30, 3, 0.30, 0.0003, "32-2å¾®è°ƒå°ºå¯¸"),
    (34, 3, 0.30, 0.0003, "32+2å¾®è°ƒå°ºå¯¸"),
    (32, 3, 0.28, 0.0003, "é™dropout-ä½"),
    (32, 3, 0.32, 0.0003, "å‡dropout-ä½"),
    (32, 3, 0.30, 0.00025, "é™lr"),
    (32, 3, 0.30, 0.00035, "å‡lr"),
    (32, 3, 0.30, 0.0004, "å†å‡lr"),
    (28, 3, 0.30, 0.0003, "æ›´å°"),
    (36, 3, 0.28, 0.0003, "36å¾®è°ƒ"),
    (32, 4, 0.32, 0.0003, "åŠ æ·±å±‚"),
    (40, 3, 0.28, 0.0003, "40å¾®è°ƒ"),
    (32, 3, 0.25, 0.00025, "æç®€ç»„åˆ"),
]

gru_r2_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_round2_configs, 1):
    name = f"GRU_R2_{hs}_{nl}"
    print(f"  [{i}/{len(gru_round2_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 200 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 200 + i)
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        gru_r2_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru_r2 = gru_r2_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_gru_r2]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_gru_r2:
    all_results[name] = result

# LSTMç¬¬äºŒè½®ï¼šå›´ç»•LSTM_16_2(RÂ²=0.0011)è½»é‡åŒ–æœç´¢
print("\n[LSTM Round 2] å›´ç»•è½»é‡LSTMä¼˜åŒ–...")
lstm_round2_configs = [
    (16, 2, 0.30, 0.0005, "åŸºå‡†é‡ç°"),
    (14, 2, 0.30, 0.0005, "æ›´è½»"),
    (18, 2, 0.30, 0.0005, "ç•¥é‡"),
    (16, 2, 0.25, 0.0005, "é™dropout"),
    (16, 2, 0.35, 0.0005, "å‡dropout"),
    (16, 2, 0.30, 0.0004, "é™lr"),
    (16, 2, 0.30, 0.0006, "å‡lr"),
    (20, 2, 0.28, 0.0005, "20è½»é‡"),
    (12, 2, 0.25, 0.0005, "è¶…è½»"),
    (16, 3, 0.32, 0.0005, "åŠ æ·±"),
    (16, 1, 0.20, 0.0005, "å•å±‚"),
    (18, 2, 0.25, 0.0004, "å¹³è¡¡ç»„åˆ"),
]

lstm_r2_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_round2_configs, 1):
    name = f"LSTM_R2_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_round2_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 300 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 300 + i)
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=150, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        lstm_r2_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm_r2 = lstm_r2_results[:3]
top_3_info = [(n, r['r2']) for n, r, d in top_3_lstm_r2]
print(f"  ğŸ† Top 3: {top_3_info}")
for name, result, _ in top_3_lstm_r2:
    all_results[name] = result

# Ridgeç¬¬äºŒè½®ï¼šæ›´å¤šalphaå°è¯•
print("\n[Ridge Round 2] ç²¾ç»†è°ƒä¼˜æ­£åˆ™åŒ–...")
ridge_alphas = [80, 90, 95, 100, 105, 110, 120, 150, 200, 250]
ridge_r2_results = []
for alpha in ridge_alphas:
    name = f"Ridge_{alpha}"
    model = Ridge(alpha=alpha, random_state=RANDOM_STATE)
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.1:
        ridge_r2_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred
        }))
        print(f"  {name}: RÂ²={r2:.4f}")

ridge_r2_results.sort(key=lambda x: x[1]['r2'], reverse=True)
if ridge_r2_results:
    print(f"  ğŸ† æœ€ä½³: {ridge_r2_results[0][0]} (RÂ²={ridge_r2_results[0][1]['r2']:.4f})")
    all_results[ridge_r2_results[0][0]] = ridge_r2_results[0][1]

# ===== ç¬¬ä¸‰è½®æ¿€è¿›æœç´¢ï¼šåŸºäºR1æœ€ä¼˜ï¼ˆè€ŒéR2ï¼‰æ‰©å¤§æœç´¢ç©ºé—´ =====
print("\n[ç¬¬ä¸‰è½®ä¼˜åŒ–] æ¿€è¿›æ‰©å±•æœç´¢ç©ºé—´ï¼ˆåŸºäºRound 1æœ€ä¼˜ï¼‰...")

# GRUç¬¬ä¸‰è½®ï¼šæ¿€è¿›æ¢ç´¢ï¼Œæ”¾å¼ƒå¾®è°ƒï¼Œæ‰©å¤§æœç´¢ç©ºé—´
print("\n[GRU Round 3] æ¿€è¿›æœç´¢ï¼ˆæ›´å¤§èŒƒå›´+æ›´å¤šå˜åŒ–ï¼‰...")
gru_round3_configs = [
    # åŸå§‹GRU_32_3(0.0086)æ˜¯baselineï¼Œç°åœ¨æ¢ç´¢å®Œå…¨ä¸åŒçš„åŒºåŸŸ
    # æ›´æ·±çš„ç½‘ç»œ (4-6å±‚)
    (24, 4, 0.35, 0.0002, "æ·±4å±‚-è¶…ä½lr"),
    (32, 5, 0.40, 0.0002, "æ·±5å±‚-é«˜dropout"),
    (40, 4, 0.35, 0.00025, "æ·±4å±‚-å¤§hidden"),
    # æ›´å®½çš„ç½‘ç»œ (hidden 50-80)
    (64, 2, 0.25, 0.0003, "å®½64-æµ…2å±‚"),
    (80, 2, 0.20, 0.0002, "è¶…å®½80-æä½dropout"),
    (96, 1, 0.15, 0.0003, "å·¨å®½96-å•å±‚"),
    # æç®€ç½‘ç»œ (hidden 16-20)
    (16, 3, 0.25, 0.0005, "æç®€16-3å±‚"),
    (20, 4, 0.30, 0.0003, "å°20-æ·±4å±‚"),
    (24, 2, 0.20, 0.0005, "å°24-ä½dropout"),
    # ç»„åˆæ¢ç´¢
    (48, 3, 0.25, 0.0004, "ä¸­48-ä½dropout"),
    (32, 6, 0.45, 0.0002, "è¶…æ·±6å±‚-å¼ºæ­£åˆ™"),
    (56, 3, 0.30, 0.0003, "ä¸­å¤§56-3å±‚"),
    # epochsåŠ å€ï¼ˆå…³é”®ï¼ï¼‰
    (32, 3, 0.30, 0.0003, "åŸæœ€ä¼˜-epochs*2"),  # ä¼šåœ¨ä¸‹é¢ç‰¹æ®Šå¤„ç†
]

gru_r3_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(gru_round3_configs, 1):
    name = f"GRU_R3_{hs}_{nl}"
    print(f"  [{i}/{len(gru_round3_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 400 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 400 + i)
    
    # åŸæœ€ä¼˜é…ç½®epochsåŠ å€
    epochs_use = 300 if "epochs*2" in desc else 150
    
    model = SimpleGRU(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=epochs_use, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        gru_r3_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

gru_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_gru_r3 = gru_r3_results[:5]  # ä¿ç•™Top 5
top_5_info = [(n, r['r2']) for n, r, d in top_3_gru_r3]
print(f"  ğŸ† Top 5: {top_5_info}")
for name, result, _ in top_3_gru_r3:
    all_results[name] = result

# LSTMç¬¬ä¸‰è½®ï¼šæ¿€è¿›æ¢ç´¢ä¸åŒæ¶æ„
print("\n[LSTM Round 3] æ¿€è¿›æœç´¢ï¼ˆçªç ´è½»é‡é™åˆ¶ï¼‰...")
lstm_round3_configs = [
    # LSTM_16_2(0.0011)æ˜¯baselineï¼Œæ¢ç´¢å®Œå…¨ä¸åŒæ–¹å‘
    # æ›´æ·±ç½‘ç»œ
    (20, 3, 0.35, 0.0004, "æ·±3å±‚"),
    (24, 4, 0.40, 0.0003, "æ·±4å±‚-é«˜dropout"),
    (16, 4, 0.35, 0.0004, "è½»é‡æ·±4å±‚"),
    # æ›´å®½ç½‘ç»œ
    (40, 2, 0.25, 0.0005, "å®½40"),
    (56, 2, 0.20, 0.0004, "æ›´å®½56"),
    (64, 1, 0.15, 0.0005, "è¶…å®½64-å•å±‚"),
    # æç®€
    (8, 2, 0.20, 0.0006, "è¶…è½»8"),
    (10, 3, 0.25, 0.0005, "è½»10-æ·±3å±‚"),
    (12, 1, 0.15, 0.0008, "è½»12-å•å±‚-é«˜lr"),
    # ç»„åˆ
    (32, 3, 0.30, 0.0003, "ä¸­32-3å±‚"),
    (48, 2, 0.25, 0.0004, "ä¸­48-2å±‚"),
    (16, 2, 0.30, 0.0005, "åŸæœ€ä¼˜-epochs*2"),  # epochsåŠ å€
]

lstm_r3_results = []
for i, (hs, nl, dp, lr, desc) in enumerate(lstm_round3_configs, 1):
    name = f"LSTM_R3_{hs}_{nl}"
    print(f"  [{i}/{len(lstm_round3_configs)}] {name} ({desc})...", end=' ')
    torch.manual_seed(RANDOM_STATE + 500 + i)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(RANDOM_STATE + 500 + i)
    
    epochs_use = 300 if "epochs*2" in desc else 150
    
    model = SimpleLSTM(input_size, hs, nl, dp).to(device)
    result = train_model(model, name, X_train_seq_tensor, y_train_seq_tensor,
                        X_val_seq_tensor, y_val_seq_tensor,
                        X_test_seq_tensor, y_test_seq,
                        epochs=epochs_use, lr=lr, batch_size=16)
    if result and result['r2'] > -0.5:
        lstm_r3_results.append((name, result, desc))
        print(f"RÂ²={result['r2']:.4f} âœ…")
    else:
        if result:
            print(f"RÂ²={result['r2']:.4f} (å¤ªä½)")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")

lstm_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
top_3_lstm_r3 = lstm_r3_results[:5]
top_5_info = [(n, r['r2']) for n, r, d in top_3_lstm_r3]
print(f"  ğŸ† Top 5: {top_5_info}")
for name, result, _ in top_3_lstm_r3:
    all_results[name] = result

# Ridgeç¬¬ä¸‰è½®ï¼šæç«¯æ­£åˆ™åŒ–
print("\n[Ridge Round 3] æç«¯æ­£åˆ™åŒ–ï¼ˆalpha 300-2000ï¼‰...")
ridge_alphas_r3 = [300, 400, 500, 750, 1000, 1500, 2000, 3000]
ridge_r3_results = []
for alpha in ridge_alphas_r3:
    name = f"Ridge_{alpha}"
    model = Ridge(alpha=alpha, random_state=RANDOM_STATE)
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, pred)
    
    if r2 > -0.1:
        ridge_r3_results.append((name, {
            'r2': r2,
            'rmse': np.sqrt(mean_squared_error(y_test, pred)),
            'mae': mean_absolute_error(y_test, pred),
            'predictions': pred
        }))
        print(f"  {name}: RÂ²={r2:.4f}")

ridge_r3_results.sort(key=lambda x: x[1]['r2'], reverse=True)
if ridge_r3_results:
    print(f"  ğŸ† æœ€ä½³: {ridge_r3_results[0][0]} (RÂ²={ridge_r3_results[0][1]['r2']:.4f})")
    for name, result in ridge_r3_results[:3]:  # Top 3
        all_results[name] = result

# === é›†æˆå­¦ä¹ ï¼ˆä¸¤ç§ç­–ç•¥å¯¹æ¯”ï¼‰===
print("\n[é›†æˆå­¦ä¹ ] å¼‚è´¨æ¨¡å‹é›†æˆ...")

# æ•´åˆæ‰€æœ‰æ¨¡å‹ç»“æœ
all_models_for_ensemble = {}

# ä¼ ç»ŸMLæ¨¡å‹ï¼šéœ€è¦æˆªå–åˆ°åºåˆ—é•¿åº¦
for name, result in all_results.items():
    if not name.startswith(('LSTM', 'GRU')):  # ä¼ ç»ŸML
        predictions_aligned = result['predictions'][-len(y_test_seq):]
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': predictions_aligned,
            'type': 'ML',
            'model': result.get('model')
        }
    else:  # æ·±åº¦å­¦ä¹ 
        all_models_for_ensemble[name] = {
            'r2': result['r2'],
            'predictions': result['predictions'],
            'type': 'DL',
            'model': result.get('model')
        }

# é€‰æ‹©Topæ¨¡å‹ï¼ˆè‡³å°‘RÂ²>0.05ï¼‰
sorted_all = sorted(all_models_for_ensemble.items(), key=lambda x: x[1]['r2'], reverse=True)
selected_models = []
gru_selected = False
lstm_selected = False

for model_name, model_result in sorted_all:
    if model_result['r2'] > 0.05:  # åªé€‰æ‹©RÂ²>0.05çš„æ¨¡å‹
        # GRUå’ŒLSTMå„é€‰ä¸€ä¸ªæœ€ä½³
        if 'GRU' in model_name:
            if not gru_selected:
                selected_models.append((model_name, model_result))
                gru_selected = True
        elif 'LSTM' in model_name:
            if not lstm_selected:
                selected_models.append((model_name, model_result))
                lstm_selected = True
        else:
            selected_models.append((model_name, model_result))
    
    if len(selected_models) >= 5:  # æœ€å¤š5ä¸ªæ¨¡å‹
        break

# è‡³å°‘ä¿ç•™Top 3
if len(selected_models) < 3:
    for model_name, model_result in sorted_all:
        if (model_name, model_result) not in selected_models:
            if ('GRU' in model_name and gru_selected) or ('LSTM' in model_name and lstm_selected):
                continue
            selected_models.append((model_name, model_result))
            if len(selected_models) >= 3:
                break

if len(selected_models) >= 2:
    print(f"  ä½¿ç”¨{len(selected_models)}ä¸ªå¼‚è´¨æ¨¡å‹:")
    for i, (name, result) in enumerate(selected_models, 1):
        model_type = "ä¼ ç»ŸML" if result['type'] == 'ML' else "æ·±åº¦å­¦ä¹ "
        print(f"    {i}. {name}: RÂ²={result['r2']:.4f} ({model_type})")
    
    # === ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆ ===
    print("\n  ã€ç­–ç•¥1: RÂ²Â³åŠ æƒé›†æˆã€‘")
    r2_values = np.array([result['r2'] for _, result in selected_models])
    weights_r2 = r2_values ** 3
    weights_r2 = weights_r2 / weights_r2.sum()
    print(f"  æƒé‡åˆ†é…: {dict(zip([name for name, _ in selected_models], weights_r2.round(3)))}")
    
    ensemble_pred_r2 = np.zeros_like(selected_models[0][1]['predictions'])
    for (name, result), w in zip(selected_models, weights_r2):
        ensemble_pred_r2 += w * result['predictions']
    
    ensemble_r2_weighted = r2_score(y_test_seq, ensemble_pred_r2)
    ensemble_rmse_weighted = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_r2))
    ensemble_mae_weighted = mean_absolute_error(y_test_seq, ensemble_pred_r2)
    
    print(f"  ç»“æœ: RÂ²={ensemble_r2_weighted:.4f}, RMSE={ensemble_rmse_weighted:.6f}, MAE={ensemble_mae_weighted:.6f}")
    
    # === ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰===
    print("\n  ã€ç­–ç•¥2: æœ€å°äºŒä¹˜æ³•é›†æˆï¼ˆStackingï¼‰ã€‘")
    from sklearn.linear_model import LinearRegression
    
    # æ„å»ºè®­ç»ƒé›†é¢„æµ‹çŸ©é˜µï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰
    # æ·±åº¦å­¦ä¹ å’Œä¼ ç»ŸMLåˆ†åˆ«åˆ›å»ºæ•°æ®
    X_val_seq_dl, y_val_seq_dl = create_sequences(X_val_scaled_dl, y_val, seq_len)  # æ·±åº¦å­¦ä¹ 10ç»´
    X_val_seq_ml, y_val_seq_ml = create_sequences(X_val_scaled, y_val, seq_len)  # ä¼ ç»ŸML27ç»´
    
    stacking_train_preds = []
    for name, result in selected_models:
        model_obj = result.get('model')
        if model_obj is not None:
            if name.startswith(('LSTM', 'GRU')):  # æ·±åº¦å­¦ä¹ 
                model_obj.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val_seq_dl).to(device)
                    val_pred = model_obj(X_val_tensor).cpu().numpy().flatten()
                    stacking_train_preds.append(val_pred)
            else:  # ä¼ ç»ŸML
                val_pred = model_obj.predict(X_val_scaled)[-len(y_val_seq_ml):]
                stacking_train_preds.append(val_pred)
    
    # æ„å»ºæµ‹è¯•é›†é¢„æµ‹çŸ©é˜µ
    stacking_test_preds = []
    for name, result in selected_models:
        test_pred = result['predictions']
        stacking_test_preds.append(test_pred)
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨
    if len(stacking_train_preds) >= len(selected_models):
        X_stacking_train = np.column_stack(stacking_train_preds)
        X_stacking_test = np.column_stack(stacking_test_preds)
        
        print(f"  è®­ç»ƒé›†çŸ©é˜µ: {X_stacking_train.shape}, æµ‹è¯•é›†çŸ©é˜µ: {X_stacking_test.shape}")
        
        meta_learner = LinearRegression()
        meta_learner.fit(X_stacking_train, y_val_seq_dl)  # ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„yåºåˆ—
        
        # è·å–æƒé‡
        weights_ols = meta_learner.coef_
        weights_ols = np.maximum(weights_ols, 0)
        if weights_ols.sum() > 0:
            weights_ols = weights_ols / weights_ols.sum()
        
        print(f"  æƒé‡åˆ†é…: {dict(zip([name for name, _ in selected_models], weights_ols.round(3)))}")
        
        ensemble_pred_ols = meta_learner.predict(X_stacking_test)
        
        ensemble_r2_ols = r2_score(y_test_seq, ensemble_pred_ols)
        ensemble_rmse_ols = np.sqrt(mean_squared_error(y_test_seq, ensemble_pred_ols))
        ensemble_mae_ols = mean_absolute_error(y_test_seq, ensemble_pred_ols)
        
        print(f"  ç»“æœ: RÂ²={ensemble_r2_ols:.4f}, RMSE={ensemble_rmse_ols:.6f}, MAE={ensemble_mae_ols:.6f}")
    else:
        print(f"  âš ï¸ éªŒè¯é›†é¢„æµ‹ä¸è¶³ï¼Œä½¿ç”¨RÂ²Â³åŠ æƒç»“æœ")
        ensemble_r2_ols = ensemble_r2_weighted
        ensemble_rmse_ols = ensemble_rmse_weighted
        ensemble_mae_ols = ensemble_mae_weighted
        ensemble_pred_ols = ensemble_pred_r2
    
    # å¯¹æ¯”ä¸¤ç§ç­–ç•¥
    print(f"\n  ã€ç­–ç•¥å¯¹æ¯”ã€‘")
    print(f"  RÂ²Â³åŠ æƒ: RÂ²={ensemble_r2_weighted:.4f}")
    print(f"  æœ€å°äºŒä¹˜: RÂ²={ensemble_r2_ols:.4f}")
    print(f"  æœ€ä½³åŸºçº¿: RÂ²={sorted_all[0][1]['r2']:.4f}")
    
    # ä¿å­˜ä¸¤ç§ç­–ç•¥çš„ç»“æœ
    all_results['Ensemble_R2Â³'] = {
        'r2': ensemble_r2_weighted,
        'rmse': ensemble_rmse_weighted,
        'mae': ensemble_mae_weighted,
        'predictions': ensemble_pred_r2
    }
    all_results['Ensemble_OLS'] = {
        'r2': ensemble_r2_ols,
        'rmse': ensemble_rmse_ols,
        'mae': ensemble_mae_ols,
        'predictions': ensemble_pred_ols
    }
    
    # é€‰æ‹©æ›´å¥½çš„ç­–ç•¥
    if ensemble_r2_ols > ensemble_r2_weighted:
        print(f"  âœ… æœ€å°äºŒä¹˜æ³•æ›´ä¼˜ï¼Œæå‡: +{(ensemble_r2_ols-ensemble_r2_weighted):.4f}")
    elif ensemble_r2_weighted > ensemble_r2_ols:
        print(f"  âœ… RÂ²Â³åŠ æƒæ›´ä¼˜ï¼Œä¼˜åŠ¿: +{(ensemble_r2_weighted-ensemble_r2_ols):.4f}")
    else:
        print(f"  âš–ï¸ ä¸¤ç§ç­–ç•¥æ€§èƒ½ç›¸å½“")

# ç»“æœæ±‡æ€»
print("\n" + "="*80)
print("æœ€ç»ˆç»“æœæ±‡æ€»ï¼ˆä¸°å¯Œç‰¹å¾ç‰ˆï¼‰")
print("="*80)

sorted_results = sorted(all_results.items(), key=lambda x: x[1]['r2'], reverse=True)

print(f"\n{'æ¨¡å‹':<25} {'RÂ²':<10} {'RMSE':<12} {'MAE':<12}")
print("-" * 65)
for model_name, result in sorted_results:
    print(f"{model_name:<25} {result['r2']:>8.4f}  {result['rmse']:>10.6f}  {result['mae']:>10.6f}")

best_model = sorted_results[0][0]
best_r2 = sorted_results[0][1]['r2']
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (RÂ²={best_r2:.4f})")

print(f"\nç‰¹å¾ä¿¡æ¯:")
print(f"  ä¼ ç»ŸMLç‰¹å¾æ•°: {len(selected_features_ml)}")
print(f"  æ·±åº¦å­¦ä¹ ç‰¹å¾æ•°: {len(selected_features_dl)}")
print(f"  Top 10ç‰¹å¾: {selected_features_ml[:10]}")

print("\n" + "="*80)
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("="*80)

